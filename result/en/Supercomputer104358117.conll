The    O
ACS-1    B-Supercomputer104358117
and    O
ACS-360    B-Supercomputer104358117
are    O
two    O
related    O
supercomputers    O
designed    O
by    O
IBM    O
as    O
part    O
of    O
the    O
IBM    O
"    O
Advanced    O
Computing    O
Systems    O
"    O
project    O
from    O
1961    O
to    O
1969    O
.    O

The    O
SX-9    B-Supercomputer104358117
is    O
a    O
supercomputer    O
built    O
by    O
NEC    O
Corporation    O
.    O

The    O
ETA10    B-Supercomputer104358117
is    O
a    O
line    O
of    O
vector    O
supercomputers    O
designed    O
,    O
manufactured    O
,    O
and    O
marketed    O
by    O
ETA    O
Systems    O
,    O
a    O
spin    O
-    O
off    O
division    O
of    O
Control    O
Data    O
Corporation    O
(    O
CDC    O
)    O
.    O

In    O
1988    O
,    O
a    O
team    O
from    O
the    O
school    O
won    O
an    O
ETA-10    B-Supercomputer104358117
supercomputer    O
in    O
the    O
SuperQuest    O
competition    O
,    O
a    O
national    O
science    O
competition    O
for    O
high    O
school    O
students    O
.    O

In    O
1957    O
began    O
re    O
-    O
equipping    O
with    O
the    O
North    O
American    O
F-86L    O
Sabre    O
,    O
an    O
improved    O
version    O
of    O
the    O
F-86D    O
which    O
incorporated    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
,    O
or    O
SAGE    O
computer    O
-    O
controlled    O
direction    O
system    O
for    O
intercepts    O
.    O

The    O
324th    O
FIS    O
and    O
337th    O
FIS    O
upgraded    O
to    O
newer    O
model    O
Sabres    O
with    O
data    O
link    O
for    O
interception    O
control    O
through    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
system    O
in    O
the    O
fall    O
of    O
1957    O
.    O

All    O
models    O
were    O
equipped    O
with    O
data    O
link    O
for    O
interception    O
control    O
through    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
system    O
.    O

The    O
squadron    O
was    O
upgraded    O
to    O
the    O
North    O
American    O
F-86L    O
,    O
an    O
improved    O
version    O
of    O
the    O
Sabre    O
which    O
incorporated    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
computer    O
-    O
controlled    O
direction    O
system    O
for    O
intercepts    O
.    O

In    O
November    O
1956    O
,    O
the    O
squadron    O
upgraded    O
to    O
supersonic    O
Convair    O
F-102    O
Delta    O
Daggers    O
equipped    O
with    O
data    O
link    O
to    O
communicate    O
directly    O
with    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
computers    O
located    O
in    O
combat    O
control    O
centers    O
and    O
armed    O
with    O
the    O
AIM-4    O
Falcon    O
Missile    O
.    O

In    O
December    O
1956    O
,    O
the    O
squadron    O
began    O
to    O
receive    O
upgraded    O
F-86L    O
Sabres    O
equipped    O
with    O
data    O
link    O
communications    O
equipment    O
to    O
interface    O
with    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
system    O
.    O

These    O
aircraft    O
were    O
also    O
equipped    O
with    O
data    O
link    O
,    O
which    O
enabled    O
them    O
to    O
interface    O
directly    O
with    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
computers    O
in    O
ground    O
direction    O
centers    O
,    O
removing    O
the    O
need    O
for    O
voice    O
communications    O
by    O
air    O
to    O
ground    O
radions    O
.    O

The    O
DC    O
had    O
High    O
Frequency    O
Crosstell    O
communication    O
with    O
the    O
1959    O
-    O
1966    O
SAGE    B-Supercomputer104358117
Master    O
Direction    O
Center    O
at    O
Norton    O
Air    O
Force    O
Base    O
(    O
DC-17    O
)    O
for    O
coordinating    O
Army    O
intercepts    O
of    O
targets    O
penetrating    O
through    O
the    O
larger    O
USAF    O
Los    O
Angeles    O
Air    O
Defense    O
Sector    O
defended    O
by    O
ground    O
-    O
controlled    O
aircraft    O
.    O

In    O
1957    O
began    O
re    O
-    O
equipping    O
with    O
the    O
North    O
American    O
F-86L    O
Sabre    O
,    O
an    O
improved    O
version    O
of    O
the    O
F-86D    O
which    O
incorporated    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
,    O
or    O
SAGE    O
computer    O
-    O
controlled    O
direction    O
system    O
for    O
intercepts    O
.    O

Designated    O
site    O
"    O
Z-211    O
"    O
(    O
FAA    O
J-05    O
)    O
,    O
the    O
645th    O
Radar    O
Squadron    O
was    O
reactivated    O
on    O
28    O
June    O
1962    O
to    O
operate    O
the    O
radar    O
,    O
feeding    O
data    O
to    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
Data    O
Center    O
DC-09    O
at    O
Gunter    O
AFB    O
,    O
Alabama    O
.    O

The    O
computer    O
,    O
named    O
Whirlwind    O
,    O
helped    O
the    O
USAF    O
develop    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
system    O
.    O

The    O
need    O
for    O
the    O
FYQ-93    O
system    O
became    O
apparent    O
in    O
the    O
1970s    O
when    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
system    O
became    O
technologically    O
obsolete    O
and    O
logistically    O
unsupportable    O
.    O

In    O
1957    O
began    O
re    O
-    O
equipping    O
with    O
the    O
North    O
American    O
F-86L    O
Sabre    O
,    O
an    O
improved    O
version    O
of    O
the    O
F-86D    O
which    O
incorporated    O
data    O
link    O
to    O
interface    O
with    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
computer    O
-    O
controlled    O
direction    O
system    O
for    O
intercepts    O
.    O

It    O
finally    O
equipped    O
with    O
nuclear    O
-    O
capable    O
F-89Js    O
,    O
armed    O
with    O
the    O
AIR-2    O
Genie    O
and    O
equipped    O
with    O
data    O
link    O
for    O
interception    O
control    O
through    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
system    O
in    O
the    O
spring    O
of    O
1958    O
.    O

In    O
1959    O
Malmstrom    O
was    O
performing    O
air    O
-    O
traffic    O
-    O
control    O
duties    O
for    O
the    O
FAA    O
,    O
and    O
joined    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
system    O
on    O
1    O
March    O
1961    O
,    O
the    O
squadron    O
being    O
redesignated    O
as    O
the    O
801st    O
Radar    O
Squadron    O
(    O
SAGE    O
)    O
.    O

Each    O
air    O
division    O
was    O
commanded    O
from    O
a    O
SAGE    O
(    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
)    O
blockhouse    O
housing    O
that    O
division    O
's    O
command    O
and    O
control    O
element    O
plus    O
associated    O
air    O
defense    O
radar    O
and    O
computer    O
hardware    O
.    O

Continental    O
Air    O
Defense    O
Integration    O
North    O
(    O
CADIN    O
)    O
was    O
a    O
Cold    O
War    O
program    O
to    O
develop    O
military    O
installations    O
in    O
Canada    O
for    O
the    O
air    O
defense    O
of    O
North    O
America    O
using    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
already    O
being    O
deployed    O
in    O
the    O
CONUS    O
.    O

The    O
US    O
'    O
SAGE    B-Supercomputer104358117
system    O
was    O
perhaps    O
the    O
most    O
complex    O
attempted    O
,    O
using    O
building    O
-    O
filling    O
computers    O
linked    O
to    O
dozens    O
of    O
radars    O
and    O
other    O
sensors    O
to    O
automate    O
the    O
entire    O
task    O
of    O
identifying    O
an    O
enemy    O
aircraft    O
's    O
track    O
and    O
directing    O
interceptor    O
aircraft    O
or    O
surface    O
-    O
to    O
-    O
air    O
missiles    O
against    O
it    O
.    O

The    O
most    O
advanced    O
GCI    O
system    O
deployed    O
to    O
date    O
was    O
the    O
US    O
's    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
system    O
.    O

During    O
1962    O
Fort    O
Fisher    O
AFS    O
joined    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
system    O
,    O
initially    O
feeding    O
data    O
to    O
DC-04    O
at    O
Fort    O
Lee    O
AFS    O
,    O
Virginia    O
.    O

During    O
1958    O
Montauk    O
AFS    O
joined    O
the    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
system    O
,    O
feeding    O
data    O
to    O
DC-01    O
at    O
McGuire    O
AFB    O
,    O
New    O
Jersey    O
.    O

In    O
September    O
1957    O
the    O
86th    O
FIS    O
traded    O
its    O
Sabres    O
for    O
Convair    O
F-102    O
Delta    O
Dagger    O
aircraft    O
equipped    O
with    O
data    O
link    O
for    O
interception    O
control    O
through    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
system    O
.    O

LA-45DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
RP-39    O
/    O
Z-39    O
The    O
AADCP    O
was    O
inactivated    O
1    O
Sep    O
1974    O
along    O
with    O
the    O
remaining    O
Nike    O
Hercules    O
sites    O
.    O

SF-90DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-38    O
/    O
Z-38    O
The    O
AADCP    O
was    O
inactivated    O
in    O
mid-1971    O
.    O

HM-01DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
Z-210    O
.    O

C-80DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
RP-31    O
/    O
Z-31    O
.    O

The    O
AADCP    O
was    O
later    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-80    O
with    O
FPS-10    O
(    O
2    O
)    O
;    O
FPS-8/GPS-3    O
;    O
FPS-7C    O
and    O
FPS-6A    O
radars    O
.    O

On    O
1    O
Oct    O
1961    O
W-13DC    O
was    O
integrated    O
with    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
RP-54/Z-227    O
.    O

B-21DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
MM-1    O
.    O

D-15DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-20    O
/    O
Z-20    O
The    O
Air    O
Force    O
ceased    O
radar    O
operations    O
when    O
the    O
Army    O
no    O
longer    O
needed    O
radar    O
support    O
and    O
the    O
AADCP    O
was    O
inactivated    O
1    O
Sep    O
1974    O
.    O

KC-65DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-72    O
/    O
Z-72    O
.    O

SL-47DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-70    O
/    O
Z-70    O
.    O

The    O
AAFC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-71    O
/    O
Z-71    O
.    O

NF-17DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-21    O
/    O
Z-21    O
Nike    O
operations    O
at    O
the    O
site    O
inativated    O
in    O
1962    O
.    O

NY-55DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-9    O
/    O
Z-9    O
Air    O
Force    O
operations    O
at    O
the    O
site    O
ended    O
on    O
1    O
July    O
1966    O
,    O
and    O
Nike    O
operations    O
were    O
inactivated    O
on    O
31    O
Oct    O
1974    O
.    O

On    O
1    O
May    O
1961    O
PH-64DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
RP-63/Z-63    O
Nike    O
operations    O
were    O
inactivated    O
on    O
30    O
Sep    O
1966    O

PI-70DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
RP-62    O
/    O
Z-62    O
.    O

The    O
post    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
M-97    O
.    O

DF-30DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
P-78    O
/    O
Z-78    O
.    O

The    O
AADCP    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
M-89    O
/    O
Z-89    O
.    O

The    O
AADCP    O
was    O
later    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
"    O
P-56    O
/    O
Z-56    O
"    O
'    O
.    O

The    O
AAFC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
SM-151    O
/    O
Z-151    O
.    O

S-90DC    O
was    O
integrated    O
with    O
the    O
USAF    O
Air    O
Defense    O
Command    O
/    O
NORAD    O
Semi    B-Supercomputer104358117
Automatic    I-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
air    O
defense    O
radar    O
network    O
as    O
Site    O
RP-1    O
/    O
Z-1    O
The    O
Air    O
Force    O
ceased    O
radar    O
operations    O
in    O
March    O
1963    O
and    O
the    O
AADCP    O
was    O
inactivated    O
1    O
Sep    O
1974    O
.    O

On    O
1    O
June    O
1962    O
,    O
the    O
Navy    O
AN    O
/    O
FPS-37    O
Radar    O
site    O
was    O
added    O
to    O
the    O
United    O
States    O
Air    O
Force    O
's    O
Air    O
Defense    O
Command    O
SAGE    B-Supercomputer104358117
network    O
feeding    O
data    O
to    O
DC-09    O
at    O
Gunter    O
AFB    O
,    O
Alabama    O
.    O

It    O
finally    O
equipped    O
with    O
nuclear    O
-    O
capable    O
F-89Js    O
,    O
armed    O
with    O
the    O
AIR-2    O
Genie    O
and    O
equipped    O
with    O
data    O
link    O
for    O
interception    O
control    O
through    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
system    O
in    O
the    O
spring    O
of    O
1958    O
.    O

The    O
group    O
's    O
first    O
tenant    O
,    O
the    O
Grand    O
Forks    O
Air    O
Defense    O
Sector    O
was    O
activated    O
in    O
December    O
,    O
and    O
its    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
Direction    O
Center    O
(    O
DC-11    O
)    O
was    O
accepted    O
for    O
operation    O
in    O
March    O
1958    O
.    O

Consequently    O
,    O
RCAF    O
Station    O
Sydney    O
's    O
radar    O
equipment    O
underwent    O
an    O
upgrade    O
in    O
the    O
late    O
1950s    O
and    O
early    O
1960s    O
with    O
the    O
operational    O
implementation    O
of    O
the    O
Semi-Automatic    B-Supercomputer104358117
Ground    I-Supercomputer104358117
Environment    I-Supercomputer104358117
(    O
SAGE    O
)    O
System    O
on    O
September    O
15    O
,    O
1962    O
.    O

Urgent    B-Supercomputer104358117
computing    I-Supercomputer104358117

In    O
another    O
approach    O
,    O
a    O
large    O
number    O
of    O
processors    O
are    O
used    O
in    O
close    O
proximity    O
to    O
each    O
other    O
,    O
e.g.    O
,    O
in    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

MPP    O
architectures    O
are    O
the    O
second    O
most    O
common    O
supercomputer    O
implementations    O
after    O
cluster    B-Supercomputer104358117
,    O
as    O
of    O
November    O
2013    O
.    O

It    O
is    O
a    O
full    O
-    O
function    O
server    O
operating    O
system    O
that    O
supports    O
up    O
to    O
8    O
physical    O
processors    O
and    O
provides    O
enterprise    O
-    O
class    O
features    O
such    O
as    O
eight    O
-    O
node    O
clustering    B-Supercomputer104358117
using    O
Microsoft    O
Cluster    O
Server    O
(    O
MSCS    O
)    O
software    O
and    O
support    O
for    O
up    O
to    O
64    O
GB    O
of    O
RAM    O
through    O
PAE    O
.    O

The    O
Datacenter    O
edition    O
,    O
like    O
the    O
Enterprise    O
edition    O
,    O
supports    O
8-node    O
clustering    B-Supercomputer104358117
.    O

Windows    O
Compute    O
Cluster    O
Server    O
2003    O
(    O
CCS    O
)    O
,    O
released    O
in    O
June    O
2006    O
,    O
is    O
designed    O
for    O
high    O
-    O
end    O
applications    O
that    O
require    O
high    O
performance    O
computing    O
clusters    B-Supercomputer104358117
.    O

A    O
GPU    O
cluster    O
is    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
in    O
which    O
each    O
node    O
is    O
equipped    O
with    O
a    O
Graphics    O
Processing    O
Unit    O
(    O
GPU    O
)    O
.    O

Mapping    O
an    O
algorithm    O
to    O
run    O
a    O
GPU    O
cluster    O
is    O
somewhat    O
similar    O
to    O
mapping    O
an    O
algorithm    O
to    O
run    O
on    O
a    O
traditional    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

In    O
2009    O
,    O
Purdue    O
named    O
the    O
Coates    B-Supercomputer104358117
supercomputing    B-Supercomputer104358117
cluster    I-Supercomputer104358117
,    O
after    O
him    O
,    O
continuing    O
a    O
practice    O
of    O
naming    O
the    O
machines    O
for    O
prominent    O
figures    O
in    O
the    O
history    O
of    O
computing    O
at    O
the    O
university    O
,    O
which    O
began    O
with    O
Purdue    O
's    O
Steele    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

Desmond    O
is    O
a    O
software    O
package    O
developed    O
at    O
D.    O
E.    O
Shaw    O
Research    O
to    O
perform    O
high    O
-    O
speed    O
molecular    O
dynamics    O
simulations    O
of    O
biological    O
systems    O
on    O
conventional    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

Babel    O
works    O
on    O
all    O
known    O
POSIX    O
and    O
Unix    O
variants    O
,    O
including    O
Linux    O
,    O
Mac    O
OS    O
X    O
,    O
AIX    O
,    O
IRIX    O
,    O
Solaris    O
,    O
Tru64    O
,    O
Cray    O
's    O
XT4    B-Supercomputer104358117
,    O
IBM    O
's    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
,    O
and    O
many    O
commodity    O
clusters    B-Supercomputer104358117
.    O

The    O
building    O
is    O
the    O
location    O
of    O
several    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
and    O
auditorium    O
-    O
style    O
classrooms    O
.    O

OpenSSI    O
is    O
an    O
open    O
source    O
single    O
-    O
system    O
image    O
clustering    B-Supercomputer104358117
system    O
.    O

In    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
a    O
single    O
system    O
image    O
(    O
SSI    O
)    O
cluster    O
is    O
a    O
cluster    B-Supercomputer104358117
of    O
machines    O
that    O
appears    O
to    O
be    O
one    O
single    O
system    O
.    O

Computer    B-Supercomputer104358117
clusters    I-Supercomputer104358117

A    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
consists    O
of    O
a    O
set    O
of    O
loosely    O
or    O
tightly    O
connected    O
computers    O
that    O
work    O
together    O
so    O
that    O
,    O
in    O
many    O
respects    O
,    O
they    O
can    O
be    O
viewed    O
as    O
a    O
single    O
system    O
.    O

While    O
a    O
graduate    O
student    O
at    O
MIT    O
,    O
he    O
initially    O
worked    O
on    O
high    O
-    O
performance    O
system    O
area    O
network    O
for    O
cluster    B-Supercomputer104358117
computing    I-Supercomputer104358117
(    O
StarT    O
-    O
Jr    O
and    O
Start    O
-    O
X    O
)    O
.    O

CHAOS    O
–    O
small    O
(    O
6    O
MB    O
)    O
and    O
designed    O
for    O
creating    O
ad    O
hoc    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117

Cluster    B-Supercomputer104358117
(computing)    I-Supercomputer104358117

Coherence    O
uses    O
a    O
specialized    O
scalable    O
protocol    O
and    O
many    O
inexpensive    O
computers    O
to    O
create    O
a    O
cluster    B-Supercomputer104358117
which    O
can    O
be    O
seamlessly    O
expanded    O
to    O
add    O
more    O
memory    O
,    O
processing    O
power    O
or    O
both    O
.    O

There    O
are    O
150,000    O
computers    O
on    O
the    O
Stanford    O
University    O
Network    O
,    O
including    O
clusters    O
of    O
printer    O
-    O
enabled    O
computers    O
in    O
every    O
undergraduate    O
residence    O
(    O
the    O
first    O
residential    O
computing    O
program    O
)    O
,    O
as    O
well    O
as    O
high    O
-    O
performance    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
for    O
general    O
use    O
throughout    O
the    O
campus    O
.    O

An    O
Aiyara    O
cluster    O
is    O
a    O
low    O
-    O
powered    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
specially    O
designed    O
to    O
process    O
Big    O
Data    O
.    O

It    O
consists    O
of    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
built    O
from    O
commodity    O
hardware    O
.    O

Each    O
cluster    B-Supercomputer104358117
member    O
(    O
called    O
node    O
)    O
has    O
the    O
same    O
rights    O
and    O
responsibilities    O
of    O
the    O
others    O
(    O
with    O
the    O
exception    O
of    O
the    O
oldest    O
member    O
,    O
that    O
we    O
are    O
going    O
to    O
see    O
in    O
details    O
)    O
:    O
this    O
is    O
because    O
Hazelcast    O
implements    O
a    O
peer    O
-    O
to    O
-    O
peer    O
cluster    B-Supercomputer104358117
,    O
so    O
that    O
there    O
's    O
no    O
"    O
master    O
"    O
node    O
.    O

If    O
no    O
cluster    B-Supercomputer104358117
is    O
found    O
,    O
the    O
node    O
will    O
be    O
the    O
first    O
member    O
of    O
the    O
cluster    O
.    O

JEM    O
manages    O
several    O
queues    O
used    O
to    O
maintain    O
the    O
life    O
-    O
cycle    O
of    O
a    O
job    O
:    O
the    O
queues    O
are    O
implemented    O
using    O
Hazelcast    O
data    B-Supercomputer104358117
sharing    I-Supercomputer104358117
.    O

Each    O
of    O
these    O
paths    O
should    O
be    O
mount    O
in    O
a    O
shared    O
file    O
system    O
(    O
may    O
be    O
different    O
shared    O
file    O
systems    O
,    O
one    O
for    O
each    O
parh    O
if    O
needed    O
)    O
so    O
that    O
all    O
the    O
nodes    O
in    O
the    O
cluster    B-Supercomputer104358117
will    O
refers    O
to    O
files    O
in    O
the    O
same    O
way    O
,    O
will    O
avoid    O
redundancy    O
and    O
will    O
always    O
be    O
up    O
to    O
date    O
relative    O
to    O
the    O
libraries    O
versions    O
,    O
binary    O
versions    O
etc    O
...    O

Computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117

The    O
node    O
itself    O
is    O
based    O
on    O
a    O
two    O
-    O
way    O
Intel    O
Xeon    O
5400    O
architecture    O
and    O
supports    O
Nvidia    O
Quadro    O
graphic    O
cards    O
and    O
is    O
intended    O
to    O
be    O
clustered    B-Supercomputer104358117
.    O

It    O
was    O
notable    O
for    O
providing    O
an    O
early    O
implementation    O
of    O
the    O
single    O
-    O
system    O
image    O
idea    O
,    O
where    O
a    O
cluster    B-Supercomputer104358117
of    O
machines    O
appeared    O
to    O
be    O
one    O
larger    O
machine    O
.    O

Components    O
of    O
the    O
Linux    O
kernel    O
,    O
such    O
as    O
Logical    O
Volume    O
Manager    O
(    O
LVM    O
)    O
,    O
are    O
well    O
suited    O
to    O
support    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

Both    O
DAS    O
and    O
NAS    O
can    O
potentially    O
increase    O
availability    O
of    O
data    O
by    O
using    O
RAID    O
or    O
clustering    B-Supercomputer104358117
.    O

Pacemaker    O
is    O
an    O
open    O
source    O
high    O
availability    O
resource    O
manager    O
software    O
used    O
on    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
since    O
2004    O
.    O

In    O
the    O
domain    O
of    O
quantum    O
computing    O
,    O
being    O
able    O
to    O
send    O
qubits    O
from    O
one    O
quantum    O
processor    O
to    O
another    O
allows    O
them    O
to    O
be    O
connected    O
to    O
form    O
a    O
quantum    O
computing    O
cluster    B-Supercomputer104358117
.    O

This    O
is    O
analogous    O
to    O
connecting    O
several    O
classical    O
computers    O
to    O
form    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
in    O
classical    O
computing    O
.    O

Sun    O
Fire    O
X4100    O
cluster    B-Supercomputer104358117
.    O

database    O
clusters    B-Supercomputer104358117
implement    O
multi    O
-    O
master    O
replication    O
using    O
one    O
of    O
two    O
methods    O
.    O

Thus    O
the    O
quite    O
effective    O
utilization    O
of    O
local    O
techniques    O
in    O
such    O
distributed    O
environments    O
is    O
common    O
,    O
e.g.    O
,    O
in    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
and    O
multi    O
-    O
core    O
processors    O
.    O

Computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117

It    O
is    O
intended    O
to    O
speed    O
up    O
clustering    B-Supercomputer104358117
operations    O
on    O
large    O
data    O
sets    O
,    O
where    O
using    O
another    O
algorithm    O
directly    O
may    O
be    O
impractical    O
due    O
to    O
the    O
size    O
of    O
the    O
data    O
set    O
.    O

Maui    O
Cluster    O
Scheduler    O
is    O
a    O
job    O
scheduler    O
for    O
use    O
on    O
clusters    B-Supercomputer104358117
and    O
supercomputers    O
initially    O
developed    O
by    O
Cluster    O
Resources    O
,    O
Inc    O
..    O

Optimus    O
is    O
integrated    O
with    O
several    O
resource    O
management    O
systems    O
to    O
support    O
parallel    O
execution    O
on    O
a    O
computational    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

In    O
a    O
Hazelcast    O
grid    O
,    O
data    O
is    O
evenly    O
distributed    O
among    O
the    O
nodes    O
of    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
,    O
allowing    O
for    O
horizontal    O
scaling    O
of    O
processing    B-Supercomputer104358117
and    O
available    O
storage    O
.    O

Melomics109    O
is    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
(    O
three    O
cabinets    O
with    O
customized    O
front    O
panels    O
)    O
located    O
at    O
Universidad    O
de    O
Málaga    O
.    O

The    O
number    O
of    O
traversed    O
edges    O
per    O
second    O
that    O
can    O
be    O
performed    O
by    O
a    O
supercomputer    O
cluster    B-Supercomputer104358117
is    O
a    O
measure    O
of    O
both    O
the    O
communications    O
capabilities    O
and    O
computational    O
power    O
of    O
the    O
machine    O
.    O

The    O
Cubieboard    O
team    O
managed    O
to    O
run    O
an    O
Apache    O
Hadoop    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
using    O
the    O
Lubuntu    O
Linux    O
distribution    O
.    O

Cluster    B-Supercomputer104358117
(computing)    I-Supercomputer104358117

Kan    B-Supercomputer104358117
Balam    I-Supercomputer104358117
(    O
Spanish    O
pronunciation    O
:    O
kan    O
'    O
balaːm    O
,    O
Tzotzil    O
Maya    O
pronunciation    O
:    O
'    O
kʱaŋ    O
βalɒm    O
)    O
is    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
located    O
in    O
Mexico    O
City    O
,    O
on    O
the    O
main    O
campus    O
of    O
the    O
UNAM    O
.    O

Unified    O
Parallel    O
C    O
(    O
UPC    O
)    O
is    O
an    O
extension    O
of    O
the    O
C    O
programming    O
language    O
designed    O
for    O
high    O
-    O
performance    O
computing    O
on    O
large    O
-    O
scale    O
parallel    B-Supercomputer104358117
machine    I-Supercomputer104358117
,    O
including    O
those    O
with    O
a    O
common    O
global    O
address    O
space    O
(    O
SMP    O
and    O
NUMA    B-Supercomputer104358117
)    O
and    O
those    O
with    O
distributed    O
memory    O
(    O
e.g.    O
clusters    B-Supercomputer104358117
)    O
.    O

The    O
NCHC    O
is    O
Taiwan    O
’s    O
primary    O
facility    O
for    O
high    O
performance    O
computing    O
(    O
HPC    O
)    O
resources    O
including    O
large    O
-    O
scale    O
computational    O
science    O
and    O
engineering    O
,    O
cluster    B-Supercomputer104358117
and    O
grid    O
computing    O
,    O
middleware    O
development    O
,    O
visualization    O
and    O
virtual    O
reality    O
,    O
data    O
storage    O
,    O
networking    O
,    O
and    O
HPC    O
-    O
related    O
training    O
.    O

The    O
second    O
group    O
is    O
meant    O
for    O
scale    O
-    O
out    O
cluster    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
consists    O
of    O
the    O
CX1000    O
Blade    O
Enclosure    O
,    O
and    O
the    O
CX1000-HN    O
,    O
CX1000-C    O
and    O
CX1000-G    O
nodes    O
.    O

The    O
CX1000    O
scale    O
-    O
out    O
cluster    B-Supercomputer104358117
computing    I-Supercomputer104358117
group    O
of    O
systems    O
consists    O
of    O
the    O
CX1000    O
Blade    O
Enclosure    O
,    O
CX1000-C    O
compute    O
Node    O
,    O
CX1000-G    O
GPU    O
Node    O
and    O
CX1000-HN    O
Management    O
Node    O
.    O

Clustering    B-Supercomputer104358117
support    O

In    O
June    O
2012    O
,    O
K    O
was    O
superseded    O
as    O
the    O
world    O
's    O
fastest    O
supercomputer    O
by    O
the    O
American    O
IBM    B-Supercomputer104358117
Sequoia    I-Supercomputer104358117
.    O

On    O
18    O
June    O
2012    O
,    O
the    O
TOP500    O
Project    O
Committee    O
announced    O
that    O
the    O
California    O
-    O
based    O
IBM    B-Supercomputer104358117
Sequoia    I-Supercomputer104358117
supercomputer    O
had    O
replaced    O
K    O
as    O
the    O
world    O
's    O
fastest    O
supercomputer    O
,    O
with    O
a    O
LINPACK    O
performance    O
of    O
16.325    O
petaflops    O
.    O

An    O
American    O
research    O
team    O
uses    O
the    O
world    O
's    O
most    O
powerful    O
supercomputer    O
at    O
the    O
time    O
–    O
the    O
IBM    B-Supercomputer104358117
Sequoia    I-Supercomputer104358117
–    O
to    O
perform    O
a    O
record    O
-    O
breaking    O
computation    O
,    O
modelling    O
an    O
experimental    O
jet    O
engine    O
on    O
over    O
one    O
million    O
processor    O
cores    O
.    O

2013    O
Announced    O
the    O
SX-ACE    B-Supercomputer104358117

Amalka    B-Supercomputer104358117
Supercomputing    I-Supercomputer104358117
facility    I-Supercomputer104358117
-    O
a    O
supercomputer    O
in    O
the    O
Czech    O
Republic    O

Supercomputers    O
that    O
use    O
a    O
fat    O
tree    O
network    O
include    O
the    O
Tianhe-2    B-Supercomputer104358117
,    O
the    O
Meiko    O
Scientific    O
CS-2    O
,    O
Yellowstone    B-Supercomputer104358117
,    O
the    O
Earth    B-Supercomputer104358117
Simulator    I-Supercomputer104358117
,    O
the    O
Cray    B-Supercomputer104358117
X2    I-Supercomputer104358117
,    O
the    O
Connection    O
Machine    O
CM-5    B-Supercomputer104358117
,    O
and    O
various    O
Altix    B-Supercomputer104358117
supercomputers    O
.    O

Like    O
Steele    O
,    O
Rossman    O
and    O
Carter    B-Supercomputer104358117
,    O
Coates    O
is    O
part    O
of    O
the    O
DiaGrid    O
distributed    O
computing    O
network    O
.    O

The    O
Holland    O
Computing    O
Center    O
,    O
which    O
houses    O
the    O
Firefly    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
,    O
is    O
located    O
inside    O
the    O
institute    O
.    O

Hut    O
is    O
one    O
of    O
the    O
founders    O
of    O
the    O
B612    O
Foundation    O
,    O
MODEST    O
,    O
MICA    O
,    O
ACS    O
,    O
the    O
GRAPE    B-Supercomputer104358117
(Gravity    I-Supercomputer104358117
Pipe)    I-Supercomputer104358117
project    O
,    O
and    O
AMUSE    O
.    O

Historically    O
,    O
the    O
Gravity    B-Supercomputer104358117
Pipe    I-Supercomputer104358117
(    O
GRAPE    O
)    O
system    O
for    O
astrophysics    O
at    O
the    O
University    O
of    O
Tokyo    O
was    O
distinguished    O
not    O
by    O
its    O
top    O
speed    O
of    O
64    O
Tflops    O
,    O
but    O
by    O
its    O
cost    O
and    O
energy    O
efficiency    O
,    O
having    O
won    O
the    O
Gordon    O
Bell    O
Prize    O
in    O
1999    O
,    O
at    O
about    O
$    O
7    O
per    O
megaflops    O
,    O
using    O
special    O
purpose    O
processing    O
elements    O
.    O

Intel    B-Supercomputer104358117
iPSC    I-Supercomputer104358117

It    O
was    O
available    O
for    O
Sun    O
,    O
Iris    O
,    O
iPSC    B-Supercomputer104358117
,    O
and    O
nCUBE    O
,    O
but    O
is    O
no    O
longer    O
supported    O
.    O

Dataparallel    O
-    O
C    O
was    O
based    O
on    O
an    O
early    O
version    O
of    O
C    O
and    O
runs    O
on    O
the    O
Intel    O
iPSC/2    B-Supercomputer104358117
and    O
nCUBE    O
.    O

Finite    B-Supercomputer104358117
element    I-Supercomputer104358117
machine    I-Supercomputer104358117

The    O
RIKEN    B-Supercomputer104358117
MDGRAPE-3    I-Supercomputer104358117
for    O
molecular    O
dynamics    O
simulations    O
of    O
proteins    O
is    O
a    O
special    O
purpose    O
petascale    O
supercomputer    O
at    O
the    O
Advanced    O
Center    O
for    O
Computing    O
and    O
Communication    O
,    O
RIKEN    O
in    O
Wako    O
,    O
Saitama    O
,    O
just    O
outside    O
Tokyo    O
.    O

To    O
predict    O
protein    O
structure    O
"    O
de    O
novo    O
"    O
for    O
larger    O
proteins    O
will    O
require    O
better    O
algorithms    O
and    O
larger    O
computational    O
resources    O
like    O
those    O
afforded    O
by    O
either    O
powerful    O
supercomputers    O
(    O
such    O
as    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
or    O
MDGRAPE-3    B-Supercomputer104358117
)    O
or    O
distributed    O
computing    O
(    O
such    O
as    O
Folding@home    O
,    O
the    O
Human    O
Proteome    O
Folding    O
Project    O
and    O
Rosetta@Home    O
)    O
.    O

iDataCool    B-Supercomputer104358117
is    O
a    O
high    O
-    O
performance    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
based    O
on    O
a    O
modified    O
IBM    O
System    O
x    O
iDataPlex    O
.    O

Another    O
well    O
-    O
known    O
installation    O
is    O
"    O
Watson    B-Supercomputer104358117
"    O
,    O
an    O
artificial    O
intelligence    O
system    O
capable    O
of    O
answering    O
natural    O
language    O
questions    O
,    O
which    O
won    O
several    O
"    O
Jeopardy    O
!    O
"    O
games    O
against    O
human    O
contestants    O
in    O
February    O
2011    O
on    O
the    O
site    O
.    O

Watson    B-Supercomputer104358117
(computer)    I-Supercomputer104358117

The    O
funds    O
were    O
to    O
begin    O
a    O
pilot    O
program    O
using    O
IBM    O
's    O
Watson    B-Supercomputer104358117
question    O
answering    O
computer    O
system    O
as    O
a    O
clinical    O
decision    O
support    O
system    O
in    O
cancer    O
.    O

IBM    O
bought    O
Blekko    O
and    O
closed    O
the    O
search    O
service    O
on    O
27    O
March    O
2015    O
,    O
redirecting    O
searches    O
to    O
a    O
page    O
announcing    O
"    O
The    O
blekko    O
technology    O
and    O
team    O
have    O
joined    O
IBM    O
Watson    O
!    O
"    O
and    O
linking    O
to    O
a    O
blog    O
post    O
announcing    O
that    O
the    O
blekko    O
service    O
was    O
closed    O
,    O
with    O
blekko    O
's    O
web    O
-    O
crawling    O
abilities    O
to    O
be    O
integrated    O
into    O
IBM    B-Supercomputer104358117
Watson    I-Supercomputer104358117
,    O
adding    O
advanced    O
Web    O
-    O
crawling    O
,    O
categorization    O
and    O
intelligent    O
filtering    O
technology    O
.    O

Competitors    O
include    O
IBM    B-Supercomputer104358117
Watson    I-Supercomputer104358117
and    O
Grok    O
.    O

In    O
contrast    O
to    O
these    O
views    O
,    O
Artificial    O
Intelligence    O
companies    O
such    O
as    O
Amazon    O
Mechanical    O
Turk    O
and    O
CrowdFlower    O
are    O
using    O
collective    O
intelligence    O
and    O
crowdsourcing    O
or    O
consensus    O
-    O
based    O
assessment    O
to    O
collect    O
the    O
enormous    O
amounts    O
of    O
data    O
for    O
machine    O
learning    O
algorithms    O
such    O
as    O
Keras    O
and    O
IBM    B-Supercomputer104358117
Watson    I-Supercomputer104358117
.    O

Yseop    O
's    O
software    O
suite    O
also    O
competes    O
indirectly    O
with    O
IBM    B-Supercomputer104358117
Watson    I-Supercomputer104358117
.    O

Behaivior    O
is    O
currently    O
competing    O
for    O
the    O
$    O
5    O
million    O
IBM    B-Supercomputer104358117
Watson    I-Supercomputer104358117
AI    O
XPRIZE    O
.    O

The    O
term    O
was    O
used    O
by    O
"    O
New    O
Scientist    O
"    O
magazine    O
,    O
and    O
was    O
referenced    O
on    O
the    O
February    O
16    O
,    O
2011    O
episode    O
of    O
"    O
Jeopardy    O
!    O
"    O
by    O
Ken    O
Jennings    O
in    O
acknowledgment    O
of    O
the    O
accomplishments    O
of    O
the    O
computer    O
Watson    B-Supercomputer104358117
.    O

IBM    O
(    O
acquired    O
Vivisimo    O
,    O
rebranded    O
"    O
Watson    B-Supercomputer104358117
"    O
)    O

Ruchir    O
Puri    O
is    O
the    O
chief    O
architect    O
of    O
IBM    B-Supercomputer104358117
Watson    I-Supercomputer104358117
,    O
responsible    O
for    O
developing    O
and    O
deploying    O
Watson    O
platform    O
architecture    O
across    O
the    O
range    O
of    O
Watson    O
offerings    O
.    O

David    O
Ferrucci    O
(    O
1994    O
)    O
,    O
computer    O
scientist    O
,    O
developed    O
IBM    O
Watson    B-Supercomputer104358117
AI    O
Jeopardy    O
player    O

Watson    B-Supercomputer104358117
,    O
a    O
question    O
answering    O
system    O
developed    O
by    O
IBM    O
.    O

Watson    B-Supercomputer104358117
,    O
a    O
pilot    O
service    O
by    O
IBM    O
to    O
uncover    O
and    O
share    O
data    O
-    O
driven    O
insights    O
,    O
and    O
to    O
spur    O
cognitive    O
applications    O
.    O

Watson    B-Supercomputer104358117
is    O
a    O
question    O
answering    O
computer    O
system    O
capable    O
of    O
answering    O
questions    O
posed    O
in    O
natural    O
language    O
,    O
developed    O
in    O
IBM    O
's    O
DeepQA    O
project    O
by    O
a    O
research    O
team    O
led    O
by    O
principal    O
investigator    O
David    O
Ferrucci    O
.    O

Ken    O
Jennings    O
,    O
defeated    O
along    O
with    O
Brad    O
Rutter    O
in    O
a    O
Jeopardy    O
!    O
match    O
against    O
IBM    O
's    O
Watson    B-Supercomputer104358117
,    O
writes    O
that    O
Watson    O
's    O
avatar    O
which    O
appeared    O
on    O
-    O
screen    O
for    O
those    O
games    O
showed    O
42    O
"    O
threads    O
of    O
thought    O
,    O
"    O
shown    O
as    O
colorful    O
lines    O
spinning    O
around    O
Watson    O
's    O
logo    O
,    O
and    O
that    O
the    O
number    O
was    O
chosen    O
in    O
reference    O
to    O
this    O
meme    O
.    O

And    O
To    O
The    O
Republic    O
"    O
around    O
the    O
world    O
to    O
even    O
playing    O
keyboard    O
to    O
a    O
song    O
written    O
by    O
the    O
IBM    O
super    O
computer    O
,    O
Watson    B-Supercomputer104358117

The    O
XT5h    O
(    O
"    O
hybrid    O
"    O
)    O
variant    O
also    O
includes    O
support    O
for    O
Cray    B-Supercomputer104358117
X2    I-Supercomputer104358117
vector    O
processor    O
blades    O
,    O
and    O
Cray    O
XR1    O
blades    O
which    O
combine    O
Opterons    O
with    O
FPGA    O
-    O
based    O
Reconfigurable    O
Processor    O
Units    O
(    O
RPUs    O
)    O
provided    O
by    O
DRC    O
Computer    O
Corporation    O
.    O

The    O
SX-3    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
family    O
was    O
developed    O
by    O
NEC    O
Corporation    O
and    O
announced    O
in    O
April    O
1989    O
.    O

QPACE    B-Supercomputer104358117
(    O
QCD    O
Parallel    O
Computing    O
on    O
the    O
Cell    O
Broadband    O
Engine    O
)    O
is    O
a    O
massively    O
parallel    O
and    O
scalable    O
supercomputer    O
designed    O
for    O
applications    O
in    O
lattice    O
quantum    O
chromodynamics    O
.    O

Shaheen    B-Supercomputer104358117
—    O
the    O
Arabic    O
word    O
for    O
peregrine    O
falcon    O
—    O
is    O
the    O
fastest    O
supercomputer    O
in    O
the    O
Middle    O
East    O
and    O
is    O
the    O
15th    O
most    O
powerful    O
in    O
the    O
world    O
,    O
the    O
most    O
powerful    O
supercomputer    O
housed    O
in    O
an    O
academic    O
environment    O
.    O

The    O
earlier    O
Cray-3    B-Supercomputer104358117
was    O
the    O
first    O
major    O
application    O
of    O
gallium    O
arsenide    O
(    O
GaAs    O
)    O
semiconductors    O
in    O
computing    O
.    O

He    O
started    O
a    O
new    O
VLSI    O
technology    O
lab    O
for    O
the    O
Cray-2    O
in    O
Boulder    O
,    O
Colorado    O
,    O
Cray    O
Laboratories    O
,    O
in    O
1979    O
,    O
which    O
closed    O
in    O
1982    O
;    O
undaunted    O
,    O
Cray    O
later    O
headed    O
a    O
similar    O
spin    O
-    O
off    O
in    O
1989    O
,    O
Cray    O
Computer    O
Corporation    O
(    O
CCC    O
)    O
in    O
Colorado    O
Springs    O
,    O
where    O
he    O
worked    O
on    O
the    O
Cray-3    B-Supercomputer104358117
project    O
—    O
the    O
first    O
attempt    O
at    O
major    O
use    O
of    O
gallium    O
arsenide    O
(    O
GaAs    O
)    O
semiconductors    O
in    O
computing    O
.    O

iWarp    B-Supercomputer104358117

The    O
first    O
"    O
successful    O
"    O
implementation    O
of    O
vector    O
processing    O
appears    O
to    O
be    O
the    O
Control    O
Data    O
Corporation    O
STAR-100    B-Supercomputer104358117
and    O
the    O
Texas    O
Instruments    O
Advanced    B-Supercomputer104358117
Scientific    I-Supercomputer104358117
Computer    I-Supercomputer104358117
(    O
ASC    O
)    O
.    O

System    B-Supercomputer104358117
G    I-Supercomputer104358117
has    O
324    O
Mac    O
Pros    O
(    O
2592    O
processor    O
cores    O
)    O
with    O
QDR    O
InfiniBand    B-Supercomputer104358117
in    O
Virginia    O
Tech    O
's    O
Center    O
for    O
High    O
-    O
End    O
Computing    O
Systems    O
.    O

The    O
Rossmann    B-Supercomputer104358117
cluster    I-Supercomputer104358117
is    O
named    O
for    O
Michael    O
Rossmann    O
,    O
Purdue    O
's    O
Hanley    O
Distinguished    O
Professor    O
of    O
Biological    O
Sciences    O
,    O
who    O
is    O
a    O
pioneer    O
in    O
employing    O
high    O
-    O
performance    O
computing    O
in    O
research    O
to    O
reveal    O
the    O
structure    O
of    O
viruses    O
and    O
their    O
component    O
protein    O
molecules    O
.    O

IBM    O
Blue    O
Gene    O
/    O
P    O
,    O
Blue    O
Gene    O
/    O
Q    O
and    O
PERCS    B-Supercomputer104358117

In    O
the    O
HPCS    O
program    O
,    O
he    O
is    O
playing    O
a    O
leading    O
role    O
in    O
helping    O
define    O
the    O
architecture    O
of    O
IBM    O
's    O
PERCS    B-Supercomputer104358117
multiprocessor    O
(    O
POWER7    O
)    O
.    O

Cray    O
ported    O
Cougar    O
to    O
the    O
Opteron    O
based    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
and    O
renamed    O
it    O
Catamount    B-Supercomputer104358117
.    O

This    O
line    O
evolved    O
into    O
the    O
Cray    B-Supercomputer104358117
J90    I-Supercomputer104358117
and    O
eventually    O
the    O
Cray    B-Supercomputer104358117
SV1    I-Supercomputer104358117
in    O
1998    O
.    O

ASCI    B-Supercomputer104358117
Blue    I-Supercomputer104358117
Mountain    I-Supercomputer104358117

IRIX    O
6.4    O
improved    O
multiprocessor    O
scalability    O
for    O
the    O
Octane    O
,    O
Origin    B-Supercomputer104358117
2000    I-Supercomputer104358117
,    O
and    O
Onyx2    B-Supercomputer104358117
systems    O
.    O

The    O
Onyx    O
was    O
succeeded    O
by    O
the    O
Onyx2    B-Supercomputer104358117
in    O
1996    O
and    O
was    O
discontinued    O
on    O
March    O
31    O
,    O
1999    O
.    O

He    O
invented    O
the    O
fat    O
-    O
tree    O
interconnection    O
network    O
,    O
a    O
hardware    O
-    O
universal    O
interconnection    O
network    O
used    O
in    O
many    O
supercomputers    O
,    O
including    O
the    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
CM5    O
,    O
for    O
which    O
he    O
was    O
network    O
architect    O
.    O

At    O
Optomystic    O
,    O
Sims    O
developed    O
software    O
for    O
the    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
2    I-Supercomputer104358117
(    O
CM-2    O
)    O
that    O
animated    O
the    O
water    O
from    O
drawings    O
of    O
a    O
deluge    O
by    O
Leonardo    O
da    O
Vinci    O
,    O
used    O
in    O
Mark    O
Whitney    O
's    O
film    O
'    O
'    O
Excerpts    O
from    O
Leonardo    O
's    O
Deluge    O
''    O
.    O

"    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
"    O
,    O
a    O
real    O
life    O
supercomputer    O
family    O
with    O
a    O
similar    O
architecture    O

A    O
1989    O
"    O
Computerworld    O
"    O
review    O
of    O
the    O
market    O
for    O
mid    O
-    O
range    O
high    O
-    O
performance    O
machines    O
showed    O
only    O
one    O
machine    O
in    O
the    O
same    O
class    O
,    O
the    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
CM-2    I-Supercomputer104358117
.    O

The    O
Thinking    O
Machines    O
CM-5    B-Supercomputer104358117
is    O
an    O
excellent    O
examples    O
of    O
the    O
MIMD    O
concept    O
.    O

The    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
,    O
a    O
65,536-processor    O
parallel    O
computer    O
designed    O
in    O
the    O
mid-1980s    O
,    O
was    O
a    O
black    O
cube    O
with    O
one    O
side    O
covered    O
with    O
a    O
grid    O
of    O
red    O
blinkenlights    O
;    O
the    O
sales    O
demo    O
had    O
them    O
evolving    O
Conway    O
's    O
Game    O
of    O
Life    O
patterns    O
.    O

It    O
was    O
developed    O
in    O
1987    O
as    O
an    O
alternative    O
language    O
to    O
Lisp    O
and    O
CM    O
-    O
Fortran    O
for    O
the    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
CM-2    O
and    O
above    O
.    O

In    O
October    O
2012    O
Cray    O
announced    O
the    O
Cray    B-Supercomputer104358117
XK7    I-Supercomputer104358117
which    O
supports    O
the    O
NVIDIA    O
Kepler    O
GPGPU    O
and    O
announced    O
that    O
the    O
ORNL    O
Jaguar    O
system    O
would    O
be    O
upgraded    O
to    O
an    O
XK7    O
(    O
renamed    O
"    O
Titan    B-Supercomputer104358117
"    O
)    O
and    O
capable    O
of    O
over    O
20    O
petaflops    O
.    O

JUGENE    B-Supercomputer104358117
,    O
an    O
upgraded    O
Blue    O
Gene    O
/    O
P    O
system    O
at    O
Jülich    O
Research    O
Centre    O
in    O
Germany    O
,    O
operational    O
in    O
mid    O
2009    O
.    O

At    O
first    O
,    O
Cray    O
Research    O
denigrated    O
such    O
approaches    O
by    O
complaining    O
that    O
developing    O
software    O
to    O
effectively    O
use    O
the    O
machines    O
was    O
difficult    O
–    O
a    O
true    O
complaint    O
in    O
the    O
era    O
of    O
the    O
ILLIAC    B-Supercomputer104358117
IV    I-Supercomputer104358117
,    O
but    O
becoming    O
less    O
so    O
each    O
day    O
.    O

The    O
ILLIAC    B-Supercomputer104358117
IV    I-Supercomputer104358117
was    O
one    O
of    O
the    O
first    O
attempts    O
to    O
build    O
a    O
massively    O
parallel    O
computer    O
.    O

ILLIAC    B-Supercomputer104358117
IV    I-Supercomputer104358117

ILLIAC    B-Supercomputer104358117
IV    I-Supercomputer104358117

The    O
SGI    O
Altix    O
platform    O
was    O
selected    O
due    O
to    O
a    O
positive    O
experience    O
with    O
Kalpana    B-Supercomputer104358117
,    O
a    O
single    O
-    O
node    O
Altix    O
512-CPU    O
system    O
built    O
and    O
operated    O
by    O
NAS    O
and    O
SGI    O
and    O
named    O
after    O
Columbia    O
astronaut    O
Kalpana    O
Chawla    O
,    O
the    O
first    O
Indian    O
-    O
born    O
woman    O
to    O
fly    O
in    O
space    O
,    O
which    O
was    O
later    O
integrated    O
into    O
the    O
Columbia    O
supercomputer    O
system    O
as    O
the    O
first    O
node    O
of    O
twenty    O
.    O

The    O
Cray-4    B-Supercomputer104358117
was    O
intended    O
to    O
be    O
Cray    O
Computer    O
Corporation    O
's    O
successor    O
to    O
the    O
failed    O
Cray-3    B-Supercomputer104358117
supercomputer    O
.    O

In    O
such    O
a    O
centralized    O
system    O
the    O
speed    O
and    O
flexibility    O
of    O
the    O
interconnect    O
becomes    O
very    O
important    O
,    O
and    O
modern    O
supercomputers    O
have    O
used    O
various    O
approaches    O
ranging    O
from    O
enhanced    O
Infiniband    B-Supercomputer104358117
systems    O
to    O
three    O
-    O
dimensional    O
torus    B-Supercomputer104358117
interconnect    I-Supercomputer104358117
.    O

Some    O
examples    O
of    O
interconnects    O
include    O
Gigabit    O
Ethernet    O
and    O
InfiniBand    B-Supercomputer104358117
.    O

InfiniBand    B-Supercomputer104358117
,    O
a    O
computer    O
network    O
communications    O
link    O
used    O
in    O
high    O
-    O
performance    O
computing    O

The    O
nodes    O
were    O
connected    O
with    O
InfiniBand    B-Supercomputer104358117
single    O
and    O
double    O
data    O
rate    O
(    O
SDR    O
and    O
DDR    O
)    O
cabling    O
with    O
transfer    O
speeds    O
of    O
up    O
to    O
10    O
gigabits    O
per    O
second    O
.    O

A    O
2.5    O
GB    O
/    O
s    O
InfiniBand    B-Supercomputer104358117
network    O
provides    O
the    O
internode    O
connectivity    O
.    O

The    O
primary    O
interconnect    O
is    O
Infiband    B-Supercomputer104358117
4x    O
DDR    O
.    O

The    O
software    O
is    O
designed    O
to    O
operate    O
on    O
UDP    O
/    O
IP    O
and    O
InfiniBand    B-Supercomputer104358117
networks    O
.    O

One    O
of    O
the    O
physical    O
layer    O
specifications    O
of    O
the    O
InfiniBand    B-Supercomputer104358117
protocol    O
,    O
supporting    O
25    O
Gbit    O
/    O
s    O
signalling    O
rate    O

His    O
work    O
with    O
protected    O
user    O
-    O
level    O
communication    O
has    O
contributed    O
significantly    O
to    O
the    O
Remote    O
Direct    O
Memory    O
Access    O
(    O
RDMA    O
)    O
mechanism    O
and    O
Virtual    B-Supercomputer104358117
Interface    I-Supercomputer104358117
Architecture    I-Supercomputer104358117
standard    O
and    O
Infiniband    B-Supercomputer104358117
standard    O
,    O
which    O
are    O
the    O
communication    O
mechanism    O
for    O
the    O
Direct    O
Access    O
File    O
System    O
(    O
DAFS    O
)    O
.    O

It    O
provides    O
twelve    O
10    O
Gbit    O
/    O
s    O
links    O
suitable    O
for    O
single    O
100    O
Gigabit    O
Ethernet    O
,    O
three    O
40    O
Gigabit    O
Ethernet    O
channels    O
,    O
or    O
twelve    O
10    O
Gigabit    O
Ethernet    O
channels    O
or    O
a    O
single    O
Infiniband    B-Supercomputer104358117
12×    O
QDR    O
link    O
.    O

Examples    O
of    O
such    O
network    O
adapters    O
are    O
InfiniBand    B-Supercomputer104358117
HCAs    O
and    O
10    O
GbE    O
network    O
adapters    O
with    O
iWARP    B-Supercomputer104358117
support    O
.    O

MVAPICH2    O
,    O
with    O
support    O
for    O
InfiniBand    B-Supercomputer104358117
,    O
iWARP    B-Supercomputer104358117
,    O
RoCE    O
,    O
and    O
Intel    O
Omni    O
-    O
Path    O

MVAPICH2-GDR    O
,    O
with    O
support    O
for    O
InfiniBand    B-Supercomputer104358117
and    O
NVIDIA    O
CUDA    O
GPUs    O

MVAPICH2-MIC    O
,    O
with    O
support    O
for    O
InfiniBand    B-Supercomputer104358117
and    O
Intel    O
MIC    O

MVAPICH2-Virt    O
,    O
with    O
support    O
for    O
InfiniBand    B-Supercomputer104358117
and    O
SR    O
-    O
IOV    O

MVAPICH2-EA    O
,    O
which    O
is    O
energy    O
-    O
aware    O
and    O
supports    O
InfiniBand    B-Supercomputer104358117
,    O
iWARP    B-Supercomputer104358117
,    O
and    O
RoCE    O

In    O
April    O
2016    O
,    O
implementation    O
of    O
the    O
InfiniBand    B-Supercomputer104358117
"    O
verbs    O
"    O
interface    O
for    O
the    O
Omni    O
-    O
Path    O
fabric    O
was    O
discussed    O
.    O

InfiniBand    B-Supercomputer104358117

IEEE    O
1355    O
had    O
goals    O
like    O
Futurebus    O
and    O
its    O
derivatives    O
Scalable    O
Coherent    O
Interface    O
(    O
SCI    O
)    O
,    O
and    O
InfiniBand    B-Supercomputer104358117
.    O

In    O
verticals    O
such    O
as    O
oil    O
&    O
gas    O
,    O
InfiniBand    B-Supercomputer104358117
storage    O
is    O
sometimes    O
used    O
.    O

The    O
interconnects    O
between    O
nodes    O
can    O
be    O
standard    O
Ethernet    O
,    O
Gigabit    O
Ethernet    O
,    O
InfiniBand    B-Supercomputer104358117
,    O
or    O
SCI    O
interconnects    O
.    O

One    O
example    O
would    O
be    O
to    O
connect    O
via    O
infiniband    B-Supercomputer104358117
between    O
two    O
Bladecenter    O
chassis    O
,    O
where    O
one    O
has    O
SAN    O
attachment    O
to    O
tape    O
,    O
and    O
the    O
other    O
does    O
not    O
.    O

Mellanox    O
-    O
provider    O
of    O
InfiniBand    B-Supercomputer104358117
chipset    O
,    O
company    O
went    O
public    O
on    O
NASDAQ    O
in    O
2007    O

Infiniband    B-Supercomputer104358117

Announced    O
in    O
the    O
early    O
1990s    O
,    O
the    O
family    O
succeeded    O
the    O
HITAC    B-Supercomputer104358117
S-820    I-Supercomputer104358117
.    O

Discussion    O
topics    O
include    O
CDC    O
1604    O
,    O
CDC    O
6600    O
,    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
8600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117
and    O
Seymour    O
Cray    O
.    O

Instead    O
they    O
continued    O
with    O
the    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117
while    O
Cray    O
went    O
off    O
to    O
build    O
the    O
Cray-1    B-Supercomputer104358117
.    O

The    O
system    O
was    O
an    O
evolution    O
of    O
the    O
CDC    O
Cyber    O
205    O
,    O
which    O
can    O
trace    O
its    O
origins    O
back    O
to    O
the    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117
.    O

Blueice    O
,    O
the    O
predecessor    O
of    O
the    O
Bluefire    B-Supercomputer104358117
Supercomputer    I-Supercomputer104358117

Yet    O
even    O
Cray    O
eventually    O
succumbed    O
to    O
the    O
problem    O
during    O
the    O
CDC    B-Supercomputer104358117
8600    I-Supercomputer104358117
project    O
,    O
which    O
eventually    O
led    O
to    O
him    O
leaving    O
Control    O
Data    O
.    O

When    O
CDC    O
ran    O
into    O
financial    O
difficulties    O
in    O
the    O
late    O
1960s    O
,    O
development    O
funds    O
for    O
his    O
follow    O
-    O
on    O
CDC    B-Supercomputer104358117
8600    I-Supercomputer104358117
became    O
scarce    O
.    O

Because    O
of    O
the    O
game    O
's    O
early    O
DirectX    O
12    O
support    O
and    O
extensive    O
use    O
of    O
parallel    B-Supercomputer104358117
computation    I-Supercomputer104358117
,    O
it    O
is    O
commonly    O
used    O
as    O
a    O
benchmark    O
.    O

It    O
is    O
especially    O
useful    O
for    O
describing    O
the    O
performance    O
of    O
parallel    O
programs    O
and    O
multi-processor    B-Supercomputer104358117
systems    O
.    O

in    O
contrast    O
to    O
classical    O
Runge    O
-    O
Kutta    O
or    O
linear    O
multistep    O
methods    O
,    O
they    O
can    O
offer    O
concurrency    B-Supercomputer104358117
in    O
temporal    O
direction    O
.    O

Support    O
for    O
parallel    B-Supercomputer104358117
processing    I-Supercomputer104358117
using    O
multi    O
-    O
core    O
processors    O
,    O
multiple    O
processors    O
,    O
or    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

In    O
computing    O
,    O
massively    O
parallel    O
refers    O
to    O
the    O
use    O
of    O
a    O
large    O
number    O
of    O
processors    O
(    O
or    O
separate    O
computers    O
)    O
to    O
perform    O
a    O
set    O
of    O
coordinated    O
computations    O
in    B-Supercomputer104358117
parallel    I-Supercomputer104358117
(    O
simultaneously    O
)    O
.    O

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117

:    O
The    O
books    O
discusses    O
parallel    O
algorithms    O
for    O
basic    O
problems    O
in    O
computational    O
geometry    O
in    O
various    O
models    O
of    O
parallel    B-Supercomputer104358117
computation    I-Supercomputer104358117
.    O

Within    O
the    O
field    O
of    O
computer    O
science    O
,    O
CIMS    O
concentrates    O
in    O
machine    O
learning    O
,    O
theory    O
,    O
programming    O
languages    O
,    O
computer    O
graphics    O
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

The    O
BBN    B-Supercomputer104358117
Butterfly    I-Supercomputer104358117
was    O
a    O
massively    O
parallel    B-Supercomputer104358117
computer    I-Supercomputer104358117
built    O
by    O
Bolt    O
,    O
Beranek    O
and    O
Newman    O
in    O
the    O
1980s    O
.    O

Efficient    O
parallel    B-Supercomputer104358117
processing    O
support    O
based    O
on    O
domain    O
decomposition    O
and    O
message    O
passing    O
paradigms    O
.    O

This    O
generalization    O
finds    O
applications    O
in    O
combinatorics    O
and    O
in    O
the    O
study    O
of    O
parallelism    B-Supercomputer104358117
in    O
computer    O
science    O
.    O

ND4J    O
's    O
operations    O
include    O
distributed    B-Supercomputer104358117
parallel    B-Supercomputer104358117
versions    O
.    O

Candlin    O
moved    O
on    O
to    O
teach    O
more    O
advanced    O
students    O
,    O
designed    O
courses    O
on    O
real    O
-    O
time    O
programming    O
,    O
and    O
also    O
on    O
parallel    B-Supercomputer104358117
programming    I-Supercomputer104358117
which    O
became    O
her    O
speciality    O
.    O

Additionally    O
there    O
is    O
inherent    O
parallelism    B-Supercomputer104358117
in    O
image    O
processing    O
algorithms    O
.    O

A    O
unified    O
I    O
/    O
O    O
API    O
may    O
not    O
necessarily    O
mirror    O
the    O
structure    O
of    O
the    O
real    O
hardware    O
bus    O
:    O
bus    O
design    O
is    O
limited    O
by    O
several    O
electric    O
constraints    O
and    O
a    O
need    O
for    O
hardware    O
concurrency    B-Supercomputer104358117
management    O
that    O
can    O
mostly    O
be    O
ignored    O
in    O
a    O
software    O
implementation    O
.    O

Key    O
to    O
the    O
design    O
as    O
conceived    O
by    O
Daniel    O
Slotnick    O
,    O
the    O
director    O
of    O
the    O
project    O
,    O
was    O
fairly    O
high    O
parallelism    B-Supercomputer104358117
with    O
up    O
to    O
256    O
processors    O
,    O
used    O
to    O
allow    O
the    O
machine    O
to    O
work    O
on    O
large    O
data    O
sets    O
in    O
what    O
would    O
later    O
be    O
known    O
as    O
array    O
processing    O
.    O

His    O
research    O
has    O
also    O
resulted    O
in    O
the    O
development    O
of    O
techniques    O
,    O
tools    O
and    O
algorithms    O
for    O
high    O
-    O
performance    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

His    O
research    O
is    O
primarily    O
in    O
the    O
area    O
of    O
machine    O
learning    O
,    O
signal    O
processing    O
and    O
compressive    O
sensing    O
,    O
and    O
parallel    O
computing    O
,    O
but    O
his    O
interests    O
have    O
been    O
broad    O
-    O
ranging    O
,    O
including    O
computational    O
complexity    O
theory    O
,    O
database    O
theory    O
,    O
VLSI    O
design    O
,    O
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

Charles    O
Eric    O
Leiserson    O
is    O
a    O
computer    O
scientist    O
,    O
specializing    O
in    O
the    O
theory    O
of    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
particularly    O
practical    O
applications    O
thereof    O
.    O

This    O
version    O
was    O
coded    O
in    O
a    O
mixture    O
of    O
FORTRAN    O
77    O
,    O
Fortran    O
90    O
,    O
and    O
C.    O
MODFLOW-2000    O
can    O
also    O
be    O
compiled    O
for    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
which    O
can    O
allow    O
multiple    O
processors    O
to    O
be    O
used    O
to    O
increase    O
model    O
complexity    O
and/or    O
reduce    O
simulation    O
time    O
.    O

Bit    O
-    O
level    O
parallelism    O
is    O
a    O
form    O
of    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
based    O
on    O
increasing    O
processor    O
word    O
size    O
.    O

This    O
system    O
contained    O
"    O
a    O
20-bit    O
,    O
pipelined    O
,    O
parallel    B-Supercomputer104358117
multi    O
-    O
microprocessor    O
"    O
.    O

Alternatively    O
,    O
the    O
associativity    O
of    O
monoid    O
operations    O
ensures    O
that    O
the    O
operation    O
can    O
be    O
parallelized    B-Supercomputer104358117
by    O
employing    O
a    O
prefix    O
sum    O
or    O
similar    O
algorithm    O
,    O
in    O
order    O
to    O
utilize    O
multiple    O
cores    O
or    O
processors    O
efficiently    O
.    O

It    O
is    O
under    O
the    O
direction    O
of    O
Dr.    O
Manish    O
Parashar    O
and    O
the    O
current    O
research    O
fields    O
include    O
Autonomic    O
Computing    O
,    O
Parallel    B-Supercomputer104358117
Computing    I-Supercomputer104358117
and    O
Distributed    B-Supercomputer104358117
Computing    I-Supercomputer104358117
,    O
Grid    O
Computing    O
,    O
Peer    O
-    O
to    O
-    O
peer    O
Computing    O
,    O
Adaptive    O
Computing    O
Systems    O
,    O
and    O
Scientific    O
Computation    O
..    O

Since    O
all    O
subdivisions    O
of    O
the    O
series    O
can    O
be    O
computed    O
independently    O
of    O
each    O
other    O
,    O
binary    O
splitting    O
lends    O
well    O
to    O
parallelization    B-Supercomputer104358117
and    O
checkpointing    O
.    O

While    O
in    O
that    O
cell    O
they    O
begin    O
to    O
see    O
the    O
outlines    O
of    O
the    O
"    O
Aleph    O
"    O
conspiracy    O
,    O
the    O
true    O
reason    O
for    O
the    O
virus    O
,    O
and    O
how    O
the    O
information    O
retrieved    O
from    O
the    O
computers    O
would    O
be    O
routed    O
by    O
the    O
virus    O
and    O
co    O
-    O
ordinated    O
by    O
Tohiro    O
Natsume    O
's    O
newest    O
project    O
a    O
supercomputer    O
with    O
64    O
parallel    B-Supercomputer104358117
processor    I-Supercomputer104358117
.    O

Yoshio    O
discovers    O
a    O
massive    O
mobilization    O
of    O
Digitronix    O
personnel    O
and    O
hardware    O
being    O
sent    O
to    O
a    O
secret    O
facility    O
at    O
Tyuratam    O
in    O
Kazakhstan    O
,    O
parallel    B-Supercomputer104358117
processors    I-Supercomputer104358117
,    O
Prolog    O
language    O
modules    O
,    O
and    O
experimental    O
fiber    O
optic    O
buses    O
.    O

The    O
6600    O
CPU    O
had    O
multiple    O
functional    O
units    O
which    O
could    O
operate    O
simultaneously    O
(    O
i.e.    O
,    O
"    O
in    O
parallel    B-Supercomputer104358117
"    O
)    O
,    O
allowing    O
the    O
CPU    O
to    O
overlap    O
instructions    O
'    O
execution    O
times    O
.    O

While    O
at    O
Columbia    O
,    O
Stolfo    O
has    O
received    O
close    O
to    O
$    O
50    O
M    O
in    O
funding    O
for    O
research    O
that    O
has    O
broadly    O
focused    O
on    O
Security    O
,    O
Intrusion    O
Detection    O
,    O
Anomaly    O
Detection    O
,    O
Machine    O
Learning    O
and    O
includes    O
early    O
work    O
in    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
and    O
artificial    O
intelligence    O
.    O

Stolfo    O
and    O
students    O
Dan    O
Miranker    O
,    O
Mike    O
van    O
Biema    O
,    O
Alexander    O
Pasik    O
and    O
Steve    O
Taylor    O
,    O
designed    O
the    O
architecture    O
and    O
software    O
systems    O
for    O
the    O
DADO    O
parallel    B-Supercomputer104358117
computer    I-Supercomputer104358117
,    O
an    O
example    O
"    O
fifth    O
generation    O
computer    O
"    O
sponsored    O
by    O
DARPA    O
's    O
high    O
performance    O
parallel    O
computing    O
initiative    O
in    O
the    O
mid-1980s    O
.    O

It    O
was    O
designed    O
for    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
performs    O
significantly    O
better    O
than    O
SANDER    O
when    O
running    O
on    O
more    O
than    O
8–16    O
processors    O
.    O

Single-    O
or    O
multi-process    B-Supercomputer104358117
optimization    O
over    O
networks    O
(    O
SYMPHONY    O
)    O
is    O
an    O
open    O
source    O
branch    O
and    O
cut    O
framework    O
for    O
solving    O
mixed    O
integer    O
programs    O
(    O
MIPs    O
)    O
over    O
heterogeneous    O
networks    O
.    O

By    O
combining    O
locality    O
,    O
bandwidth    O
,    O
and    O
different    O
parallelization    B-Supercomputer104358117
paradigms    O
into    O
a    O
single    O
performance    O
figure    O
,    O
the    O
model    O
can    O
be    O
an    O
effective    O
alternative    O
to    O
assess    O
the    O
quality    O
of    O
attained    O
performance    O
instead    O
of    O
using    O
simple    O
percent    O
-    O
of    O
-    O
peak    O
estimates    O
,    O
as    O
it    O
provides    O
insights    O
on    O
both    O
the    O
implementation    O
and    O
inherent    O
performance    O
limitations    O
.    O

The    O
"    O
in    O
-    O
core    O
ceilings    O
"    O
are    O
roofline    O
-    O
like    O
curve    O
beneath    O
the    O
actual    O
roofline    O
that    O
may    O
be    O
present    O
due    O
to    O
the    O
lack    O
of    O
some    O
form    O
of    O
parallelism    B-Supercomputer104358117
.    O

Encore    O
Computer    O
was    O
an    O
early    O
pioneer    O
in    O
the    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
market    O
,    O
based    O
in    O
Marlborough    O
,    O
Massachusetts    O
.    O

They    O
wanted    O
to    O
improve    O
the    O
software    O
framework    O
for    O
expressing    O
dependencies    O
,    O
to    O
allow    O
more    O
processing    O
to    O
be    O
done    O
concurrently    O
or    O
in    O
parallel    B-Supercomputer104358117
during    O
system    O
booting    O
,    O
and    O
to    O
reduce    O
the    O
computational    O
overhead    O
of    O
the    O
shell    O
.    O

He    O
is    O
known    O
for    O
his    O
work    O
on    O
the    O
organization    O
of    O
computer    O
systems    O
,    O
on    O
parallel    B-Supercomputer104358117
processing    I-Supercomputer104358117
and    O
SARA    O
(    O
system    O
architects    O
apprentice    O
)    O
.    O

Many    O
batch    O
jobs    O
are    O
run    O
in    O
parallel    B-Supercomputer104358117
and    O
JCL    O
is    O
used    O
to    O
control    O
the    O
operation    O
of    O
each    O
job    O
.    O

In    O
special    O
circumstances    O
,    O
such    O
as    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
or    O
embedded    O
computing    O
,    O
however    O
,    O
alternative    O
implementations    O
by    O
QR    O
or    O
even    O
the    O
use    O
of    O
an    O
explicit    O
inverse    O
might    O
be    O
preferable    O
,    O
and    O
custom    O
implementations    O
may    O
be    O
unavoidable    O
.    O

It    O
has    O
been    O
the    O
basis    O
for    O
pathbreaking    O
research    O
in    O
parallel    B-Supercomputer104358117
statistical    I-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117

Parallel    B-Supercomputer104358117
programming    I-Supercomputer104358117
language    I-Supercomputer104358117
such    O
as    O
the    O
open    O
-    O
source    O
X10    O
programming    O
language    O
are    O
designed    O
to    O
assist    O
with    O
this    O
task    O
.    O

The    O
Intel    O
Personal    O
SuperComputer    O
(    O
Intel    B-Supercomputer104358117
iPSC    I-Supercomputer104358117
)    O
was    O
a    O
product    O
line    O
of    O
parallel    B-Supercomputer104358117
computer    I-Supercomputer104358117
in    O
the    O
1980s    O
and    O
1990s    O
.    O

Topics    O
covered    O
:    O
concurrent    O
computing    O
,    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

While    O
this    O
method    O
has    O
many    O
advantages    O
–    O
robustness    O
,    O
easy    O
to    O
understand    O
,    O
highly    O
parallelizable    B-Supercomputer104358117
because    O
each    O
direction    O
can    O
be    O
computed    O
independently    O
,    O
versatile    O
(    O
there    O
exist    O
many    O
types    O
of    O
beamformers    O
to    O
include    O
various    O
types    O
of    O
hypothesis    O
)    O
,    O
relatively    O
fast    O
–    O
it    O
also    O
has    O
some    O
drawbacks    O
:    O
the    O
produced    O
acoustic    O
map    O
has    O
artifacts    O
(    O
also    O
called    O
side    O
lobes    O
or    O
ghost    O
sources    O
)    O
and    O
it    O
does    O
not    O
model    O
correctly    O
correlated    O
sound    O
sources    O
.    O

Its    O
solution    O
for    O
a    O
reasonably    O
large    O
number    O
of    O
particles    O
is    O
therefore    O
typically    O
impossible    O
,    O
even    O
for    O
modern    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
technology    O
in    O
a    O
reasonable    O
amount    O
of    O
time    O
.    O

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117

A    O
parallel    O
database    O
system    O
seeks    O
to    O
improve    O
performance    O
through    O
parallelization    B-Supercomputer104358117
of    O
various    O
operations    O
,    O
such    O
as    O
loading    O
data    O
,    O
building    O
indexes    O
and    O
evaluating    O
queries    O
.    O

The    O
International    O
Parallel    O
and    O
Distributed    O
Processing    O
Symposium    O
(    O
or    O
IPDPS    O
)    O
is    O
an    O
annual    O
conference    O
for    O
engineers    O
and    O
scientists    O
to    O
present    O
recent    O
findings    O
in    O
the    O
fields    O
of    O
parallel    B-Supercomputer104358117
processing    I-Supercomputer104358117
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

parallelism    B-Supercomputer104358117
become    O
the    O
dominant    O
computing    O
platform    O
(    O
through    O
the    O

The    O
main    O
assets    O
held    O
by    O
the    O
lattice    O
gas    O
model    O
are    O
that    O
the    O
boolean    O
states    O
mean    O
there    O
will    O
be    O
exact    O
computing    O
without    O
any    O
round    O
-    O
off    O
error    O
due    O
to    O
floating    O
-    O
point    O
precision    O
,    O
and    O
that    O
the    O
cellular    O
automata    O
system    O
makes    O
it    O
possible    O
to    O
run    O
lattice    O
gas    O
automaton    O
simulations    O
with    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

The    O
Karp    O
–    O
Flatt    O
metric    O
is    O
a    O
measure    O
of    O
parallelization    B-Supercomputer104358117
of    O
code    O
in    O
parallel    B-Supercomputer104358117
processor    I-Supercomputer104358117
systems    O
.    O

Data    O
parallelism    O
is    O
a    O
form    O
of    O
parallelization    O
across    O
multiple    O
processors    O
in    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
environments    O
.    O

Task    O
parallelism    O
(    O
also    O
known    O
as    O
function    O
parallelism    O
and    O
control    O
parallelism    O
)    O
is    O
a    O
form    O
of    O
parallelization    B-Supercomputer104358117
of    O
computer    O
code    O
across    O
multiple    O
processors    O
in    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
environments    O
.    O

Thread    O
-    O
level    O
parallelism    O
(    O
TLP    O
)    O
is    O
the    O
parallelism    B-Supercomputer104358117
inherent    O
in    O
an    O
application    O
that    O
runs    O
multiple    O
threads    O
at    O
once    O
.    O

Doing    O
so    O
provides    O
a    O
degree    O
of    O
parallelism    B-Supercomputer104358117
.    O

For    O
parallel    B-Supercomputer104358117
and    O
concurrent    O
processing    O
the    O
"    O
Parallel    O
Mode    O
"    O
horizontal    O
lines    O
or    O
a    O
horizontal    O
bar    O
indicate    O
the    O
start    O
or    O
end    O
of    O
a    O
section    O
of    O
processes    O
that    O
can    O
be    O
done    O
independently    O
:    O

The    O
school    O
offers    O
an    O
extensive    O
mathematics    O
and    O
science    O
curriculum    O
,    O
including    O
courses    O
in    O
artificial    O
intelligence    O
,    O
computer    O
vision    O
,    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
organic    O
chemistry    O
,    O
neurobiology    O
,    O
nanobiotechnology    O
,    O
marine    O
biology    O
,    O
DNA    O
science    O
,    O
signal    O
processing    O
,    O
computational    O
physics    O
,    O
and    O
quantum    O
mechanics    O
.    O

He    O
developed    O
integrated    O
circuits    O
for    O
early    O
parallel    B-Supercomputer104358117
computer    I-Supercomputer104358117
and    O
undertook    O
pioneering    O
work    O
in    O
high    O
performance    O
computer    O
image    O
generation    O
and    O
public    O
display    O
systems    O
.    O

pexec    O
is    O
a    O
command    O
-    O
line    O
utility    O
for    O
Linux    O
and    O
other    O
Unix    O
-    O
like    O
operating    O
systems    O
which    O
allows    O
the    O
user    O
to    O
execute    O
shell    O
commands    O
in    O
parallel    B-Supercomputer104358117
.    O

Loop    O
-    O
level    O
parallelism    O
is    O
a    O
form    O
of    O
parallelism    B-Supercomputer104358117
in    O
software    O
programming    O
that    O
is    O
concerned    O
with    O
extracting    O
parallel    O
tasks    O
from    O
loops    O
.    O

Vitanyi    O
has    O
worked    O
on    O
cellular    O
automata    O
,    O
computational    O
complexity    O
,    O
distributed    B-Supercomputer104358117
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
machine    O
learning    O
and    O
prediction    O
,    O
physics    O
of    O
computation    O
,    O
Kolmogorov    O
complexity    O
,    O
information    O
theory    O
and    O
quantum    O
computing    O
,    O
publishing    O
over    O
200    O
research    O
papers    O
and    O
some    O
books    O
.    O

One    O
of    O
a    O
series    O
of    O
research    O
machines    O
(    O
the    O
ILLIACs    O
from    O
the    O
University    O
of    O
Illinois    O
)    O
,    O
the    O
ILLIAC    O
IV    O
design    O
featured    O
fairly    O
high    O
parallelism    B-Supercomputer104358117
with    O
up    O
to    O
256    O
processors    O
,    O
used    O
to    O
allow    O
the    O
machine    O
to    O
work    O
on    O
large    O
data    O
sets    O
in    O
what    O
would    O
later    O
be    O
known    O
as    O
vector    O
processing    O
.    O

Another    O
solution    O
to    O
the    O
problem    O
was    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
;    O
building    O
a    O
computer    O
out    O
of    O
a    O
number    O
of    O
general    O
purpose    O
CPUs    O
.    O

in    O
1965    O
.    O
Because    O
Sollin    O
was    O
the    O
only    O
computer    O
scientist    O
in    O
this    O
list    O
living    O
in    O
an    O
English    O
speaking    O
country    O
,    O
this    O
algorithm    O
is    O
frequently    O
called    O
Sollin    O
's    O
algorithm    O
,    O
especially    O
in    O
the    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
literature    O
.    O

In    O
a    O
Hazelcast    O
grid    O
,    O
data    O
is    O
evenly    O
distributed    O
among    O
the    O
nodes    O
of    O
a    O
computer    B-Supercomputer104358117
cluster    I-Supercomputer104358117
,    O
allowing    O
for    O
horizontal    O
scaling    O
of    O
processing    B-Supercomputer104358117
and    O
available    O
storage    O
.    O

Efficient    O
parallel    B-Supercomputer104358117
processing    O
support    O
based    O
on    O
domain    O
decomposition    O
and    O
message    O
passing    O
paradigms    O
.    O

The    O
Fujitsu    O
FR    O
-    O
V    O
(    O
Fujitsu    O
RISC    O
-    O
VLIW    O
)    O
is    O
one    O
of    O
the    O
very    O
few    O
processors    O
ever    O
able    O
to    O
process    O
both    O
a    O
very    O
long    O
instruction    O
word    O
(    O
VLIW    O
)    O
and    O
vector    O
processor    O
instructions    O
at    O
the    O
same    O
time    O
,    O
increasing    O
throughput    O
with    O
high    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
while    O
increasing    O
performance    B-Supercomputer104358117
per    I-Supercomputer104358117
watt    I-Supercomputer104358117
and    O
hardware    O
efficiency    O
.    O

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117

This    O
is    O
a    O
parallel    B-Supercomputer104358117
task    O
distribution    O
and    O
collection    O
pattern    O
.    O

Her    O
areas    O
of    O
research    O
include    O
numerical    O
analysis    O
,    O
computational    O
fluid    O
dynamics    O
,    O
and    O
high    O
-    O
performance    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
mathematical    O
methods    O
for    O
research    O
of    O
program    O
fine    O
structures    O
,    O
methods    O
for    O
description    O
and    O
analysis    O
of    O
computer    O
architecture    O
,    O
parallel    B-Supercomputer104358117
programming    I-Supercomputer104358117
technology    O
,    O
program    O
optimization    O
methods    O
for    O
supercomputers    O
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
systems    O
,    O
the    O
Internet    O
-    O
based    O
technology    O
and    O
organization    O
of    O
distributed    O
computing    O
,    O
metacomputing    O
.    O

The    O
ABC    O
innovations    O
included    O
electronic    O
computation    O
,    O
binary    O
arithmetic    O
,    O
parallel    B-Supercomputer104358117
processing    I-Supercomputer104358117
,    O
regenerative    O
capacitor    O
memory    O
,    O
and    O
a    O
separation    O
of    O
memory    O
and    O
computing    O
functions    O
.    O

Because    O
APL    O
's    O
core    O
objects    O
are    O
arrays    O
,    O
it    O
lends    O
itself    O
well    O
to    O
parallelism    O
,    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
massively    O
parallel    O
applications    O
,    O
and    O
very    O
-    O
large    O
-    O
scale    O
integration    O
or    O
VLSI    O
.    O

Atomic    O
semantics    O
is    O
a    O
term    O
which    O
describes    O
a    O
type    O
of    O
guarantee    O
provided    O
by    O
a    O
data    O
register    O
shared    O
by    O
several    O
processors    O
in    O
a    O
parallel    B-Supercomputer104358117
machine    I-Supercomputer104358117
or    O
in    O
a    O
network    O
of    O
computers    O
working    O
together    O
.    O

More    O
importantly    O
,    O
since    O
the    O
recursive    O
calls    O
to    O
union    O
,    O
intersection    O
or    O
difference    O
are    O
independent    O
of    O
each    O
other    O
,    O
they    O
can    O
be    O
executed    O
in    B-Supercomputer104358117
parallel    I-Supercomputer104358117
with    O
a    O
parallel    O
depth    O
.    O

Amdahl    O
's    O
law    O
is    O
often    O
used    O
in    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
to    O
predict    O
the    O
theoretical    O
speedup    O
when    O
using    O
multiple    O
processors    O
.    O

The    O
Scalable    O
Computing    O
Laboratory    O
was    O
established    O
to    O
find    O
ways    O
of    O
making    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
accessible    O
and    O
cost    O
-    O
effective    O
for    O
the    O
scientific    O
community    O
.    O

In    O
computing    O
,    O
MISD    O
(    O
multiple    O
instruction    O
,    O
single    O
data    O
)    O
is    O
a    O
type    O
of    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
architecture    O
where    O
many    O
functional    O
units    O
perform    O
different    O
operations    O
on    O
the    O
same    O
data    O
.    O

In    O
a    O
typical    O
systolic    O
array    O
,    O
parallel    B-Supercomputer104358117
input    O
data    O
flows    O
through    O
a    O
network    O
of    O
hard    O
-    O
wired    O
processor    O
nodes    O
,    O
resembling    O
the    O
human    O
brain    O
which    O
combine    O
,    O
process    O
,    O
merge    O
or    O
sort    O
the    O
input    O
data    O
into    O
a    O
derived    O
result    O
.    O

Systolic    O
arrays    O
are    O
often    O
hard    O
-    O
wired    O
for    O
a    O
specific    O
operation    O
,    O
such    O
as    O
"    O
multiply    O
and    O
accumulate    O
"    O
,    O
to    O
perform    O
massively    O
parallel    B-Supercomputer104358117
integration    O
,    O
convolution    O
,    O
correlation    O
,    O
matrix    O
multiplication    O
or    O
data    O
sorting    O
tasks    O
.    O

Depending    O
on    O
the    O
language    O
used    O
,    O
multiple    O
COMEFROMs    O
referencing    O
the    O
same    O
departure    O
point    O
may    O
be    O
invalid    O
,    O
be    O
non    O
-    O
deterministic    O
,    O
be    O
executed    O
in    O
some    O
sort    O
of    O
defined    O
priority    O
,    O
or    O
even    O
induce    O
parallel    B-Supercomputer104358117
or    O
otherwise    O
concurrent    O
execution    O
as    O
seen    O
in    O
Threaded    O
Intercal    O
.    O

His    O
thesis    O
described    O
parallel    B-Supercomputer104358117
programming    I-Supercomputer104358117
language    I-Supercomputer104358117
,    O
parallel    B-Supercomputer104358117
processor    I-Supercomputer104358117
computer    I-Supercomputer104358117
and    O
a    O
basis    O
for    O
a    O
network    O
architecture    O
,    O
which    O
orients    O
itself    O
at    O
data    O
flow    O
.    O

Unified    O
Parallel    O
C    O
(    O
UPC    O
)    O
is    O
an    O
extension    O
of    O
the    O
C    O
programming    O
language    O
designed    O
for    O
high    O
-    O
performance    O
computing    O
on    O
large    O
-    O
scale    O
parallel    B-Supercomputer104358117
machine    I-Supercomputer104358117
,    O
including    O
those    O
with    O
a    O
common    O
global    O
address    O
space    O
(    O
SMP    O
and    O
NUMA    B-Supercomputer104358117
)    O
and    O
those    O
with    O
distributed    O
memory    O
(    O
e.g.    O
clusters    B-Supercomputer104358117
)    O
.    O

Parallel    O
rendering    O
(    O
or    O
Distributed    O
rendering    O
)    O
is    O
the    O
application    O
of    O
parallel    B-Supercomputer104358117
programming    I-Supercomputer104358117
to    O
the    O
computational    O
domain    O
of    O
computer    O
graphics    O
.    O

However    O
,    O
Barron    O
's    O
long    O
-    O
term    O
aim    O
was    O
to    O
produce    O
an    O
innovative    O
microprocessor    O
architecture    O
intended    O
for    O
parallel    B-Supercomputer104358117
processing    I-Supercomputer104358117
,    O
the    O
"    O
transputer    O
"    O
.    O

Parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117

The    O
June    O
2013    O
internships    O
included    O
seven    O
participants    O
contributing    O
to    O
the    O
Linux    O
kernel    O
,    O
for    O
example    O
working    O
on    O
parallelizing    B-Supercomputer104358117
the    O
x86    O
boot    O
process    O
.    O

In    O
particular    O
,    O
he    O
has    O
written    O
books    O
on    O
automata    O
theory    O
and    O
the    O
semantics    O
of    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

Computational    O
electromagnetics    O
(    O
CEM    O
)    O
,    O
fast    O
solvers    O
,    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
electromagnetic    O
theory    O
are    O
Prof    O
.    O

In    O
August    O
2011    O
,    O
AMD    O
released    O
version    O
2.5    O
of    O
the    O
ATI    O
APP    O
Software    O
Development    O
Kit    O
,    O
which    O
includes    O
support    O
for    O
OpenCL    O
1.1    O
,    O
a    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
language    O
developed    O
by    O
the    O
Khronos    O
Group    O
.    O

It    O
is    O
intended    O
to    O
facilliate    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
on    O
multi    O
-    O
core    O
processors    O
,    O
GPU    O
,    O
Grid    O
and    O
Cloud    O
.    O

EKA    B-Supercomputer104358117
is    O
a    O
supercomputer    O
built    O
by    O
the    O
Computational    O
Research    O
Laboratories    O
(    O
a    O
subsidiary    O
of    O
Tata    O
Sons    O
)    O
with    O
technical    O
assistance    O
and    O
hardware    O
provided    O
by    O
Hewlett    O
-    O
Packard    O
.    O

1997    O
–    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
,    O
a    O
chess    O
-    O
playing    O
supercomputer    O
,    O
defeats    O
Garry    O
Kasparov    O
in    O
the    O
last    O
game    O
of    O
the    O
rematch    O
,    O
becoming    O
the    O
first    O
computer    O
to    O
beat    O
a    O
world    O
-    O
champion    O
chess    O
player    O
in    O
a    O
classic    O
match    O
format    O
.    O

For    O
example    O
,    O
the    O
chess    O
computer    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
(    O
the    O
first    O
one    O
to    O
beat    O
a    O
reigning    O
world    O
champion    O
,    O
Garry    O
Kasparov    O
at    O
that    O
time    O
)    O
looked    O
ahead    O
at    O
least    O
12    O
plies    O
,    O
then    O
applied    O
a    O
heuristic    O
evaluation    O
function    O
.    O

HiTech    O
was    O
the    O
highest    O
ranked    O
chess    O
machine    O
for    O
some    O
time    O
in    O
mid-1980s    O
until    O
it    O
was    O
surpassed    O
by    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
.    O

Hsu    O
was    O
the    O
architect    O
and    O
the    O
principal    O
designer    O
of    O
the    O
IBM    B-Supercomputer104358117
Deep    I-Supercomputer104358117
Blue    I-Supercomputer104358117
chess    O
computer    O
.    O

Prior    O
to    O
building    O
the    O
supercomputer    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
that    O
defeated    O
Kasparov    O
,    O
Hsu    O
worked    O
on    O
many    O
other    O
chess    O
computers    O
.    O

Hsu    O
went    O
on    O
to    O
build    O
the    O
successively    O
better    O
chess    O
-    O
playing    O
computers    O
Deep    O
Thought    O
,    O
Deep    B-Supercomputer104358117
Thought    I-Supercomputer104358117
II    I-Supercomputer104358117
,    O
and    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
Prototype    I-Supercomputer104358117
.    O

Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
,    O
another    O
chess    O
computer    O
co    O
-    O
developed    O
by    O
Feng    O
-    O
hsiung    O
Hsu    O
,    O
being    O
the    O
first    O
computer    O
to    O
win    O
a    O
chess    O
match    O
against    O
the    O
world    O
champion    O

The    O
overall    O
architecture    O
of    O
Belle    O
was    O
used    O
for    O
the    O
initial    O
designs    O
of    O
ChipTest    O
,    O
the    O
progenitor    O
of    O
IBM    B-Supercomputer104358117
Deep    I-Supercomputer104358117
Blue    I-Supercomputer104358117
.    O

Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117

It    O
is    O
the    O
predecessor    O
of    O
Deep    O
Thought    O
which    O
in    O
turn    O
evolved    O
into    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
.    O

Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
(chess    I-Supercomputer104358117
computer)    I-Supercomputer104358117
,    O
another    O
chess    O
computer    O
developed    O
by    O
Feng    O
-    O
hsiung    O
Hsu    O
,    O
being    O
the    O
first    O
computer    O
to    O
win    O
a    O
chess    O
match    O
against    O
the    O
world    O
champion    O

HiTech    O
was    O
one    O
of    O
two    O
competing    O
chess    O
projects    O
at    O
Carnegie    O
Mellon    O
;    O
the    O
one    O
that    O
would    O
succeed    O
in    O
the    O
quest    O
of    O
beating    O
the    O
World    O
Chess    O
Champion    O
was    O
its    O
rival    O
ChipTest    O
(    O
later    O
Deep    O
Thought    O
and    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
)    O
.    O

Feng    O
-    O
hsiung    O
Hsu    O
(    O
許峰雄    O
)    O
-    O
IBM    O
developer    O
of    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
,    O
which    O
beat    O
World    O
Chess    O
Champion    O
Garry    O
Kasparov    O
in    O
1997    O

IBM    O
's    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
computer    I-Supercomputer104358117
defeated    O
world    O
chess    O
champion    O
Garry    O
Kasparov    O
in    O
1997    O
.    O

Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
,    O
a    O
chess    O
-    O
playing    O
computer    O
developed    O
by    O
IBM    O
which    O
beat    O
Garry    O
Kasparov    O
in    O
1997    O
.    O

Since    O
Deep    B-Supercomputer104358117
Blue    I-Supercomputer104358117
's    O
victory    O
over    O
Garry    O
Kasparov    O
in    O
chess    O
in    O
1997    O
,    O
IBM    O
had    O
been    O
on    O
the    O
hunt    O
for    O
a    O
new    O
challenge    O
.    O

Cray    O
immediately    O
turned    O
his    O
attention    O
to    O
its    O
replacement    O
,    O
this    O
time    O
setting    O
a    O
goal    O
of    O
10    O
times    O
the    O
performance    O
of    O
the    O
6600    O
,    O
delivered    O
as    O
the    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
.    O

The    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
was    O
originally    O
intended    O
to    O
be    O
fully    O
compatible    O
with    O
the    O
existing    O
6000-series    O
machines    O
as    O
well    O
,    O
it    O
started    O
life    O
known    O
as    O
the    O
CDC    O
6800    O
.    O

Discussion    O
topics    O
include    O
CDC    O
1604    O
,    O
CDC    O
6600    O
,    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
8600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117
and    O
Seymour    O
Cray    O
.    O

Cray    O
had    O
a    O
string    O
of    O
successes    O
at    O
CDC    O
,    O
including    O
the    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
and    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
.    O

Pipelining    O
was    O
a    O
major    O
feature    O
of    O
Seymour    O
Cray    O
's    O
groundbreaking    O
design    O
,    O
the    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
,    O
which    O
outperformed    O
almost    O
all    O
other    O
machines    O
by    O
about    O
ten    O
times    O
when    O
it    O
was    O
introduced    O
.    O

On    O
problems    O
that    O
could    O
be    O
parallelized    O
the    O
machine    O
was    O
still    O
the    O
fastest    O
in    O
the    O
world    O
,    O
outperforming    O
the    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
by    O
two    O
to    O
six    O
times    O
,    O
and    O
it    O
is    O
generally    O
credited    O
as    O
the    O
fastest    O
machine    O
in    O
the    O
world    O
until    O
1981    O
.    O

The    O
Magerit    B-Supercomputer104358117
computer    O
at    O
the    O
Supercomputing    O
and    O
Visualization    O
Center    O
of    O
Madrid    O
.    O

Magerit    B-Supercomputer104358117
is    O
the    O
name    O
of    O
the    O
one    O
of    O
the    O
most    O
powerful    O
supercomputers    O
in    O
Spain    O
.    O

He    O
also    O
chaired    O
the    O
University    O
of    O
Edinburgh    O
,    O
Edinburgh    O
Parallel    O
Computing    O
Centre    O
which    O
attracted    O
the    O
UK    O
National    O
Supercomputing    O
Service    O
in    O
1994    O
,    O
running    O
today    O
using    O
the    O
ARCHER    O
,    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
and    O
HECTOR    O
supercomputers    O
.    O

More    O
recently    O
,    O
IBM    O
announced    O
in    O
2011    O
that    O
Blue    B-Supercomputer104358117
Gene/Q    I-Supercomputer104358117
had    O
hardware    O
support    O
for    O
both    O
transactional    O
memory    O
and    O
speculative    O
multithreading    O
.    O

Blue    B-Supercomputer104358117
Gene/Q    I-Supercomputer104358117
processor    O
from    O
IBM    O
(    O
Sequoia    O
supercomputer    O
)    O

Blue    B-Supercomputer104358117
Gene/Q    I-Supercomputer104358117
:    O
As    O
of    O
2013    O
,    O
this    O
system    O
consists    O
of    O
6144    O
compute    O
nodes    O
housed    O
in    O
6    O
frames    O
.    O

Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
:    O
Launched    O
in    O
2005    O
,    O
EPCC    O
's    O
Blue    O
Gene    O
/    O
L    O
was    O
the    O
first    O
Blue    O
Gene    O
system    O
available    O
outside    O
the    O
United    O
States    O
.    O

Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117

Babel    O
works    O
on    O
all    O
known    O
POSIX    O
and    O
Unix    O
variants    O
,    O
including    O
Linux    O
,    O
Mac    O
OS    O
X    O
,    O
AIX    O
,    O
IRIX    O
,    O
Solaris    O
,    O
Tru64    O
,    O
Cray    O
's    O
XT4    B-Supercomputer104358117
,    O
IBM    O
's    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
,    O
and    O
many    O
commodity    O
clusters    B-Supercomputer104358117
.    O

Lattice    O
QCD    O
has    O
also    O
been    O
used    O
as    O
a    O
benchmark    O
for    O
high    O
-    O
performance    O
computing    O
,    O
an    O
approach    O
originally    O
developed    O
in    O
the    O
context    O
of    O
the    O
IBM    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
supercomputer    O
.    O

As    O
of    O
November    O
2010    O
,    O
the    O
center    O
houses    O
three    O
TOP500    B-Supercomputer104358117
supercomputers    O
;    O
the    O
oldest    O
and    O
still    O
fastest    O
of    O
which    O
,    O
a    O
BlueGene/L    B-Supercomputer104358117
system    O
designed    O
for    O
protein    O
folding    O
simulations    O
,    O
called    O
"    O
BGW    O
"    O
(    O
Blue    O
Gene    O
Watson    O
)    O
,    O
entered    O
the    O
list    O
in    O
the    O
06/2005    O
issue    O
,    O
then    O
positioned    O
second    O
behind    O
fellow    O
Blue    O
Gene    O
/    O
L    O
in    O
LLNL    O
.    O

To    O
predict    O
protein    O
structure    O
"    O
de    O
novo    O
"    O
for    O
larger    O
proteins    O
will    O
require    O
better    O
algorithms    O
and    O
larger    O
computational    O
resources    O
like    O
those    O
afforded    O
by    O
either    O
powerful    O
supercomputers    O
(    O
such    O
as    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
or    O
MDGRAPE-3    B-Supercomputer104358117
)    O
or    O
distributed    O
computing    O
(    O
such    O
as    O
Folding@home    O
,    O
the    O
Human    O
Proteome    O
Folding    O
Project    O
and    O
Rosetta@Home    O
)    O
.    O

blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117

INK    O
(    O
for    O
I    O
/    O
O    O
Node    O
Kernel    O
)    O
is    O
the    O
operating    O
system    O
that    O
runs    O
on    O
the    O
input    O
output    O
nodes    O
of    O
the    O
IBM    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
supercomputer    O
.    O

Initial    O
work    O
on    O
building    O
a    O
complete    O
biochemical    O
model    O
of    O
cellular    O
behavior    O
is    O
underway    O
as    O
part    O
of    O
a    O
number    O
of    O
different    O
research    O
projects    O
,    O
namely    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
which    O
seeks    O
to    O
understand    O
the    O
mechanisms    O
behind    O
protein    O
folding    O
.    O

Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117

The    O
Banyan    O
VINES    O
,    O
AppleTalk    O
,    O
ServerNet    B-Supercomputer104358117
,    O
IPX    O
/    O
SPX    O
,    O
Giganet    O
,    O
and    O
RPC    O
Net    O
-    O
Libs    O
were    O
dropped    O
from    O
MDAC    O
2.5    O
onwards    O
.    O

Loongson    B-Supercomputer104358117
,    O
another    O
Chinese    O
developed    O
architecture    O

The    O
source    O
code    O
also    O
unofficially    O
supports    O
a    O
wide    O
variety    O
of    O
operating    O
systems    O
and    O
platforms    O
,    O
including    O
Raspberry    O
Pi    O
,    O
Loongson    B-Supercomputer104358117
,    O
Maemo    O
,    O
Meego    O
Harmattan    O
and    O
Pandora    O
.    O

Lemote    O
,    O
with    O
partners    O
,    O
provides    O
integrated    O
circuit    O
design    O
services    O
,    O
helps    O
develop    O
the    O
Loongson    B-Supercomputer104358117
series    O
of    O
MIPS    O
-    O
based    O
RISC    O
microprocessors    O
,    O
and    O
builds    O
small    O
form    O
factor    O
computers    O
including    O
network    O
computers    O
and    O
netbooks    O
.    O

Dragon    B-Supercomputer104358117
chip    I-Supercomputer104358117
,    O
a    O
line    O
of    O
Chinese    O
CPUs    O

As    O
an    O
ongoing    O
program    O
,    O
it    O
has    O
produced    O
several    O
notable    O
developments    O
including    O
the    O
Loongson    B-Supercomputer104358117
computer    O
processor    O
family    O
(    O
originally    O
named    O
"    O
Godson    O
"    O
)    O
,    O
the    O
Tianhe    B-Supercomputer104358117
supercomputers    O
,    O
and    O
aspects    O
of    O
the    O
Shenzhou    O
spacecraft    O
.    O

Cray    B-Supercomputer104358117
CX1    I-Supercomputer104358117

Cielo,    B-Supercomputer104358117
which    O
is    O
also    O
located    O
in    O
Los    O
Alamos    O
National    O
Laboratory    O
,    O
is    O
57th    O
on    O
the    O
TOP500    O
list    O
for    O
most    O
powerful    O
computer    O
.    O

Cyclops64    B-Supercomputer104358117
,    O
an    O
IBM    O
supercomputer    O
architecture    O

The    O
Quadrics    O
name    O
was    O
first    O
used    O
in    O
1993    O
for    O
a    O
commercialized    O
version    O
of    O
the    O
APE100    B-Supercomputer104358117
SIMD    O
parallel    B-Supercomputer104358117
computer    I-Supercomputer104358117
produced    O
by    O
Alenia    O
Spazio    O
and    O
originally    O
developed    O
by    O
INFN    O
,    O
the    O
Italian    O
National    O
Institute    O
of    O
Nuclear    O
Physics    O
.    O

QCDOC    B-Supercomputer104358117
:    O
One    O
of    O
the    O
world    O
’s    O
most    O
powerful    O
systems    O
dedicated    O
to    O
the    O
numerical    O
investigation    O
of    O
quantum    O
chromodynamics    O
,    O
which    O
describes    O
the    O
interactions    O
between    O
quarks    O
and    O
gluons    O
.    O

The    O
company    O
's    O
first    O
supercomputer    O
product    O
,    O
named    O
MTA    B-Supercomputer104358117
,    O
featured    O
interleaved    O
multi    O
-    O
threading    O
,    O
i.e.    O
a    O
barrel    O
processor    O
.    O

CDC    O
1604    O
et    O
al    O
-    O
1604    O
,    O
1604-A    O
,    O
1604-B    O
,    O
1604-C    O
,    O
924    O
(    O
a    O
"    O
cut    O
down    O
"    O
1604    O
sibling    O
)    O
CDC    O
160    O
series    O
-    O
160    O
,    O
160A    O
(    O
160-A    O
)    O
,    O
160    O
G    O
(    O
160-G    O
)    O
CDC    O
3000    O
series    O
-    O
3100    O
,    O
3200    O
,    O
3300    O
,    O
3400    O
,    O
3500    O
,    O
3600    O
,    O
3800    O
CDC    B-Supercomputer104358117
6000    I-Supercomputer104358117
series    I-Supercomputer104358117
-    O
6200    O
,    O
6400    O
,    O
6500    O
,    O
6700    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
CDC    B-Supercomputer104358117
CYBER    I-Supercomputer104358117
-    O
17    O
,    O
18    O
,    O
71    O
,    O
72    O
,    O
73    O
,    O
74    O
,    O
76    O
,    O
170    O
,    O
171    O
,    O
172    O
,    O
173    O
,    O
174    O
,    O
175    O
,    O
176    O
,    O
203    O
,    O
205    O
,    O
Omega/480    O
,    O
700    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117

1963    O
–    O
160A    O
(    O
160-A    O
)    O
,    O
1604-A    O
,    O
3400    O
,    O
6600    B-Supercomputer104358117

Discussion    O
topics    O
include    O
CDC    O
1604    O
,    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
8600    I-Supercomputer104358117
,    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117
and    O
Seymour    O
Cray    O
.    O

The    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
was    O
the    O
flagship    O
mainframe    O
supercomputer    O
of    O
the    O
6000    B-Supercomputer104358117
series    I-Supercomputer104358117
of    O
computer    O
systems    O
manufactured    O
by    O
Control    O
Data    O
Corporation    O
.    O

Many    O
early    O
computers    O
,    O
including    O
the    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
,    O
the    O
LINC    O
,    O
the    O
PDP-1    O
,    O
and    O
the    O
UNIVAC    O
1107    O
,    O
use    O
ones    O
'    O
complement    O
notation    O
;    O
the    O
descendants    O
of    O
the    O
UNIVAC    O
1107    O
,    O
the    O
UNIVAC    O
1100/2200    O
series    O
,    O
continue    O
to    O
do    O
so    O
.    O

For    O
example    O
,    O
the    O
operator    O
consoles    O
of    O
the    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
formed    O
each    O
letter    O
all    O
at    O
once    O
by    O
sending    O
the    O
Charactron    O
CRT    O
electron    O
beam    O
through    O
a    O
metallic    O
stencil    O
mask    O
with    O
an    O
A    O
-    O
shaped    O
hole    O
,    O
or    O
through    O
a    O
B    O
-    O
shaped    O
hole    O
,    O
etc    O
.    O

Cray    O
had    O
a    O
string    O
of    O
successes    O
at    O
CDC    O
,    O
including    O
the    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
and    O
CDC    B-Supercomputer104358117
7600    I-Supercomputer104358117
.    O

The    O
tower    O
had    O
a    O
patented    O
hyperboloid    O
design    O
,    O
which    O
was    O
checked    O
by    O
computer    O
simulation    O
on    O
a    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
mainframe    O
to    O
verify    O
its    O
integrity    O
.    O

Scoreboarding    O
is    O
a    O
centralized    O
method    O
,    O
used    O
in    O
the    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
computer    O
,    O
for    O
dynamically    O
scheduling    O
a    O
pipeline    O
so    O
that    O
the    O
instructions    O
can    O
execute    O
out    O
of    O
order    O
when    O
there    O
are    O
no    O
conflicts    O
and    O
the    O
hardware    O
is    O
available    O
.    O

In    O
1970    O
,    O
TS    O
organized    O
the    O
takeover    O
and    O
occupation    O
of    O
NYU    O
's    O
Courant    O
Institute    O
where    O
they    O
held    O
a    O
$    O
3.5    O
million    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
computer    O
hostage    O
(    O
equivalent    O
to    O
$    O
19.4    O
million    O
in    O
2010    O
dollars    O
)    O
,    O
demanding    O
$    O
100,000    O
ransom    O
to    O
be    O
used    O
for    O
bail    O
for    O
the    O
"    O
Panther    O
21    O
"    O
.    O

2006    O
Announced    O
the    O
SX-8R    B-Supercomputer104358117

Anton    B-Supercomputer104358117
is    O
a    O
massively    O
parallel    O
supercomputer    O
designed    O
and    O
built    O
by    O
D.    O
E.    O
Shaw    O
Research    O
in    O
New    O
York    O
.    O

Wipro    B-Supercomputer104358117
Supernova    I-Supercomputer104358117

SX    B-Supercomputer104358117
architecture    I-Supercomputer104358117

;    O
MathKeisan    O
:    O
NEC    O
's    O
math    O
library    O
,    O
supporting    O
NEC    B-Supercomputer104358117
SX    I-Supercomputer104358117
architecture    I-Supercomputer104358117
under    O
SUPER-UX    B-Supercomputer104358117
,    O
and    O
Itanium    O
under    O
Linux    O

;    O
PDLIB    O
/    O
SX    O
:    O
NEC    O
's    O
Public    O
Domain    O
Mathematical    O
Library    O
for    O
the    O
NEC    O
SX-4    B-Supercomputer104358117
system    O
.    O

EPCC    O
has    O
hosted    O
a    O
variety    O
of    O
supercomputers    O
over    O
the    O
years    O
,    O
including    O
several    O
Meiko    O
Computing    O
Surfaces    O
,    O
a    O
Thinking    O
Machines    O
CM-200    O
Connection    B-Supercomputer104358117
Machine    I-Supercomputer104358117
,    O
and    O
a    O
number    O
of    O
Cray    O
systems    O
including    O
a    O
Cray    B-Supercomputer104358117
T3D    I-Supercomputer104358117
and    O
T3E    B-Supercomputer104358117

The    O
Cray    B-Supercomputer104358117
T3E    I-Supercomputer104358117
was    O
Cray    O
Research    O
's    O
second    O
-    O
generation    O
massively    O
parallel    O
supercomputer    O
architecture    O
,    O
launched    O
in    O
late    O
November    O
1995    O
.    O

Cray    O
eventually    O
realized    O
that    O
the    O
approach    O
was    O
likely    O
the    O
only    O
way    O
forward    O
and    O
started    O
a    O
five    O
-    O
year    O
project    O
to    O
capture    O
the    O
lead    O
in    O
this    O
area    O
:    O
the    O
plan    O
's    O
result    O
was    O
the    O
DEC    O
Alpha    O
-    O
based    O
Cray    B-Supercomputer104358117
T3D    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
T3E    I-Supercomputer104358117
series    O
,    O
which    O
left    O
Cray    O
as    O
the    O
only    O
remaining    O
supercomputer    O
vendor    O
in    O
the    O
market    O
besides    O
NEC    B-Supercomputer104358117
by    O
2000    O
.    O

ScREC    B-Supercomputer104358117
,    O
the    O
supercomputing    O
facility    O
at    O
RCMS    O

DEC    O
's    O
founder    O
,    O
Ken    O
Olsen    O
,    O
had    O
worked    O
with    O
both    O
it    O
and    O
a    O
still    O
earlier    O
computer    O
,    O
the    O
18-bit    O
64,000-word    O
TX-0    B-Supercomputer104358117
,    O
at    O
MIT    O
's    O
Lincoln    O
Laboratory    O
.    O

That    O
view    O
carried    O
over    O
into    O
his    O
designs    O
for    O
the    O
TX-0    B-Supercomputer104358117
and    O
TX-2    O
and    O
the    O
LINC    O
.    O

one    O
of    O
the    O
fathers    O
of    O
the    O
personal    O
computer    O
...    O
he    O
was    O
the    O
architect    O
of    O
both    O
the    O
TX-0    B-Supercomputer104358117
and    O
TX-2    O
at    O
Lincoln    O
Labs    O
.    O

Dennis    O
describes    O
his    O
educational    O
background    O
and    O
work    O
in    O
time    O
-    O
sharing    O
computer    O
systems    O
at    O
the    O
Massachusetts    O
Institute    O
of    O
Technology    O
(    O
MIT    O
)    O
,    O
including    O
the    O
TX-0    B-Supercomputer104358117
computer    O
,    O
the    O
work    O
of    O
John    O
McCarthy    O
on    O
time    O
-    O
sharing    O
,    O
and    O
the    O
influence    O
of    O
the    O
Information    O
Processing    O
Techniques    O
Office    O
of    O
the    O
Advanced    O
Research    O
Projects    O
Agency    O
.    O

Other    O
early    O
transistorized    O
computers    O
included    O
TRADIC    O
,    O
Harwell    O
CADET    O
and    O
TX-0    B-Supercomputer104358117
.    O

Compute    O
Node    O
Linux    O
(    O
CNL    O
)    O
is    O
a    O
runtime    O
environment    O
based    O
on    O
the    O
Linux    O
kernel    O
for    O
the    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT5    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT6    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XE6    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
XK6    I-Supercomputer104358117
supercomputer    O
systems    O
based    O
on    O
SUSE    O
Linux    O
Enterprise    O
Server    O
.    O

In    O
May    O
2010    O
,    O
the    O
Cray    B-Supercomputer104358117
XE6    I-Supercomputer104358117
supercomputer    O
was    O
announced    O
.    O

The    O
QPACE    O
cooling    O
solution    O
also    O
influenced    O
other    O
supercomputer    O
designs    O
such    O
as    O
SuperMUC    B-Supercomputer104358117
.    O

SuperMUC    B-Supercomputer104358117
is    O
the    O
name    O
of    O
a    O
supercomputer    O
of    O
the    O
Leibniz    O
Supercomputing    O
Centre    O
(    O
LRZ    O
)    O
of    O
the    O
Bavarian    O
Academy    O
of    O
Sciences    O
.    O

On    O
October    O
4    O
,    O
2004    O
,    O
the    O
company    O
announced    O
the    O
Cray    B-Supercomputer104358117
XD1    I-Supercomputer104358117
range    O
of    O
entry    O
-    O
level    O
supercomputers    O
which    O
use    O
dual    O
-    O
core    O
64-bit    O
AMD    O
Opteron    O
CPUs    O
running    O
Linux    O
.    O

As    O
of    O
November    O
2010    O
,    O
the    O
center    O
houses    O
three    O
TOP500    B-Supercomputer104358117
supercomputers    O
;    O
the    O
oldest    O
and    O
still    O
fastest    O
of    O
which    O
,    O
a    O
BlueGene/L    B-Supercomputer104358117
system    O
designed    O
for    O
protein    O
folding    O
simulations    O
,    O
called    O
"    O
BGW    O
"    O
(    O
Blue    O
Gene    O
Watson    O
)    O
,    O
entered    O
the    O
list    O
in    O
the    O
06/2005    O
issue    O
,    O
then    O
positioned    O
second    O
behind    O
fellow    O
Blue    O
Gene    O
/    O
L    O
in    O
LLNL    O
.    O

In    O
June    O
2004    O
,    O
Kabru    O
was    O
listed    O
as    O
#    O
264    O
in    O
the    O
TOP500    B-Supercomputer104358117
list    O
of    O
the    O
world    O
's    O
most    O
powerful    O
computers    O
.    O

12    O
monthly    O
issues    O
and    O
4    O
bonus    O
issues    O
:    O
a    O
special    O
wine    O
issue    O
,    O
The    O
NorthBay    O
biz    O
500    O
(    O
a    O
complete    O
listing    O
of    O
the    O
top    B-Supercomputer104358117
500    I-Supercomputer104358117
companies    O
in    O
the    O
North    O
Bay    O
)    O
and    O
a    O
"    O
Best    O
of    O
"    O
issue    O
(    O
a    O
readers    O
poll    O
of    O
the    O
best    O
businesses    O
in    O
the    O
North    O
Bay    O
)    O
.    O

systems    O
running    O
CNL    O
were    O
ranked    O
3rd    O
,    O
6th    O
and    O
8th    O
among    O
the    B-Supercomputer104358117
fastest    I-Supercomputer104358117
supercomputers    I-Supercomputer104358117
in    O
the    O
world    O
.    O

The    O
Sunway    O
TaihuLight    O
(    O
,    O
"    O
Shénwēi·tàihú    O
zhī    O
guāng    O
"    O
)    O
is    O
a    O
Chinese    O
supercomputer    O
which    O
,    O
,    O
is    O
ranked    O
number    O
one    O
in    O
the    O
TOP500    B-Supercomputer104358117
list    O
as    O
the    O
fastest    O
supercomputer    O
in    O
the    O
world    O
,    O
with    O
a    O
LINPACK    O
benchmark    O
rating    O
of    O
93    O
petaflops    O
.    O

It    O
ranked    O
#    O
2    O
in    O
the    O
2011    O
China    O
HPC    O
Top100    O
,    O
#    O
14    O
on    O
the    O
November    O
2011    O
TOP500    B-Supercomputer104358117
list    O
,    O
and    O
#    O
39    O
on    O
the    O
November    O
2011    O
Green500    O
List    O
.    O

Top500    B-Supercomputer104358117

The    O
SW26010    O
is    O
used    O
in    O
the    O
Sunway    O
TaihuLight    O
supercomputer    O
,    O
which    O
,    O
,    O
is    O
the    O
world    O
's    O
fastest    O
supercomputer    O
as    O
ranked    O
by    O
the    O
TOP500    B-Supercomputer104358117
project    O
.    O

TOP500    B-Supercomputer104358117

It    O
stood    O
out    O
at    O
the    O
top    O
of    O
the    O
TOP500    B-Supercomputer104358117
during    O
1993    O
-    O
1996    O
.    O

In    O
November    O
2005    O
,    O
SimCenter    O
was    O
listed    O
as    O
the    O
89th    O
most    O
powerful    O
supercomputer    O
by    O
Top500    B-Supercomputer104358117
.    O

An    O
Itanium    O
-    O
based    O
computer    O
first    O
appeared    O
on    O
the    O
list    O
of    O
the    O
TOP500    B-Supercomputer104358117
supercomputers    O
in    O
November    O
2001    O
.    O

November    O
:    O
IBM    O
's    O
320-processor    O
Titan    O
NOW    O
Cluster    O
at    O
National    O
Center    O
for    O
Supercomputing    O
Applications    O
is    O
listed    O
on    O
the    O
TOP500    B-Supercomputer104358117
list    O
at    O
position    O
#    O
34    O
.    O

June    O
:    O
"    O
Thunder    O
"    O
,    O
a    O
system    O
at    O
LLNL    O
with    O
4096    O
Itanium    O
2    O
processors    O
,    O
is    O
listed    O
on    O
the    O
TOP500    B-Supercomputer104358117
list    O
at    O
position    O
#    O
2    O
.    O

November    O
:    O
"    O
Columbia    B-Supercomputer104358117
"    O
,    O
an    O
SGI    O
Altix    B-Supercomputer104358117
3700    O
with    O
10160    O
Itanium    O
2    O
processors    O
at    O
NASA    O
Ames    O
Research    O
Center    O
,    O
is    O
listed    O
on    O
the    O
TOP500    B-Supercomputer104358117
list    O
at    O
position    O
#    O
2    O
.    O

Area    O
chart    O
showing    O
the    O
representation    O
of    O
different    O
families    O
of    O
micro    O
-    O
processors    O
in    O
the    O
TOP500    B-Supercomputer104358117
ranking    O
list    O
of    O
supercomputer    O
(    O
1993–2015    O
)    O
.    O

Estimates    O
of    O
how    O
much    O
processing    O
power    O
is    O
needed    O
to    O
emulate    O
a    O
human    O
brain    O
at    O
various    O
levels    O
(    O
from    O
Ray    O
Kurzweil    O
and    O
the    O
chart    O
to    O
the    O
left    O
)    O
,    O
along    O
with    O
the    O
fastest    O
supercomputer    O
from    O
TOP500    B-Supercomputer104358117
mapped    O
by    O
year    O
.    O

Several    O
Cray    O
supercomputer    O
systems    O
are    O
listed    O
in    O
the    O
TOP500    B-Supercomputer104358117
,    O
which    O
ranks    O
the    O
most    O
powerful    O
supercomputers    O
in    O
the    O
world    O
.    O

In    O
June    O
2011    O
,    O
TOP500    B-Supercomputer104358117
ranked    O
K    O
the    O
world    O
's    O
fastest    O
supercomputer    O
,    O
with    O
a    O
computation    O
speed    O
of    O
over    O
8    O
petaflops    O
,    O
and    O
in    O
November    O
2011    O
,    O
K    O
became    O
the    O
first    O
computer    O
to    O
top    O
10    O
petaflops    O
.    O

On    O
20    O
June    O
2011    O
,    O
the    O
TOP500    B-Supercomputer104358117
Project    O
Committee    O
announced    O
that    O
K    O
had    O
set    O
a    O
LINPACK    O
record    O
with    O
a    O
performance    O
of    O
8.162    O
petaflops    O
,    O
making    O
it    O
the    O
fastest    O
supercomputer    O
in    O
the    O
world    O
at    O
the    O
time    O
;    O
it    O
achieved    O
this    O
performance    O
with    O
a    O
computing    O
efficiency    O
ratio    O
of    O
93.0%    O
.    O

TOP500    B-Supercomputer104358117

A    O
stacked    O
graph    O
showing    O
the    O
changing    O
distribution    O
of    O
processor    O
families    O
in    O
TOP500    B-Supercomputer104358117
supercomputers    O
since    O
1996    O
.    O

Its    O
Linpack    O
performance    O
stands    O
at    O
80    O
TeraFLOPs    O
,    O
which    O
is    O
about    O
half    O
as    O
fast    O
as    O
the    O
cut    O
-    O
off    O
line    O
for    O
the    O
Top    B-Supercomputer104358117
500    I-Supercomputer104358117
Supercomputers    I-Supercomputer104358117
list    O
.    O

It    O
is    O
the    O
most    O
powerful    O
supercomputing    O
centre    O
for    O
scientific    O
research    O
in    O
Italy    O
,    O
as    O
stated    O
in    O
the    O
TOP500-list    B-Supercomputer104358117
of    O
the    O
most    O
powerful    O
supercomputers    O
in    O
the    O
world    O
:    O
Fermi    O
,    O
the    O
supercomputing    O
system    O
IBM    O
Blue    O
Gene    O
/    O
Q    O
installed    O
in    O
June    O
2012    O
,    O
and    O
ranked    O
at    O
the    O
7th    O
position    O
on    O
the    O
list    O
,    O
in    O
2015    O
is    O
ranked    O
at    O
the    O
23rd    O
position    O
.    O

Top500    B-Supercomputer104358117

Forty    O
-    O
two    O
Sun    O
Fire    O
X4500    O
data    O
servers    O
are    O
used    O
to    O
provide    O
Lustre    O
cluster    O
filesystem    O
storage    O
in    O
the    O
TSUBAME    O
supercomputer    O
,    O
which    O
was    O
number    O
7    O
on    O
June    O
2006    O
TOP500    B-Supercomputer104358117
list    O
.    O

TOP500    B-Supercomputer104358117
reports    O
that    O
China    O
's    O
Tianhe-2    B-Supercomputer104358117
supercomputer    O
is    O
the    O
world    O
's    O
most    O
powerful    O
computer    O
,    O
capable    O
of    O
performing    O
over    O
33    O
quadrillion    O
floating    O
point    O
operations    O
per    O
second    O
.    O

Throughout    O
,    O
Cray    O
continued    O
to    O
be    O
the    O
performance    O
leader    O
,    O
continually    O
beating    O
the    O
competition    O
with    O
a    O
series    O
of    O
machines    O
that    O
led    O
to    O
the    O
Cray-2    B-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
.    O

Seymour    O
Cray    O
continued    O
working    O
,    O
this    O
time    O
on    O
the    O
Cray-2    B-Supercomputer104358117
,    O
though    O
it    O
only    O
ended    O
up    O
being    O
marginally    O
faster    O
than    O
the    O
Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
,    O
developed    O
by    O
another    O
team    O
at    O
the    O
company    O
.    O

Cooling    O
hot    O
computer    O
components    O
with    O
various    O
fluids    O
has    O
been    O
in    O
use    O
since    O
at    O
least    O
as    O
far    O
back    O
as    O
the    O
development    O
of    O
Cray-2    B-Supercomputer104358117
in    O
1982    O
,    O
using    O
Fluorinert    O
.    O

US    O
designs    O
focused    O
on    O
this    O
problem    O
in    O
the    O
early    O
1980s    O
,    O
and    O
the    O
contemporary    O
Cray-2    B-Supercomputer104358117
could    O
drive    O
about    O
2    O
GB    O
/    O
s    O
per    O
processor    O
,    O
with    O
up    O
to    O
four    O
processors    O
.    O

Oregon    O
-    O
based    O
Floating    O
Point    O
Systems    O
(    O
FPS    O
)    O
built    O
add    O
-    O
on    O
array    O
processors    O
for    O
minicomputers    O
,    O
later    O
building    O
their    O
own    O
minisupercomputer    B-Supercomputer104358117
.    O

New    O
vendors    O
introduced    O
small    O
supercomputers    O
,    O
known    O
as    O
minisupercomputer    B-Supercomputer104358117
(    O
as    O
opposed    O
to    O
superminis    O
)    O
during    O
the    O
late    O
1980s    O
and    O
early    O
1990s    O
,    O
which    O
out    O
-    O
competed    O
low    O
-    O
end    O
Cray    O
machines    O
in    O
the    O
market    O
.    O

The    O
Cydra-5    B-Supercomputer104358117
departmental    O
supercomputer    O
is    O
the    O
first    O
minisupercomputer    B-Supercomputer104358117
designed    O
by    O
Cydrome    O
.    O

D-Wave    B-Supercomputer104358117
Two    I-Supercomputer104358117
,    O
a    O
quantum    O
computer    O

Compute    O
Node    O
Linux    O
(    O
CNL    O
)    O
is    O
a    O
runtime    O
environment    O
based    O
on    O
the    O
Linux    O
kernel    O
for    O
the    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT5    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT6    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XE6    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
XK6    I-Supercomputer104358117
supercomputer    O
systems    O
based    O
on    O
SUSE    O
Linux    O
Enterprise    O
Server    O
.    O

An    O
example    O
of    O
extending    O
Galaxy    O
is    O
Galaxy    O
-    O
P    O
from    O
the    O
University    B-Supercomputer104358117
of    I-Supercomputer104358117
Minnesota    I-Supercomputer104358117
Supercomputing    I-Supercomputer104358117
Institute    I-Supercomputer104358117
,    O
which    O
is    O
customized    O
as    O
a    O
data    O
analysis    O
platform    O
for    O
mass    O
spectrometry    O
-    O
based    O
proteomics    O
.    O

donated    O
a    O
new    O
SV1    B-Supercomputer104358117
supercomputer    O
,    O
known    O
as    O
Seymour    O
,    O
to    O
the    O
school    O
on    O
December    O
4    O
,    O
2002    O
.    O

Instead    O
they    O
continued    O
with    O
the    O
CDC    B-Supercomputer104358117
STAR-100    I-Supercomputer104358117
while    O
Cray    O
went    O
off    O
to    O
build    O
the    O
Cray-1    B-Supercomputer104358117
.    O

Description    O
:    O
The    O
Cray-1    B-Supercomputer104358117
was    O
a    O
supercomputer    O
designed    O
by    O
a    O
team    O
including    O
Seymour    O
Cray    O
for    O
Cray    O
Research    O
.    O

The    O
company    O
's    O
first    O
product    O
,    O
the    O
Cray-1    B-Supercomputer104358117
supercomputer    O
,    O
was    O
a    O
major    O
success    O
because    O
it    O
was    O
significantly    O
faster    O
than    O
all    O
other    O
computers    O
at    O
the    O
time    O
.    O

The    O
VP2000    O
was    O
similar    O
in    O
many    O
ways    O
to    O
their    O
earlier    O
designs    O
,    O
and    O
in    O
turn    O
to    O
the    O
Cray-1    B-Supercomputer104358117
,    O
using    O
a    O
register    O
-    O
based    O
vector    O
processor    O
for    O
performance    O
.    O

The    O
processor    O
was    O
similar    O
in    O
most    O
ways    O
to    O
the    O
famed    O
Cray-1    B-Supercomputer104358117
,    O
but    O
did    O
not    O
have    O
vector    O
chaining    O
capabilities    O
and    O
was    O
therefore    O
somewhat    O
slower    O
.    O

It    O
was    O
aimed    O
at    O
technical    O
and    O
scientific    O
users    O
who    O
would    O
normally    O
buy    O
a    O
machine    O
like    O
a    O
Cray-1    B-Supercomputer104358117
but    O
did    O
not    O
need    O
that    O
level    O
of    O
power    O
or    O
throughput    O
for    O
graphics    O
-    O
heavy    O
workloads    O
.    O

After    O
thorough    O
testing    O
and    O
four    O
years    O
of    O
NASA    O
use    O
,    O
ILLIAC    O
IV    O
was    O
connected    O
to    O
the    O
ARPANet    O
for    O
distributed    O
use    O
in    O
November    O
1975    O
,    O
becoming    O
the    O
first    O
network    O
-    O
available    O
supercomputer    O
,    O
beating    O
Cray    O
's    O
Cray-1    B-Supercomputer104358117
by    O
nearly    O
12    O
months    O
.    O

The    O
classic    O
example    O
of    O
this    O
design    O
is    O
the    O
Cray-1    B-Supercomputer104358117
,    O
which    O
had    O
performance    O
similar    O
to    O
the    O
ILLIAC    O
.    O

The    O
NYU    O
Ultracomputer    B-Supercomputer104358117
is    O
a    O
significant    O
processor    O
design    O
in    O
the    O
history    O
of    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

HPCx    B-Supercomputer104358117
,    O
a    O
supercomputer    O
(    O
closed    O
,    O
replaced    O
by    O
the    O
UK    O
national    O
supercomputing    O
service    O
,    O
HECToR    B-Supercomputer104358117
,    O
based    O
in    O
Edinburgh    O
)    O
.    O

In    O
2011    O
,    O
Cray    O
announced    O
the    O
Cray    B-Supercomputer104358117
XK6    I-Supercomputer104358117
hybrid    O
supercomputer    O
.    O

This    O
allows    O
the    O
dataset    O
to    O
be    O
processed    B-Supercomputer104358117
faster    O
and    O
more    O
efficiently    O
than    O
it    O
would    O
be    O
in    O
a    O
more    O
conventional    O
supercomputer    B-Supercomputer104358117
architecture    I-Supercomputer104358117
that    O
relies    O
on    O
a    O
parallel    O
file    O
system    O
where    O
computation    O
and    O
data    O
are    O
distributed    O
via    O
high    O
-    O
speed    O
networking    O
.    O

Its    O
design    O
was    O
influenced    O
by    O
the    O
VPP500/5000    O
models    O
of    O
the    O
Fujitsu    B-Supercomputer104358117
VP    I-Supercomputer104358117
2000    B-Supercomputer104358117
vector    O
processor    O
supercomputer    B-Supercomputer104358117
line    O
.    O

Universe    O
Sandbox    O
,    O
an    O
interactive    O
space    O
and    O
gravity    B-Supercomputer104358117
simulator    I-Supercomputer104358117

Like    O
the    O
previous    O
Cray    B-Supercomputer104358117
T3D    I-Supercomputer104358117
,    O
it    O
was    O
a    O
fully    O
distributed    O
memory    O
machine    O
using    O
a    O
3D    O
torus    B-Supercomputer104358117
topology    I-Supercomputer104358117
interconnect    I-Supercomputer104358117
network    O
.    O

Tofu    O
has    O
a    O
six    O
-    O
dimensional    O
mesh    O
/    O
torus    B-Supercomputer104358117
topology    I-Supercomputer104358117
,    O
a    O
scalability    O
of    O
over    O
100,000    O
nodes    O
,    O
and    O
full    O
-    O
duplex    O
links    O
that    O
have    O
a    O
peak    O
bandwidth    O
of    O
10    O
GB    O
/    O
s    O
(    O
5    O
GB    O
/    O
s    O
per    O
direction    O
)    O
.    O

The    O
PBS    O
Pro    O
scheduler    O
used    O
on    O
the    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
systems    O
does    O
not    O
attempt    O
to    O
optimize    O
locality    O
on    O
its    O
three    O
-    O
dimensional    O
torus    B-Supercomputer104358117
interconnect    I-Supercomputer104358117
,    O
but    O
simply    O
uses    O
the    O
first    O
available    O
processor    O
.    O

Compute    O
Node    O
Linux    O
(    O
CNL    O
)    O
is    O
a    O
runtime    O
environment    O
based    O
on    O
the    O
Linux    O
kernel    O
for    O
the    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT5    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT6    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XE6    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
XK6    I-Supercomputer104358117
supercomputer    O
systems    O
based    O
on    O
SUSE    O
Linux    O
Enterprise    O
Server    O
.    O

The    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
massively    O
parallel    O
supercomputer    O
became    O
a    O
commercialized    O
version    O
of    O
Red    O
Storm    O
,    O
similar    O
in    O
many    O
respects    O
to    O
the    O
earlier    O
T3E    O
architecture    O
,    O
but    O
,    O
like    O
the    O
XD1    O
,    O
using    O
AMD    O
Opteron    O
processors    O
.    O

Numerical    B-Supercomputer104358117
Wind    I-Supercomputer104358117
Tunnel    I-Supercomputer104358117
(    O
数値風洞    B-Supercomputer104358117
)    O
was    O
an    O
early    O
implementation    O
of    O
the    O
vector    O
parallel    O
architecture    O
developed    O
in    O
a    O
joint    O
project    O
between    O
National    O
Aerospace    O
Laboratory    O
of    O
Japan    O
and    O
Fujitsu    O
.    O

Thus    O
modern    O
supercomputers    O
usually    O
run    O
different    O
operating    O
systems    O
on    O
different    O
nodes    O
,    O
e.g.    O
,    O
using    O
a    O
small    O
and    O
efficient    O
lightweight    O
kernel    O
such    O
as    O
CNK    B-Supercomputer104358117
or    O
CNL    O
on    O
compute    O
nodes    O
,    O
but    O
a    O
larger    O
system    O
such    O
as    O
a    O
Linux    O
-    O
derivative    O
on    O
server    O
and    O
I    O
/    O
O    O
nodes    O
.    O

The    O
IBM    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
supercomputer    O
uses    O
the    O
CNK    B-Supercomputer104358117
operating    I-Supercomputer104358117
system    I-Supercomputer104358117
on    O
the    O
compute    O
nodes    O
,    O
but    O
uses    O
a    O
modified    O
Linux    O
-    O
based    O
kernel    O
called    O
I    O
/    O
O    O
Node    O
Kernel    O
(    O
INK    O
)    O
on    O
the    O
I    O
/    O
O    O
nodes    O
.    O

The    O
Hitachi    B-Supercomputer104358117
SR2201    I-Supercomputer104358117
was    O
a    O
distributed    O
memory    O
parallel    B-Supercomputer104358117
system    I-Supercomputer104358117
that    O
was    O
introduced    O
in    O
March    O
1996    O
by    O
Hitachi    O
.    O

Specifically    O
,    O
the    O
company    O
announced    O
they    O
would    O
use    O
Watson    O
to    O
analyze    O
data    O
from    O
over    O
200,000    O
Weather    O
Underground    O
personal    O
weather    O
stations    O
,    O
and    O
data    O
from    O
other    O
sources    O
,    O
as    O
a    O
part    O
of    O
project    O
Deep    B-Supercomputer104358117
Thunder    I-Supercomputer104358117
.    O

LU    B-Supercomputer104358117
reduction    I-Supercomputer104358117
is    O
an    O
algorithm    O
related    O
to    O
LU    O
decomposition    O
.    O

TERA-10    B-Supercomputer104358117
is    O
a    O
supercomputer    O
built    O
by    O
Bull    O
SA    O
for    O
the    O
French    O
Commissariat    O
à    O
l'Énergie    O
Atomique    O
,    O
(    O
Atomic    O
Energy    O
Commission    O
)    O
.    O

Supercomputer    B-Supercomputer104358117
operating    I-Supercomputer104358117
systems    I-Supercomputer104358117

Nebulae    B-Supercomputer104358117
built    O
by    O
Dawning    O
,    O
was    O
the    O
third    O
petascale    O
computer    O
and    O
the    O
first    O
built    O
by    O
China    O
with    O
a    O
performance    O
of    O
1.271    O
petaflops    O
in    O
2010    O
.    O

After    O
the    O
Tera    O
merger    O
,    O
the    O
Tera    O
MTA    O
system    O
was    O
relaunched    O
as    O
the    O
Cray    B-Supercomputer104358117
MTA-2    I-Supercomputer104358117
.    O

The    O
Cydra-5    B-Supercomputer104358117
departmental    O
supercomputer    O
is    O
the    O
first    O
minisupercomputer    B-Supercomputer104358117
designed    O
by    O
Cydrome    O
.    O

It    O
has    O
one    O
of    O
the    O
top    B-Supercomputer104358117
100    I-Supercomputer104358117
supercomputers    O
in    O
the    O
world    O
,    O
the    O
"    O
Tera-100    B-Supercomputer104358117
"    O
.    O

This    O
kind    O
of    O
system    O
was    O
used    O
as    O
the    O
basis    O
of    O
the    O
Japanese    B-Supercomputer104358117
Fifth    I-Supercomputer104358117
Generation    I-Supercomputer104358117
Project    I-Supercomputer104358117
(ICOT)    I-Supercomputer104358117
.    O

Fifth    B-Supercomputer104358117
generation    I-Supercomputer104358117
computer    I-Supercomputer104358117
,    O
a    O
Japanese    O
computing    O
initiative    O
begun    O
in    O
1982    O

With    O
the    O
fifth    B-Supercomputer104358117
generation    I-Supercomputer104358117
computer    I-Supercomputer104358117
Japan    O
intended    O
to    O
leap    O
over    O
its    O
competition    O
in    O
computer    O
hardware    O
and    O
software    O
,    O
and    O
one    O
project    O
that    O
many    O
large    O
Japanese    O
electronics    O
firms    O
found    O
themselves    O
involved    O
in    O
was    O
creating    O
software    O
for    O
translating    O
into    O
and    O
from    O
English    O
(    O
Fujitsu    O
,    O
Toshiba    O
,    O
NTT    O
,    O
Brother    O
,    O
Catena    O
,    O
Matsushita    O
,    O
Mitsubishi    O
,    O
Sharp    O
,    O
Sanyo    O
,    O
Hitachi    O
,    O
NEC    O
,    O
Panasonic    O
,    O
Kodensha    O
,    O
Nova    O
,    O
Oki    O
)    O
.    O

Fifth    B-Supercomputer104358117
generation    I-Supercomputer104358117
computer    I-Supercomputer104358117

In    O
1945    O
,    O
soon    O
after    O
the    O
victory    O
in    O
Europe    O
,    O
Wylie    O
demonstrated    O
how    O
Colossus    B-Supercomputer104358117
—    O
electronic    O
machines    O
used    O
to    O
help    O
solve    O
Tunny    O
—    O
could    O
have    O
been    O
used    O
unmodified    O
to    O
break    O
the    O
Tunny    O
"    O
motor    O
wheels    O
"    O
,    O
a    O
task    O
which    O
had    O
been    O
previously    O
done    O
by    O
hand    O
.    O

The    O
building    O
—    O
"    O
Block    O
H    O
"    O
—    O
was    O
the    O
first    O
purpose    O
-    O
built    O
computer    O
centre    O
in    O
the    O
world    O
,    O
hosting    O
six    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
by    O
the    O
end    O
of    O
World    O
War    O
II    O
.    O

On    O
display    O
in    O
the    O
museum    O
are    O
many    O
famous    O
early    O
computing    O
era    O
machines    O
,    O
including    O
a    O
functioning    O
Colossus    B-Supercomputer104358117
Mark    I-Supercomputer104358117
2    I-Supercomputer104358117
that    O
was    O
rebuilt    O
between    O
1993    O
and    O
2008    O
by    O
a    O
team    O
of    O
volunteers    O
led    O
by    O
Tony    O
Sale    O
.    O

A    O
team    O
led    O
by    O
Tony    O
Sale    O
reconstructed    O
a    O
Colossus    B-Supercomputer104358117
Mark    I-Supercomputer104358117
2    I-Supercomputer104358117
computer    I-Supercomputer104358117
at    O
Bletchley    O
Park    O
.    O

Lord    O
Grey    O
has    O
four    O
school    O
houses    O
:    O
Lorenz    O
,    O
Enigma    O
,    O
Colossus    B-Supercomputer104358117
and    O
Turing    O
,    O
whose    O
names    O
relate    O
to    O
World    O
War    O
II    O
code    O
-    O
breaking    O
work    O
at    O
Bletchley    O
Park    O
.    O

Although    O
not    O
a    O
graduate    O
,    O
Tommy    O
Flowers    O
,    O
a    O
British    O
engineer    O
who    O
helped    O
create    O
the    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
used    O
to    O
break    O
code    O
during    O
World    O
War    O
II    O
received    O
a    O
basic    O
computing    O
certificate    O
from    O
Hendon    O
College    O
.    O

It    O
was    O
mainly    O
an    O
electro    O
-    O
mechanical    O
machine    O
,    O
containing    O
no    O
more    O
than    O
a    O
couple    O
of    O
dozen    O
valves    O
(    O
vacuum    O
tubes    O
)    O
,    O
and    O
was    O
the    O
predecessor    O
to    O
the    O
electronic    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
.    O

As    O
the    O
Robinson    O
was    O
a    O
bit    O
slow    O
and    O
unreliable    O
,    O
it    O
was    O
later    O
replaced    O
by    O
the    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
for    O
many    O
purposes    O
,    O
including    O
the    O
methods    O
used    O
against    O
the    O
twelve    O
-    O
rotor    O
Lorenz    O
SZ42    O
on    O
-    O
line    O
teleprinter    O
cipher    O
machine    O
(    O
code    O
named    O
Tunny    O
,    O
for    O
tunafish    O
)    O
.    O

The    O
counters    O
that    O
Wynn    O
-    O
Williams    O
designed    O
for    O
Heath    O
Robinson    O
,    O
and    O
subsequently    O
for    O
the    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
used    O
thyratrons    O
to    O
count    O
units    O
of    O
1    O
,    O
2    O
,    O
4    O
,    O
8    O
;    O
high    O
speed    O
relays    O
to    O
count    O
units    O
of    O
16    O
,    O
32    O
,    O
48    O
,    O
64    O
;    O
and    O
slower    O
relays    O
to    O
count    O
80    O
,    O
160    O
,    O
240    O
,    O
320    O
,    O
400    O
,    O
800    O
,    O
1200    O
,    O
1600    O
,    O
2000    O
,    O
4000    O
,    O
6000    O
,    O
and    O
8000    O
.    O

In    O
1943    O
the    O
world    O
's    O
first    O
programmable    O
electronic    O
computer    O
,    O
Colossus    B-Supercomputer104358117
Mark    O
1    O
was    O
built    O
by    O
Tommy    O
Flowers    O
and    O
his    O
team    O
,    O
followed    O
in    O
1944    O
and    O
1945    O
by    O
nine    O
Colossus    O
Mark    O
2s    O
.    O

The    O
Colossus    B-Supercomputer104358117
of    O
1943    O
was    O
the    O
first    O
electronic    O
computing    O
device    O
,    O
but    O
it    O
was    O
not    O
a    O
general    O
-    O
purpose    O
machine    O
.    O

The    O
colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
,    O
by    O
Alan    O
Turing    O
(    O
1912–54    O
)    O
,    O
an    O
early    O
digital    O
computer    O
(    O
a    O
code    O
breaker    O
in    O
WWII    O
made    O
in    O
Bletchley    O
Park    O
)    O
computer    O
.    O

The    O
code    O
-    O
breaking    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
,    O
used    O
at    O
Bletchley    O
Park    O
during    O
the    O
Second    O
World    O
War    O
,    O
was    O
built    O
at    O
the    O
Post    O
Office    O
Research    O
Station    O
in    O
Dollis    O
Hill    O
by    O
a    O
team    O
led    O
by    O
Tommy    O
Flowers    O
.    O

The    O
use    O
of    O
computers    O
changed    O
the    O
process    O
of    O
cryptanalysis    O
,    O
famously    O
with    O
Bletchley    O
Park    O
's    O
Colossus    B-Supercomputer104358117
.    O

When    O
,    O
in    O
the    O
mid-1970s    O
,    O
the    O
secrecy    O
surrounding    O
the    O
British    O
World    O
War    O
II    O
development    O
of    O
the    O
Colossus    B-Supercomputer104358117
computer    I-Supercomputer104358117
that    O
pre    O
-    O
dated    O
ENIAC    O
,    O
was    O
lifted    O
and    O
Colossus    O
was    O
described    O
at    O
a    O
conference    O
in    O
Los    O
Alamos    O
,    O
New    O
Mexico    O
in    O
June    O
1976    O
,    O
John    O
Mauchly    O
and    O
Konrad    O
Zuse    O
were    O
reported    O
to    O
have    O
been    O
astonished    O
.    O

Turing    O
's    O
technical    O
design    O
"    O
Proposed    O
Electronic    O
Calculator    O
"    O
was    O
the    O
product    O
of    O
his    O
theoretical    O
work    O
in    O
1936    O
"    O
On    O
Computable    O
Numbers    O
"    O
and    O
his    O
wartime    O
experience    O
at    O
Bletchley    O
Park    O
where    O
the    O
Colossus    B-Supercomputer104358117
computers    I-Supercomputer104358117
had    O
been    O
successful    O
in    O
breaking    O
German    O
military    O
codes    O
.    O

Generally    O
,    O
artificial    O
action    O
selection    O
mechanisms    O
can    O
be    O
divided    O
into    O
several    O
categories    O
:    O
symbol    O
-    O
based    O
systems    O
sometimes    O
known    O
as    O
classical    O
planning    O
,    O
distributed    B-Supercomputer104358117
solutions    I-Supercomputer104358117
,    O
and    O
reactive    O
or    O
dynamic    O
planning    O
.    O

As    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
platform    O
,    O
Babel    O
provides    O
a    O
language    O
-    O
neutral    O
Remote    O
Method    O
Invocation    O
(    O
RMI    O
)    O
scheme    O
similar    O
to    O
Java    O
's    O
RMI    O
which    O
allows    O
third    O
-    O
party    O
plug    O
-    O
ins    O
to    O
specify    O
custom    O
data    O
encodings    O
and    O
network    O
protocols    O
.    O

FightAIDS@Home    O
,    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O

Michael    O
J.    O
Franklin    O
is    O
an    O
American    O
software    O
entrepreneur    O
and    O
computer    O
scientist    O
specializing    O
in    O
distributed    B-Supercomputer104358117
and    O
streaming    O
database    O
technology    O
.    O

Computer    O
science    O
programs    O
typically    O
centers    O
primarily    O
around    O
theory    O
and    O
software    O
,    O
with    O
only    O
some    O
hardware    O
;    O
upper    O
division    O
courses    O
tend    O
to    O
allow    O
a    O
lot    O
of    O
freedom    O
to    O
specialize    O
in    O
software    O
and    O
theory    O
related    O
areas    O
(    O
e.g.    O
algorithms    O
,    O
artificial    O
intelligence    O
,    O
cryptography    O
/    O
security    O
,    O
graphics    O
/    O
visualization    O
,    O
numerical    O
and    O
symbolic    O
computing    O
,    O
operating    O
systems    O
/    O
distributed    B-Supercomputer104358117
processing    I-Supercomputer104358117
,    O
software    O
engineering    O
)    O
.    O

Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

ND4J    O
's    O
operations    O
include    O
distributed    B-Supercomputer104358117
parallel    B-Supercomputer104358117
versions    O
.    O

In    O
distributed    B-Supercomputer104358117
programming    I-Supercomputer104358117
,    O
a    O
portable    O
object    O
is    O
an    O
object    O
which    O
can    O
be    O
accessed    O
through    O
a    O
normal    O
method    O
call    O
while    O
possibly    O
residing    O
in    O
memory    O
on    O
another    O
computer    O
.    O

In    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
a    O
single    O
system    O
image    O
(    O
SSI    O
)    O
cluster    O
is    O
a    O
cluster    B-Supercomputer104358117
of    O
machines    O
that    O
appears    O
to    O
be    O
one    O
single    O
system    O
.    O

Its    O
primary    O
use    O
is    O
in    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

The    O
other    O
keys    O
were    O
found    O
after    O
a    O
few    O
weeks    O
by    O
the    O
unitedti.org    O
community    O
through    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
.    O

January    O
27    O
–    O
Scientists    O
behind    O
the    O
climateprediction.net    O
project    O
,    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
run    O
from    O
Oxford    O
University    O
,    O
announce    O
that    O
first    O
results    O
indicate    O
a    O
long    O
term    O
surface    O
temperature    O
increase    O
due    O
to    O
global    O
warming    O
of    O
between    O
2    O
and    O
11    O
degrees    O
Celsius    O
as    O
a    O
consequence    O
of    O
doubling    O
carbon    O
dioxide    O
levels    O
,    O
with    O
most    O
of    O
the    O
simulations    O
predicting    O
a    O
temperature    O
rise    O
of    O
around    O
3.4    O
°C    O
.    O

It    O
uses    O
the    O
BOINC    O
volunteer    O
computing    O
platform    O
(    O
a    O
form    O
of    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
similar    O
to    O
SETI@home    O
)    O
.    O

Computer    O
clusters    O
emerged    O
as    O
a    O
result    O
of    O
convergence    O
of    O
a    O
number    O
of    O
computing    O
trends    O
including    O
the    O
availability    O
of    O
low    O
-    O
cost    O
microprocessors    O
,    O
high    O
-    O
speed    O
networks    O
,    O
and    O
software    O
for    O
high    O
-    O
performance    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

It    O
is    O
distinct    O
from    O
other    O
approaches    O
such    O
as    O
peer    O
to    O
peer    O
or    O
grid    O
computing    O
which    O
also    O
use    O
many    O
nodes    O
,    O
but    O
with    O
a    O
far    O
more    O
distributed    B-Supercomputer104358117
nature    I-Supercomputer104358117
.    O

:*    O
Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

Foster    O
's    O
research    O
focuses    O
on    O
the    O
acceleration    O
of    O
discovery    O
in    O
a    O
network    O
using    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

His    O
research    O
has    O
also    O
resulted    O
in    O
the    O
development    O
of    O
techniques    O
,    O
tools    O
and    O
algorithms    O
for    O
high    O
-    O
performance    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

Charles    O
Eric    O
Leiserson    O
is    O
a    O
computer    O
scientist    O
,    O
specializing    O
in    O
the    O
theory    O
of    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
particularly    O
practical    O
applications    O
thereof    O
.    O

The    O
Ricart    O
-    O
Agrawala    O
Algorithm    O
is    O
an    O
algorithm    O
for    O
mutual    O
exclusion    O
on    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
.    O

It    O
is    O
under    O
the    O
direction    O
of    O
Dr.    O
Manish    O
Parashar    O
and    O
the    O
current    O
research    O
fields    O
include    O
Autonomic    O
Computing    O
,    O
Parallel    B-Supercomputer104358117
Computing    I-Supercomputer104358117
and    O
Distributed    B-Supercomputer104358117
Computing    I-Supercomputer104358117
,    O
Grid    O
Computing    O
,    O
Peer    O
-    O
to    O
-    O
peer    O
Computing    O
,    O
Adaptive    O
Computing    O
Systems    O
,    O
and    O
Scientific    O
Computation    O
..    O

Maekawa    O
's    O
algorithm    O
is    O
an    O
algorithm    O
for    O
mutual    O
exclusion    O
on    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
.    O

The    O
Einstein@Home    O
project    O
is    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
similar    O
to    O
SETI@home    O
intended    O
to    O
detect    O
this    O
type    O
of    O
gravitational    O
wave    O
.    O

Under    O
a    O
non    O
-    O
GPL    O
license    O
,    O
GROMACS    O
is    O
widely    O
used    O
in    O
the    O
Folding@home    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
for    O
simulations    O
of    O
protein    O
folding    O
,    O
where    O
it    O
the    O
base    O
code    O
for    O
the    O
project    O
's    O
largest    O
and    O
most    O
regularly    O
used    O
series    O
of    O
calculation    O
cores    O
.    O

Herbert    O
's    O
research    O
interests    O
include    O
computer    O
networking    O
,    O
operating    O
systems    O
,    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
programming    O
languages    O
and    O
large    O
-    O
scale    O
data    O
driven    O
systems    O
.    O

ANSA    O
aimed    O
to    O
develop    O
a    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
software    O
architecture    O
to    O
support    O
applications    O
integration    O
in    O
enterprise    O
-    O
wide    O
systems    O
.    O

Distributed    B-Supercomputer104358117
computer    I-Supercomputer104358117
system    I-Supercomputer104358117
often    O
need    O
a    O
way    O
to    O
replicate    O
data    O
for    O
sharing    O
between    O
programs    O
running    O
on    O
multiple    O
machines    O
,    O
connected    O
by    O
a    O
network    O
.    O

Replication    O
is    O
one    O
of    O
the    O
oldest    O
and    O
most    O
important    O
topics    O
in    O
the    O
overall    O
area    O
of    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
.    O

In    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
environment    O
,    O
distributed    O
object    O
communication    O
realizes    O
communication    O
between    O
distributed    O
objects    O
.    O

Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

The    O
service    O
-    O
oriented    O
computing    O
environment    O
(    O
SORCER    O
)    O
is    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
platform    O
implemented    O
in    O
Java    O
.    O

For    O
his    O
work    O
on    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
with    O
Fischer    O
and    O
Lynch    O
,    O
he    O
received    O
the    O
Dijkstra    O
Prize    O
in    O
2001    O
,    O
and    O
his    O
work    O
with    O
Dyer    O
and    O
Goldberg    O
on    O
counting    O
graph    O
homomorphisms    O
received    O
a    O
best    O
paper    O
award    O
at    O
the    O
ICALP    O
conference    O
in    O
2006    O
.    O

His    O
research    O
is    O
in    O
the    O
area    O
of    O
concurrent    O
and    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
for    O
high    O
-    O
integrity    O
and    O
mission    O
-    O
critical    O
applications    O
.    O

Open    O
Distributed    B-Supercomputer104358117
Processing    I-Supercomputer104358117
,    O
and    O
its    O
standard    O
reference    O
model    O
RM    O
-    O
ODP    O
,    O
in    O
computer    O
science    O

It    O
was    O
found    O
by    O
Szabolcs    O
Peter    O
in    O
the    O
PrimeGrid    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    I-Supercomputer104358117
which    O
announced    O
it    O
on    O
6    O
November    O
2016    O
.    O

It    O
was    O
found    O
by    O
Szabolcs    O
Peter    O
in    O
the    O
PrimeGrid    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    I-Supercomputer104358117
which    O
announced    O
it    O
on    O
6    O
November    O
2016    O
.    O

Scientific    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
tools    O
such    O
as    O
Folding@home    O
,    O
GIMPS    O
and    O
SETI@home    O
or    O
other    O
computationally    O
intensive    O
chores    O
may    O
also    O
push    O
CPUs    O
and    O
GPUs    O
to    O
their    O
limits    O
,    O
and    O
may    O
also    O
serve    O
as    O
a    O
means    O
of    O
competition    O
,    O
such    O
as    O
tracking    O
how    O
many    O
data    O
sets    O
a    O
user    O
has    O
completed    O
.    O

This    O
allows    O
the    O
dataset    O
to    O
be    O
processed    B-Supercomputer104358117
faster    O
and    O
more    O
efficiently    O
than    O
it    O
would    O
be    O
in    O
a    O
more    O
conventional    O
supercomputer    B-Supercomputer104358117
architecture    I-Supercomputer104358117
that    O
relies    O
on    O
a    O
parallel    O
file    O
system    O
where    O
computation    O
and    O
data    O
are    O
distributed    O
via    O
high    O
-    O
speed    O
networking    O
.    O

Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

The    O
Chandy    O
–    O
Lamport    O
algorithm    O
is    O
a    O
snapshot    O
algorithm    O
that    O
is    O
used    O
in    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
for    O
recording    O
a    O
consistent    O
global    O
state    O
of    O
an    O
asynchronous    O
system    O
.    O

This    O
involves    O
ordering    O
events    O
based    O
on    O
the    O
potential    O
causal    O
relationship    O
of    O
pairs    O
of    O
events    O
in    O
a    O
concurrent    O
system    O
,    O
especially    O
asynchronous    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
.    O

Where    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
is    O
used    O
to    O
separate    O
load    O
between    O
multiple    O
servers    O
(    O
either    O
for    O
performance    O
or    O
reliability    O
reasons    O
)    O
,    O
a    O
shard    O
approach    O
may    O
also    O
be    O
useful    O
.    O

Microservices    O
is    O
a    O
specialization    O
of    O
an    O
implementation    O
approach    O
for    O
service    O
-    O
oriented    O
architectures    O
(    O
SOA    O
)    O
used    O
to    O
build    O
flexible    O
,    O
independently    O
deployable    O
software    B-Supercomputer104358117
systems    I-Supercomputer104358117
.    O

In    O
addition    O
,    O
she    O
studies    O
secure    O
distributed    B-Supercomputer104358117
protocols    O
and    O
the    O
theoretical    O
foundations    O
of    O
cryptography    O
,    O
as    O
well    O
as    O
number    O
theory    O
and    O
the    O
theory    O
of    O
algorithms    O
and    O
distributed    O
systems    O
.    O

A    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
called    O
Seventeen    O
or    O
Bust    O
is    O
currently    O
trying    O
to    O
prove    O
this    O
statement    O
,    O
only    O
five    O
of    O
the    O
original    O
seventeen    O
possibilities    O
remain    O
.    O

distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
;    O

If    O
the    O
network    O
in    O
question    O
is    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
,    O
the    O
nodes    O
are    O
clients    O
,    O
servers    O
or    O
peers    O
.    O

Visual    O
programming    O
is    O
also    O
responsible    O
for    O
the    O
power    O
of    O
distributed    B-Supercomputer104358117
programming    I-Supercomputer104358117
(    O
cf    O
.    O

An    O
anonymous    O
P2P    O
communication    O
system    O
is    O
a    O
peer    O
-    O
to    O
-    O
peer    O
distributed    B-Supercomputer104358117
application    I-Supercomputer104358117
in    O
which    O
the    O
nodes    O
or    O
participants    O
are    O
anonymous    O
or    O
pseudonymous    O
.    O

Topics    O
covered    O
:    O
concurrent    O
computing    O
,    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

Alice    O
:    O
an    O
interpreter    O
for    O
Standard    O
ML    O
by    O
Saarland    O
University    O
adding    O
features    O
for    O
lazy    O
evaluation    O
,    O
concurrency    O
(    O
multithreading    O
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
via    O
remote    O
procedure    O
calls    O
)    O
and    O
constraint    O
programming    O
.    O

A    O
number    O
of    O
these    O
challenge    O
problems    O
have    O
been    O
tackled    O
using    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
organised    O
by    O
Distributed.net    O
.    O

Paton    O
's    O
research    O
interests    O
are    O
currently    O
in    O
distributed    B-Supercomputer104358117
information    O
management    O
including    O
dataspaces    O
,    O
query    O
processing    O
in    O
Wireless    O
sensor    O
networks    O
,    O
autonomic    O
computing    O
,    O
workflow    O
management    O
,    O
and    O
data    O
management    O
for    O
systems    O
biology    O
.    O

This    O
typically    O
involves    O
the    O
ability    O
to    O
seamlessly    O
provision    O
and    O
add    O
compute    O
,    O
memory    O
,    O
networking    O
,    O
and    O
storage    O
resources    O
to    O
a    O
given    O
node    O
or    O
set    O
of    O
nodes    O
that    O
make    O
up    O
a    O
larger    O
computing    O
,    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
or    O
grid    O
computing    O
environment    O
.    O

Its    O
main    O
product    O
is    O
Ice    O
,    O
an    O
open    O
-    O
source    O
RPC    O
framework    O
that    O
helps    O
software    O
developers    O
build    O
distributed    B-Supercomputer104358117
applications    I-Supercomputer104358117
.    O

Gelenbe    O
has    O
contributed    O
pioneering    O
research    O
concerning    O
the    O
performance    O
of    O
multiprogramming    O
computer    O
systems    O
,    O
virtual    O
memory    O
management    O
,    O
data    O
base    O
reliability    O
optimisation    O
,    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
and    O
network    O
protocols    O
.    O

WER    O
is    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
.    O

Jobs    O
can    O
be    O
distributed    B-Supercomputer104358117
to    O
several    O
computers    O
over    O
a    O
network    O
,    O
giving    O
both    O
the    O
possibility    O
of    O
accelerating    O
the    O
work    O
by    O
using    O
more    O
resources    O
than    O
were    O
available    O
on    O
the    O
initiating    O
computer    O
alone    O
,    O
and    O
potentially    O
freeing    O
local    O
resources    O
for    O
other    O
tasks    O
.    O

Similarity    O
Matrix    O
of    O
Proteins    O
(    O
SIMAP    O
)    O
is    O
a    O
database    O
of    O
protein    O
similarities    O
created    O
using    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

SIMAP    O
used    O
the    O
Berkeley    O
Open    O
Infrastructure    O
for    O
Network    O
Computing    O
(    O
BOINC    O
)    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
platform    O
.    O

The    O
International    O
Parallel    O
and    O
Distributed    O
Processing    O
Symposium    O
(    O
or    O
IPDPS    O
)    O
is    O
an    O
annual    O
conference    O
for    O
engineers    O
and    O
scientists    O
to    O
present    O
recent    O
findings    O
in    O
the    O
fields    O
of    O
parallel    B-Supercomputer104358117
processing    I-Supercomputer104358117
and    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

In    O
computer    O
networking    O
and    O
databases    O
,    O
the    O
three    O
-    O
phase    O
commit    O
protocol    O
(    O
3PC    O
)    O
is    O
a    O
distributed    O
algorithm    O
which    O
lets    O
all    O
nodes    O
in    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
agree    O
to    O
commit    O
a    O
transaction    O
.    O

In    O
computer    O
science    O
,    O
global    O
file    O
system    O
has    O
historically    O
referred    O
to    O
a    O
distributed    B-Supercomputer104358117
virtual    O
name    O
space    O
built    O
on    O
a    O
set    O
of    O
local    O
file    O
systems    O
to    O
provide    O
transparent    O
access    O
to    O
multiple    O
,    O
potentially    O
distributed    O
,    O
systems    O
.    O

A    O
domain    O
application    O
protocol    O
(    O
DAP    O
)    O
is    O
the    O
set    O
of    O
rules    O
and    O
conventions    O
governing    O
the    O
interactions    O
between    O
participants    O
in    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
application    O
.    O

Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
is    O
also    O
used    O
to    O
improve    O
the    O
software    O
code    O
of    O
chess    O
engines    O
.    O

Sector    O
/    O
Sphere    O
is    O
an    O
open    O
source    O
software    O
suite    O
for    O
high    O
-    O
performance    O
distributed    O
data    O
storage    O
and    O
processing    B-Supercomputer104358117
.    O

A    O
matrix    O
clock    O
is    O
a    O
mechanism    O
for    O
capturing    O
chronological    O
and    O
causal    O
relationships    O
in    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
.    O

A    O
stress    O
test    O
for    O
a    O
system    O
running    O
24/7    O
or    O
that    O
will    O
perform    O
error    O
sensitive    O
tasks    O
such    O
as    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
or    O
"    O
folding    O
"    O
projects    O
may    O
differ    O
from    O
one    O
that    O
needs    O
to    O
be    O
able    O
to    O
run    O
a    O
single    O
game    O
with    O
reasonably    O
reliability    O
.    O

Vitanyi    O
has    O
worked    O
on    O
cellular    O
automata    O
,    O
computational    O
complexity    O
,    O
distributed    B-Supercomputer104358117
and    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
machine    O
learning    O
and    O
prediction    O
,    O
physics    O
of    O
computation    O
,    O
Kolmogorov    O
complexity    O
,    O
information    O
theory    O
and    O
quantum    O
computing    O
,    O
publishing    O
over    O
200    O
research    O
papers    O
and    O
some    O
books    O
.    O

Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

Distributed    O
deadlocks    O
can    O
occur    O
in    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
when    O
distributed    O
transactions    O
or    O
concurrency    O
control    O
is    O
being    O
used    O
.    O

Fabián    O
E.    O
Bustamante    O
is    O
an    O
Argentinian    O
-    O
American    O
computer    O
scientist    O
specializing    O
in    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
and    O
computer    O
networking    O
.    O

ZeroMQ    O
(    O
also    O
spelled    O
ØMQ    O
,    O
0MQ    O
or    O
ZMQ    O
)    O
is    O
a    O
high    O
-    O
performance    O
asynchronous    O
messaging    O
library    O
,    O
aimed    O
at    O
use    O
in    O
distributed    B-Supercomputer104358117
or    O
concurrent    O
applications    O
.    O

He    O
is    O
also    O
interested    O
in    O
how    O
insights    O
from    O
both    O
computer    O
science    O
and    O
biology    O
can    O
be    O
used    O
to    O
affect    O
the    O
other    O
field    O
,    O
in    O
particular    O
how    O
algorithms    O
from    O
nature    O
can    O
be    O
used    O
in    O
order    O
to    O
improve    O
algorithms    O
in    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
.    O

"    O
Computer    O
Architecture    O
,    O
Systems    O
and    O
Networks    O
"    O
(    O
Distributed    B-Supercomputer104358117
Systems    I-Supercomputer104358117
,    O
Security    O
,    O
Embedded    O
Systems    O
,    O
Fault    O
tolerance    O
)    O

In    O
an    O
effort    O
to    O
scale    O
with    O
larger    O
amounts    O
of    O
indexed    O
information    O
,    O
the    O
search    O
engine    O
's    O
architecture    O
may    O
involve    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
where    O
the    O
search    O
engine    O
consists    O
of    O
several    O
machines    O
operating    O
in    O
unison    O
.    O

distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
to    O
parallelize    O
data    O
analyses    O
,    O

AK    O
used    O
a    O
1024-bit    O
key    O
that    O
was    O
believed    O
to    O
be    O
large    O
enough    O
to    O
be    O
computationally    O
infeasible    O
to    O
break    O
without    O
a    O
concerted    O
distributed    B-Supercomputer104358117
effort    O
,    O
or    O
the    O
discovery    O
of    O
a    O
flaw    O
that    O
could    O
be    O
used    O
to    O
break    O
the    O
encryption    O
.    O

It    O
was    O
then    O
applied    O
to    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
and    O
to    O
game    O
theory    O
,    O
where    O
it    O
allows    O
to    O
express    O
that    O
the    O
rationality    O
of    O
the    O
players    O
,    O
the    O
rules    O
of    O
the    O
game    O
and    O
the    O
set    O
of    O
players    O
are    O
commonly    O
known    O
.    O

In    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
,    O
failure    O
transparency    O
refers    O
to    O
the    O
extent    O
to    O
which    O
errors    O
and    O
subsequent    O
recoveries    O
of    O
hosts    O
and    O
services    O
within    O
the    O
system    O
are    O
invisible    O
to    O
users    O
and    O
applications    O
.    O

Product    O
Family    O
Engineering    O
(    O
methods    O
,    O
architectures    O
,    O
techniques    O
)    O
in    O
distributed    B-Supercomputer104358117
and    O
ambient    O
computing    O
,    O
and    O

In    O
computing    O
,    O
Web    O
-    O
Based    O
Enterprise    O
Management    O
(    O
WBEM    O
)    O
comprises    O
a    O
set    O
of    O
systems    O
-    O
management    O
technologies    O
developed    O
to    O
unify    O
the    O
management    O
of    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
environments    O
.    O

Timing    O
failure    O
is    O
a    O
failure    O
of    O
a    O
process    O
,    O
or    O
part    O
of    O
a    O
process    O
,    O
in    O
a    O
synchronous    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
or    O
real    O
-    O
time    O
system    O
to    O
meet    O
limits    O
set    O
on    O
execution    O
time    O
,    O
message    O
delivery    O
,    O
clock    O
drift    O
rate    O
,    O
or    O
clock    O
skew    O
.    O

A    O
vector    O
clock    O
is    O
an    O
algorithm    O
for    O
generating    O
a    O
partial    O
ordering    O
of    O
events    O
in    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
and    O
detecting    O
causality    O
violations    O
.    O

A    O
logical    O
clock    O
is    O
a    O
mechanism    O
for    O
capturing    O
chronological    O
and    O
causal    O
relationships    O
in    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
.    O

E    O
is    O
an    O
object    O
-    O
oriented    O
programming    O
language    O
for    O
secure    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
created    O
by    O
Mark    O
S.    O
Miller    O
,    O
Dan    O
Bornstein    O
,    O
and    O
others    O
at    O
Electric    O
Communities    O
in    O
1997    O
.    O

Distributed    B-Supercomputer104358117
programming    I-Supercomputer104358117
is    O
just    O
a    O
matter    O
of    O
sending    O
messages    O
to    O
remote    O
objects    O
(    O
objects    O
in    O
other    O
vats    O
)    O
.    O

Predictor@home    O
was    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
that    O
used    O
BOINC    O
.    O

Climateprediction.net    O
(    O
CPDN    O
)    O
is    O
a    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
project    O
to    O
investigate    O
and    O
reduce    O
uncertainties    O
in    O
climate    O
modelling    O
.    O

Distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117

Masahiko    O
and    O
Chia    O
go    O
onto    O
the    O
Net    O
to    O
get    O
the    O
help    O
of    O
Chia    O
’s    O
friend    O
Zona    O
Rosa    O
,    O
who    O
claims    O
to    O
be    O
the    O
leader    O
of    O
a    O
Mexico    O
City    O
“    O
girl    O
gang    O
”    O
and    O
from    O
the    O
Walled    O
City    O
who    O
are    O
protecting    O
the    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
of    O
which    O
Masahiko    O
is    O
a    O
part    O
.    O

Cairo    O
used    O
distributed    B-Supercomputer104358117
computing    I-Supercomputer104358117
concepts    O
to    O
make    O
information    O
available    O
quickly    O
and    O
seamlessly    O
across    O
a    O
worldwide    O
network    O
of    O
computers    O
.    O

Due    O
to    O
the    O
massive    O
computing    O
needs    O
and    O
to    O
protect    O
it    O
from    O
direct    O
attack    O
,    O
Skynet    O
utilized    O
a    O
large    B-Supercomputer104358117
network    I-Supercomputer104358117
of    I-Supercomputer104358117
computers    I-Supercomputer104358117
that    O
would    O
be    O
nearly    O
impossible    O
to    O
deactivate    O
completely    O
.    O

A    O
quorum    O
is    O
the    O
minimum    O
number    O
of    O
votes    O
that    O
a    O
distributed    O
transaction    O
has    O
to    O
obtain    O
in    O
order    O
to    O
be    O
allowed    O
to    O
perform    O
an    O
operation    O
in    O
a    O
distributed    B-Supercomputer104358117
system    I-Supercomputer104358117
.    O

He    O
is    O
the    O
author    O
of    O
many    O
conference    O
and    O
journal    O
articles    O
,    O
mainly    O
on    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
technologies    O
for    O
enterprise    O
computing    O
systems    O
.    O

His    O
work    O
in    O
the    O
R&D    O
department    O
of    O
the    O
company    O
focused    O
on    O
the    O
development    O
of    O
software    O
component    O
-    O
based    O
architectures    O
for    O
distributed    B-Supercomputer104358117
systems    I-Supercomputer104358117
,    O
and    O
its    O
applications    O
for    O
multimedia    O
services    O
over    O
broadband    O
networks    O
and    O
new    O
generation    O
mobile    O
phones    O
.    O

Distributed    B-Supercomputer104358117
Computing    I-Supercomputer104358117

EPCC    O
was    O
established    O
in    O
1990    O
,    O
following    O
on    O
from    O
the    O
earlier    O
Edinburgh    B-Supercomputer104358117
Concurrent    I-Supercomputer104358117
Supercomputer    I-Supercomputer104358117
Project    I-Supercomputer104358117
and    O
chaired    O
by    O
Jeffery    O
Collins    O
from    O
1991    O
.    O

The    O
UNIVAC    O
LARC    B-Supercomputer104358117
,    O
short    O
for    O
the    O
"    O
Livermore    O
Advanced    O
Research    O
Computer    O
"    O
,    O
is    O
a    O
mainframe    O
computer    O
designed    O
to    O
a    O
requirement    O
published    O
by    O
Edward    O
Teller    O
in    O
order    O
to    O
run    O
hydrodynamic    O
simulations    O
for    O
nuclear    O
weapon    O
design    O
.    O

The    O
new    O
firm    O
's    O
first    O
job    O
was    O
fixing    O
a    O
language    O
compiler    O
on    O
the    O
UNIVAC    B-Supercomputer104358117
LARC    I-Supercomputer104358117
computer    O
,    O
which    O
was    O
being    O
used    O
by    O
the    O
United    O
States    O
Navy    O
.    O

SGI    B-Supercomputer104358117
Onyx    I-Supercomputer104358117
,    O
code    O
named    O
Eveready    B-Supercomputer104358117
(    O
deskside    O
models    O
)    O
and    O
Terminator    B-Supercomputer104358117
(    O
rackmount    O
models    O
)    O
,    O
is    O
a    O
series    O
of    O
visualization    O
systems    O
designed    O
and    O
manufactured    O
by    O
SGI    O
,    O
introduced    O
in    O
1993    O
and    O
offered    O
in    O
two    O
models    O
,    O
deskside    O
and    O
rackmount    O
.    O

CitySpace    O
installations    O
consisted    O
of    O
networked    O
Silicon    O
Graphics    O
Onyx    B-Supercomputer104358117
Reality    I-Supercomputer104358117
Engine    I-Supercomputer104358117
supercomputers    O
,    O
Macintosh    O
-    O
based    O
3D    O
modeling    O
and    O
graphics    O
software    O
,    O
videoconferencing    O
and    O
data    O
projection    O
.    O

It    O
first    O
appeared    O
in    O
Irix    O
6.5.10    O
as    O
a    O
solution    O
for    O
creating    O
desktops    O
on    O
SGI    B-Supercomputer104358117
Onyx    I-Supercomputer104358117
Visualization    O
Systems    O
(    O
their    O
so    O
-    O
called    O
Reality    O
Centers    O
)    O
which    O
took    O
advantage    O
of    O
multiple    O
Raster    O
/    O
Geometry    O
Managers    O
in    O
multiple    O
pipelines    O
.    O

The    O
CDC    B-Supercomputer104358117
6600    I-Supercomputer104358117
was    O
the    O
flagship    O
mainframe    O
supercomputer    O
of    O
the    O
6000    B-Supercomputer104358117
series    I-Supercomputer104358117
of    O
computer    O
systems    O
manufactured    O
by    O
Control    O
Data    O
Corporation    O
.    O

The    O
CDC    B-Supercomputer104358117
6000    I-Supercomputer104358117
series    I-Supercomputer104358117
included    O
four    O
basic    O
models    O
,    O
the    O
CDC    B-Supercomputer104358117
6400    I-Supercomputer104358117
,    O
the    O
CDC    B-Supercomputer104358117
6500    I-Supercomputer104358117
,    O
the    O
CDC    O
6600    O
,    O
and    O
the    O
CDC    O
6700    O
.    O

All    O
but    O
the    O
first    O
seven    O
CDC    B-Supercomputer104358117
6000    I-Supercomputer104358117
series    I-Supercomputer104358117
machines    O
could    O
be    O
configured    O
with    O
an    O
optional    O
Extended    O
Core    O
Storage    O
(    O
ECS    O
)    O
system    O
.    O

The    O
names    O
SCOPE    O
and    O
COMPASS    O
were    O
used    O
by    O
CDC    O
for    O
both    O
the    O
CDC    B-Supercomputer104358117
6000    I-Supercomputer104358117
series    I-Supercomputer104358117
,    O
including    O
the    O
6600    O
,    O
and    O
the    O
CDC    O
3000    O
series    O
:    O

The    O
four    O
threads    O
were    O
implemented    O
using    O
a    O
Barrel    O
processor    O
design    O
similar    O
to    O
that    O
used    O
in    O
the    O
CDC    B-Supercomputer104358117
6000    I-Supercomputer104358117
series    I-Supercomputer104358117
and    O
the    O
Denelcor    B-Supercomputer104358117
HEP    I-Supercomputer104358117
.    O

The    O
TOP500    B-Supercomputer104358117
organization    O
's    O
semiannual    O
list    O
of    O
the    O
500    O
fastest    O
supercomputers    O
often    O
includes    O
many    O
clusters    O
,    O
e.g.    O
the    O
world    O
's    O
fastest    O
machine    O
in    O
2011    O
was    O
the    O
K    B-Supercomputer104358117
computer    I-Supercomputer104358117
which    O
has    O
a    O
distributed    O
memory    O
,    O
cluster    O
architecture    O
.    O

While    O
early    O
supercomputers    O
excluded    O
clusters    O
and    O
relied    O
on    O
shared    O
memory    O
,    O
in    O
time    O
some    O
of    O
the    O
fastest    O
supercomputers    O
(    O
e.g.    O
the    O
K    B-Supercomputer104358117
computer    I-Supercomputer104358117
)    O
relied    O
on    O
cluster    O
architectures    O
.    O

K    B-Supercomputer104358117
computer    I-Supercomputer104358117

Fujitsu    O
produces    O
the    O
SPARC    O
compliant    O
CPU    O
(    O
SPARClite    O
)    O
,    O
the    O
"    O
Venus    O
"    O
128    O
GFLOP    O
SPARC64    O
VIIIfx    O
model    O
is    O
included    O
in    O
the    O
K    B-Supercomputer104358117
computer    I-Supercomputer104358117
,    O
the    O
world    O
's    O
fastest    O
supercomputer    O
in    O
June    O
2011    O
with    O
a    O
rating    O
of    O
over    O
8    O
petaflops    O
,    O
and    O
in    O
November    O
2011    O
,    O
K    O
became    O
the    O
first    O
computer    O
to    O
top    O
10    O
petaflops    O
in    O
September    O
2011    O
.    O

The    O
K    B-Supercomputer104358117
computer    I-Supercomputer104358117
–    O
named    O
for    O
the    O
Japanese    O
word    O
,    O
meaning    O
10    O
quadrillion    O
(    O
1016    O
)    O
–    O
is    O
a    O
supercomputer    O
manufactured    O
by    O
Fujitsu    O
,    O
currently    O
installed    O
at    O
the    O
Riken    O
Advanced    O
Institute    O
for    O
Computational    O
Science    O
campus    O
in    O
Kobe    O
,    O
Hyōgo    O
Prefecture    O
,    O
Japan    O
.    O

Pleiades    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117

It    O
was    O
slowly    O
phased    O
out    O
as    O
its    O
successors    O
at    O
NAS    O
,    O
the    O
petascale    B-Supercomputer104358117
Pleiades    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
and    O
the    O
Endeavour    B-Supercomputer104358117
shared    O
-    O
memory    O
system    O
,    O
expanded    O
to    O
meet    O
with    O
NASA    O
’s    O
growing    O
high    O
-    O
end    O
computing    O
needs    O
.    O

On    O
November    O
13    O
,    O
2006    O
,    O
Cray    O
announced    O
a    O
new    O
system    O
,    O
the    O
Cray    B-Supercomputer104358117
XMT    I-Supercomputer104358117
,    O
based    O
on    O
the    O
MTA    O
series    O
of    O
machines    O
.    O

SAGA-220    B-Supercomputer104358117
,    O
a    O
220-TeraFLOPS    O
supercomputer    O
built    O
by    O
ISRO    O

PARAM    B-Supercomputer104358117
series    O
of    O
supercomputers    O
by    O
the    O
Centre    O
for    O
Development    O
of    O
Advanced    O
Computing    O

Vijay    O
P.    O
Bhatkar    O
,    O
Inventor    O
of    O
Param    B-Supercomputer104358117
Super    I-Supercomputer104358117
computer    I-Supercomputer104358117

It    O
has    O
a    O
PARAM    B-Supercomputer104358117
10000    I-Supercomputer104358117
at    O
the    O
core    O
of    O
its    O
IT    O
infrastructure    O
.    O

Some    O
companies    O
funded    O
by    O
Khosla    O
Ventures    O
include    O
:    O
Academia.edu    O
,    O
Amyris    O
,    O
Bloom    O
Energy    O
,    O
DB    O
Networks    O
,    O
EcoMotors    O
,    O
GreatPoint    O
Energy    O
,    O
Neverware    O
,    O
Panzura    O
,    O
SeaMicro    B-Supercomputer104358117
,    O
Tapingo    O
,    O
Cogenra    O
Solar    O
,    O
Snip.it    O
,    O
Instacart    O
,    O
Relcy    O
and    O
Thync    O
.    O

In    O
2004    O
,    O
Cray    O
completed    O
the    O
Red    B-Supercomputer104358117
Storm    I-Supercomputer104358117
system    O
for    O
Sandia    O
National    O
Laboratories    O
.    O

Red    B-Supercomputer104358117
Storm    I-Supercomputer104358117
(computing)    I-Supercomputer104358117
,    O
computing    O
architecture    O

There    O
is    O
also    O
list    O
Green    O
Graph    O
500    O
,    O
which    O
uses    O
same    O
performance    O
metric    O
,    O
but    O
sorts    O
list    O
according    O
to    O
performance    O
per    O
Watt    O
,    O
like    O
Green    B-Supercomputer104358117
500    I-Supercomputer104358117
works    O
with    O
Top500    B-Supercomputer104358117
(    O
HPL    O
)    O
.    O

Green500    B-Supercomputer104358117

They    O
select    O
a    O
processor    O
on    O
the    O
basis    O
of    O
the    O
performance    B-Supercomputer104358117
per    I-Supercomputer104358117
watt    I-Supercomputer104358117
of    O
the    O
processor    O
.    O

,    O
it    O
is    O
also    O
ranked    O
as    O
the    O
fourth    O
most    O
energy    O
-    O
efficient    O
supercomputer    O
in    O
Green500    B-Supercomputer104358117
,    O
with    O
an    O
efficiency    O
of    O
6,051.30    O
MFLOPS/W    B-Supercomputer104358117
It    O
was    O
designed    O
by    O
the    O
National    O
Research    O
Center    O
of    O
Parallel    O
Computer    O
Engineering    O
&    O
Technology    O
(    O
NRCPC    O
)    O
and    O
is    O
located    O
at    O
the    O
National    O
Supercomputing    O
Center    O
in    O
Wuxi    O
in    O
the    O
city    O
of    O
Wuxi    O
,    O
in    O
Jiangsu    O
province    O
,    O
China    O
.    O

Performance    B-Supercomputer104358117
per    I-Supercomputer104358117
watt    I-Supercomputer104358117

Although    O
the    O
K    O
computer    O
reported    O
the    O
highest    O
total    O
power    O
consumption    O
of    O
any    O
2011    O
TOP500    O
supercomputer    O
(    O
9.89    O
MW    O
–    O
the    O
equivalent    O
of    O
almost    O
10,000    O
suburban    O
homes    O
)    O
,    O
it    O
is    O
relatively    O
efficient    O
,    O
achieving    O
824.6    O
GFlop/kW    B-Supercomputer104358117
This    O
is    O
29.8%    O
more    O
efficient    O
than    O
China    O
's    O
NUDT    O
TH    O
MPP    O
(    O
ranked    O
#    O
2    O
in    O
2011    O
)    O
,    O
and    O
225.8%    O
more    O
efficient    O
than    O
Oak    O
Ridge    O
's    O
Jaguar    O
-    O
Cray    O
XT5-HE    O
(    O
ranked    O
#    O
3    O
in    O
2011    O
)    O
.    O

The    O
Fujitsu    O
FR    O
-    O
V    O
(    O
Fujitsu    O
RISC    O
-    O
VLIW    O
)    O
is    O
one    O
of    O
the    O
very    O
few    O
processors    O
ever    O
able    O
to    O
process    O
both    O
a    O
very    O
long    O
instruction    O
word    O
(    O
VLIW    O
)    O
and    O
vector    O
processor    O
instructions    O
at    O
the    O
same    O
time    O
,    O
increasing    O
throughput    O
with    O
high    O
parallel    B-Supercomputer104358117
computing    I-Supercomputer104358117
while    O
increasing    O
performance    B-Supercomputer104358117
per    I-Supercomputer104358117
watt    I-Supercomputer104358117
and    O
hardware    O
efficiency    O
.    O

The    O
Bonnell    O
microarchitecture    O
therefore    O
represents    O
a    O
partial    O
revival    O
of    O
the    O
principles    O
used    O
in    O
earlier    O
Intel    O
designs    O
such    O
as    O
P5    O
and    O
the    O
i486    O
,    O
with    O
the    O
sole    O
purpose    O
of    O
enhancing    O
the    O
performance    B-Supercomputer104358117
per    I-Supercomputer104358117
watt    I-Supercomputer104358117
ratio    O
.    O

Compute    O
Node    O
Linux    O
(    O
CNL    O
)    O
is    O
a    O
runtime    O
environment    O
based    O
on    O
the    O
Linux    O
kernel    O
for    O
the    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT5    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT6    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XE6    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
XK6    I-Supercomputer104358117
supercomputer    O
systems    O
based    O
on    O
SUSE    O
Linux    O
Enterprise    O
Server    O
.    O

The    O
second    O
generation    O
,    O
launched    O
as    O
the    O
XT5h    B-Supercomputer104358117
,    O
allowed    O
a    O
system    O
to    O
combine    O
compute    O
elements    O
of    O
various    O
types    O
into    O
a    O
common    O
system    O
,    O
sharing    O
infrastructure    O
.    O

By    O
2009    O
,    O
the    O
largest    O
computer    O
system    O
Cray    O
had    O
delivered    O
was    O
the    O
XT5    B-Supercomputer104358117
system    O
at    O
National    O
Center    O
for    O
Computational    O
Sciences    O
at    O
Oak    O
Ridge    O
National    O
Laboratories    O
.    O

Chaining    B-Supercomputer104358117
(vector    I-Supercomputer104358117
processing)    I-Supercomputer104358117

It    O
was    O
marketed    O
to    O
compete    O
with    O
the    O
T90    B-Supercomputer104358117
from    O
Cray    O
Research    O
.    O

After    O
Chen    O
's    O
departure    O
,    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
C90    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
T90    I-Supercomputer104358117
were    O
developed    O
on    O
the    O
original    O
Cray-1    O
architecture    O
but    O
achieved    O
much    O
greater    O
performance    O
via    O
multiple    O
additional    O
processors    O
,    O
faster    O
clocks    O
,    O
and    O
wider    O
vector    O
pipes    O
.    O

He    O
is    O
also    O
the    O
architect    O
of    O
System    B-Supercomputer104358117
X    I-Supercomputer104358117
,    O
one    O
of    O
the    O
world    O
's    O
fastest    O
and    O
least    O
expensive    O
supercomputers    O
.    O

Big    B-Supercomputer104358117
Mac    I-Supercomputer104358117
(supercomputer)    I-Supercomputer104358117
,    O
a    O
supercomputer    O
created    O
by    O
Virginia    O
Tech    O
in    O
2003    O

ASCI    B-Supercomputer104358117
Blue    I-Supercomputer104358117
Pacific    I-Supercomputer104358117

Announced    O
in    O
July    O
1982    O
,    O
the    O
FACOM    O
VP    O
were    O
the    O
first    O
of    O
the    O
three    O
initial    O
Japanese    O
commercial    O
supercomputers    O
,    O
followed    O
by    O
the    O
Hitachi    O
HITAC    B-Supercomputer104358117
S-810    I-Supercomputer104358117
in    O
August    O
1982    O
and    O
the    O
NEC    O
SX-2    B-Supercomputer104358117
in    O
April    O
1983    O
.    O

His    O
work    O
with    O
protected    O
user    O
-    O
level    O
communication    O
has    O
contributed    O
significantly    O
to    O
the    O
Remote    O
Direct    O
Memory    O
Access    O
(    O
RDMA    O
)    O
mechanism    O
and    O
Virtual    B-Supercomputer104358117
Interface    I-Supercomputer104358117
Architecture    I-Supercomputer104358117
standard    O
and    O
Infiniband    B-Supercomputer104358117
standard    O
,    O
which    O
are    O
the    O
communication    O
mechanism    O
for    O
the    O
Direct    O
Access    O
File    O
System    O
(    O
DAFS    O
)    O
.    O

The    O
four    O
threads    O
were    O
implemented    O
using    O
a    O
Barrel    O
processor    O
design    O
similar    O
to    O
that    O
used    O
in    O
the    O
CDC    B-Supercomputer104358117
6000    I-Supercomputer104358117
series    I-Supercomputer104358117
and    O
the    O
Denelcor    B-Supercomputer104358117
HEP    I-Supercomputer104358117
.    O

This    O
system    O
,    O
with    O
over    O
150,000    O
processing    O
cores    O
,    O
was    O
dubbed    O
"    O
Jaguar    B-Supercomputer104358117
"    O
and    O
was    O
the    O
second    O
fastest    O
system    O
in    O
the    O
world    O
for    O
the    O
LINPACK    O
benchmark    O
,    O
the    O
fastest    O
system    O
available    O
for    O
open    O
science    O
and    O
the    O
first    O
system    O
to    O
exceed    O
a    O
petaflops    O
sustained    O
performance    O
on    O
a    O
64-bit    O
scientific    O
application    O
.    O

This    O
system    O
,    O
with    O
over    O
224,000    O
processing    O
cores    O
,    O
was    O
dubbed    O
"    O
Jaguar    B-Supercomputer104358117
"    O
and    O
was    O
the    O
fastest    O
computer    O
in    O
the    O
world    O
as    O
measured    O
by    O
the    O
LINPACK    O
benchmark    O
at    O
the    O
speed    O
of    O
1.75    O
petaflops    O
until    O
being    O
surpassed    O
by    O
the    O
Tianhe-1A    B-Supercomputer104358117
in    O
October    O
2010    O
.    O

An    O
example    O
of    O
project    O
is    O
the    O
project    O
Cajal    B-Supercomputer104358117
Blue    I-Supercomputer104358117
Brain    I-Supercomputer104358117
(    O
Spanish    O
participation    O
in    O
Blue    O
Brain    O
Project    O
)    O
.    O

Human    O
Brain    O
Project    O
and    O
Blue    B-Supercomputer104358117
Brain    I-Supercomputer104358117
Project    I-Supercomputer104358117
(    O
EPFL    O
)    O
(    O
5000    O
m2    O
)    O

In    O
2004    O
,    O
Henry    O
Markram    O
,    O
lead    O
researcher    O
of    O
the    O
"    O
Blue    B-Supercomputer104358117
Brain    I-Supercomputer104358117
Project    I-Supercomputer104358117
"    O
,    O
stated    O
that    O
"    O
it    O
is    O
not    O
[    O
their    O
]    O
goal    O
to    O
build    O
an    O
intelligent    O
neural    O
network    O
"    O
,    O
based    O
solely    O
on    O
the    O
computational    O
demands    O
such    O
a    O
project    O
would    O
have    O
.    O

He    O
was    O
formerly    O
co    O
-    O
Director    O
of    O
the    O
Blue    B-Supercomputer104358117
Brain    I-Supercomputer104358117
Project    I-Supercomputer104358117
at    O
the    O
École    O
Polytechnique    O
Fédérale    O
de    O
Lausanne    O
located    O
on    O
the    O
Campus    O
Biotech    O
in    O
Geneva    O
,    O
Switzerland    O
.    O

Blue    B-Supercomputer104358117
Brain    I-Supercomputer104358117
Project    I-Supercomputer104358117
,    O
an    O
attempt    O
to    O
create    O
a    O
synthetic    O
brain    O
by    O
reverse    O
-    O
engineering    O
the    O
mammalian    O
brain    O
down    O
to    O
the    O
molecular    O
level    O
.    O

Cray    O
set    O
up    O
Cray    O
Research    O
Superservers    O
,    O
Inc.    O
(    O
later    O
the    O
Business    O
Systems    O
Division    O
)    O
to    O
sell    O
this    O
system    O
as    O
the    O
Cray    B-Supercomputer104358117
S-MP    I-Supercomputer104358117
,    O
later    O
replacing    O
it    O
with    O
the    O
Cray    B-Supercomputer104358117
CS6400    I-Supercomputer104358117
.    O

The    O
IA-32    O
version    O
supports    O
up    O
to    O
four    O
physical    O
processors    O
and    O
up    O
to    O
4    O
GB    O
RAM    O
;    O
the    O
x64    O
version    O
is    O
capable    O
of    O
addressing    O
up    O
to    O
32    O
GB    O
of    O
RAM    O
and    O
also    O
supports    O
Non-Uniform    B-Supercomputer104358117
Memory    I-Supercomputer104358117
Access    I-Supercomputer104358117
.    O

This    O
edition    O
also    O
supports    O
Non-Uniform    B-Supercomputer104358117
Memory    I-Supercomputer104358117
Access    I-Supercomputer104358117
(    O
NUMA    O
)    O
.    O

IIS    O
8.0    O
includes    O
SNI    O
(    O
binding    O
SSL    O
to    O
hostnames    O
rather    O
than    O
IP    O
addresses    O
)    O
,    O
Application    O
Initialization    O
,    O
centralized    O
SSL    O
certificate    O
support    O
,    O
and    O
multicore    O
scaling    O
on    O
NUMA    B-Supercomputer104358117
hardware    O
,    O
among    O
other    O
new    O
features    O
.    O

Multicore    O
scaling    O
on    O
NUMA    B-Supercomputer104358117
hardware    O
:    O
IIS    O
8.0    O
provides    O
several    O
configuration    O
options    O
that    O
optimize    O
performance    O
on    O
systems    O
that    O
run    O
NUMA    O
,    O
such    O
as    O
running    O
several    O
worker    O
processes    O
under    O
one    O
application    O
pool    O
,    O
using    O
soft    O
or    O
hard    O
affinity    O
and    O
more    O
.    O

Since    O
launch    O
,    O
SiSoftware    O
provides    O
benchmarks    O
that    O
show    O
the    O
power    O
of    O
emerging    O
new    O
technologies    O
like    O
multi    O
-    O
core    O
,    O
WMMX2    O
,    O
WMMX    O
,    O
AMD64/EM64T    O
/    O
x86    O
-    O
64    O
,    O
IA-64    O
,    O
NUMA    B-Supercomputer104358117
,    O
SMT    O
,    O
SMP    O
,    O
SSE4    O
,    O
SSSE3    O
,    O
SSE3    O
,    O
SSE2    O
,    O
SSE    O
,    O
MMX    O
,    O
Java    O
and    O
.NET    O
.    O

As    O
well    O
as    O
virtualization    O
of    O
the    O
resources    O
of    O
a    O
single    O
machine    O
,    O
multiple    O
independent    O
nodes    O
in    O
a    O
cluster    O
can    O
be    O
combined    O
and    O
accessed    O
as    O
a    O
single    O
virtual    O
NUMA    B-Supercomputer104358117
machine    O
.    O

In    O
this    O
model    O
,    O
the    O
two    O
additional    O
ceilings    O
represent    O
the    O
absence    O
of    O
software    O
prefetching    O
and    O
NUMA    B-Supercomputer104358117
organization    O
of    O
memory    O
.    O

Already    O
available    O
in    O
literature    O
there    O
are    O
extensions    O
that    O
take    O
into    O
account    O
the    O
impact    O
of    O
NUMA    B-Supercomputer104358117
organization    O
of    O
memory    O
,    O
of    O
out    O
-    O
of    O
-    O
order    O
execution    O
,    O
of    O
memory    O
latencies    O
,    O
and    O
to    O
model    O
at    O
a    O
finer    O
grain    O
the    O
cache    O
hierarchy    O
in    O
order    O
to    O
better    O
understand    O
what    O
is    O
actually    O
limiting    O
performance    O
and    O
drive    O
the    O
optimization    O
process    O
.    O

Non    O
-    O
Uniform    O
Memory    O
Architecture    O
(    O
NUMA    O
)    O
,    O
which    O
involves    O
the    O
non-uniform    B-Supercomputer104358117
memory    I-Supercomputer104358117
access    I-Supercomputer104358117
.    O

The    O
performance    O
of    O
big    O
memory    O
systems    O
depends    O
on    O
how    O
the    O
CPU    O
's    O
or    O
CPU    O
cores    O
access    O
the    O
memory    O
,    O
via    O
a    O
conventional    O
memory    O
controller    O
or    O
via    O
NUMA    O
(    O
non-uniform    B-Supercomputer104358117
memory    I-Supercomputer104358117
access    I-Supercomputer104358117
)    O
.    O

Unified    O
Parallel    O
C    O
(    O
UPC    O
)    O
is    O
an    O
extension    O
of    O
the    O
C    O
programming    O
language    O
designed    O
for    O
high    O
-    O
performance    O
computing    O
on    O
large    O
-    O
scale    O
parallel    B-Supercomputer104358117
machine    I-Supercomputer104358117
,    O
including    O
those    O
with    O
a    O
common    O
global    O
address    O
space    O
(    O
SMP    O
and    O
NUMA    B-Supercomputer104358117
)    O
and    O
those    O
with    O
distributed    O
memory    O
(    O
e.g.    O
clusters    B-Supercomputer104358117
)    O
.    O

Although    O
most    O
computer    O
clusters    O
are    O
permanent    O
fixtures    O
,    O
attempts    O
at    O
flash    B-Supercomputer104358117
mob    I-Supercomputer104358117
computing    I-Supercomputer104358117
have    O
been    O
made    O
to    O
build    O
short    O
-    O
lived    O
clusters    O
for    O
specific    O
computations    O
.    O

Thunderbird    B-Supercomputer104358117
(supercomputer)    I-Supercomputer104358117
,    O
a    O
supercomputer    O
cluster    O
at    O
Sandia    O
National    O
Laboratories    O

This    O
supercomputing    O
center    O
has    O
been    O
planned    O
to    O
welcome    O
the    O
first    O
French    O
Petascale    O
machine    O
Curie    B-Supercomputer104358117
,    O
funded    O
by    O
GENCI    O
for    O
the    O
PRACE    O
Research    O
Infrastructure    O
,    O
and    O
the    O
next    O
generation    O
of    O
the    O
CCRT    O
Computing    O
Center    O
.    O

Tsubame    B-Supercomputer104358117
is    O
a    O
series    O
of    O
supercomputers    O
that    O
operates    O
at    O
the    O
GSIC    O
Center    O
at    O
the    O
Tokyo    O
Institute    O
of    O
Technology    O
in    O
Japan    O
,    O
designed    O
by    O
Satoshi    O
Matsuoka    O
.    O

The    O
GSIC    O
Center    O
at    O
the    O
Tokyo    O
Institute    O
of    O
Technology    O
houses    O
the    O
Tsubame    B-Supercomputer104358117
2.0    O
supercomputer    O
,    O
which    O
has    O
a    O
peak    O
of    O
2,288    O
TFLOPS    O
and    O
in    O
June    O
2011    O
ranked    O
5th    O
in    O
the    O
world    O
.    O

ASCI    B-Supercomputer104358117
Purple    I-Supercomputer104358117

Lawrence    O
Livermore    O
National    O
Laboratory    O
-    O
Hyperion    O
Project    O
(    O
2010    O
)    O
,    O
110TF    O
Graph    O
supercomputer    O
(    O
2009    O
)    O
and    O
222TF    O
Peloton    B-Supercomputer104358117
Project    I-Supercomputer104358117
consisting    O
of    O
Rhea    O
,    O
Zeus    O
,    O
Minos    O
and    O
Atlas    O
Clusters    O
(    O
2007    O
)    O

UiT    O
-    O
University    O
of    O
Tromsø    O
,    O
owner    O
of    O
the    O
Stallo    B-Supercomputer104358117
supercomputer    O

On    O
November    O
12    O
,    O
2012    O
,    O
the    O
TOP500    O
list    O
certified    O
Titan    B-Supercomputer104358117
as    O
the    O
world    O
's    O
fastest    O
supercomputer    O
per    O
the    O
LINPACK    O
benchmark    O
,    O
at    O
17.59    O
petaFLOPS    O
.    O

As    O
of    O
June    O
16    O
,    O
2013    O
,    O
this    O
includes    O
7    O
of    O
the    O
top    O
10    O
,    O
including    O
the    O
current    O
fastest    O
system    O
on    O
the    O
list    O
-    O
China    O
's    O
Tianhe-2    O
and    O
the    O
second    O
fastest    O
,    O
the    O
Titan    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
at    O
Oak    O
Ridge    O
National    O
Laboratory    O
(    O
pictured    O
on    O
the    O
right    O
)    O
.    O

In    O
October    O
2012    O
Cray    O
announced    O
the    O
Cray    B-Supercomputer104358117
XK7    I-Supercomputer104358117
which    O
supports    O
the    O
NVIDIA    O
Kepler    O
GPGPU    O
and    O
announced    O
that    O
the    O
ORNL    O
Jaguar    O
system    O
would    O
be    O
upgraded    O
to    O
an    O
XK7    O
(    O
renamed    O
"    O
Titan    B-Supercomputer104358117
"    O
)    O
and    O
capable    O
of    O
over    O
20    O
petaflops    O
.    O

Titan    B-Supercomputer104358117
was    O
the    O
world    O
's    O
fastest    O
supercomputer    O
as    O
measured    O
by    O
the    O
LINPACK    O
benchmark    O
until    O
the    O
introduction    O
of    O
the    O
Tianhe-2    B-Supercomputer104358117
in    O
2013    O
,    O
which    O
is    O
substantially    O
faster    O
.    O

Vayu    B-Supercomputer104358117
(computer    I-Supercomputer104358117
cluster)    I-Supercomputer104358117
,    O
an    O
Australian    O
computer    O
system    O
located    O
in    O
Canberra    O
,    O
Australia    O

HPCx    B-Supercomputer104358117
,    O
a    O
supercomputer    O
(    O
closed    O
,    O
replaced    O
by    O
the    O
UK    O
national    O
supercomputing    O
service    O
,    O
HECToR    B-Supercomputer104358117
,    O
based    O
in    O
Edinburgh    O
)    O
.    O

Throughout    O
,    O
Cray    O
continued    O
to    O
be    O
the    O
performance    O
leader    O
,    O
continually    O
beating    O
the    O
competition    O
with    O
a    O
series    O
of    O
machines    O
that    O
led    O
to    O
the    O
Cray-2    B-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
.    O

Seymour    O
Cray    O
continued    O
working    O
,    O
this    O
time    O
on    O
the    O
Cray-2    B-Supercomputer104358117
,    O
though    O
it    O
only    O
ended    O
up    O
being    O
marginally    O
faster    O
than    O
the    O
Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
,    O
developed    O
by    O
another    O
team    O
at    O
the    O
company    O
.    O

Cray    O
Research    O
continued    O
development    O
along    O
a    O
separate    O
line    O
of    O
computers    O
,    O
originally    O
with    O
lead    O
designer    O
Steve    O
Chen    O
and    O
the    O
Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
.    O

Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
,    O
a    O
supercomputer    O

Open    O
Compute    O
Project    O
testing    O
,    O
iSCSI    O
,    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
NVMe    O
,    O
Data    O
center    O
bridging    O
,    O
OpenFabrics    O
Alliance    O

Newer    O
generations    O
of    O
Symmetrix    O
brought    O
additional    O
host    O
connection    O
protocols    O
which    O
include    O
ESCON    O
,    O
SCSI    O
,    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
-    O
based    O
storage    O
area    O
networks    O
(    O
SANs    O
)    O
,    O
FICON    O
and    O
iSCSI    O
.    O

Upon    O
launch    O
in    O
2008    O
,    O
the    O
latest    O
generation    O
CX4    O
series    O
storage    O
arrays    O
supported    O
fibre    B-Supercomputer104358117
channel    I-Supercomputer104358117
and    O
iSCSI    O
front    O
end    O
bus    O
connectivity    O
.    O

In    O
1997    O
,    O
Data    O
General    O
's    O
Clariion    O
division    O
took    O
the    O
unusual    O
step    O
of    O
adopting    O
an    O
emerging    O
standard    O
—    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
.    O

They    O
are    O
used    O
as    O
the    O
first    O
portion    O
of    O
derivative    O
identifiers    O
to    O
uniquely    O
identify    O
a    O
particular    O
piece    O
of    O
equipment    O
as    O
MAC    O
addresses    O
,    O
Subnetwork    O
Access    O
Protocol    O
protocol    O
identifiers    O
,    O
World    O
Wide    O
Names    O
for    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
host    O
bus    O
adapters    O
,    O
and    O
other    O
Fibre    O
Channel    O
and    O
Serial    O
Attached    O
SCSI    O
devices    O
.    O

Some    O
of    O
the    O
storage    O
systems    O
in    O
which    O
an    O
OUI    O
based    O
variant    O
was    O
used    O
are    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
and    O
Serial    O
Attached    O
SCSI    O
(    O
SAS    O
)    O
.    O

A    O
World    O
Wide    O
Name    O
(    O
WWN    O
)    O
or    O
World    O
Wide    O
Identifier    O
(    O
WWID    O
)    O
is    O
a    O
unique    O
identifier    O
used    O
in    O
storage    O
technologies    O
including    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
Advanced    O
Technology    O
Attachment    O
(    O
ATA    O
)    O
or    O
Serial    O
Attached    O
SCSI    O
(    O
SAS    O
)    O
.    O

Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117

Ultrastar    O
–    O
Enterprise    O
-    O
class    O
line    O
of    O
3.5-inch    O
and    O
2.5-inch    O
HDDs    O
with    O
SCSI    O
,    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
SAS    O
,    O
and    O
SATA    O
interfaces    O
;    O
and    O
a    O
line    O
of    O
3.5-inch    O
and    O
2.5-inch    O
Fibre    O
Channel    O
and    O
SAS    O
SSDs    O
.    O

Today    O
,    O
VCSELs    O
have    O
replaced    O
edge    O
-    O
emitting    O
lasers    O
in    O
applications    O
for    O
short    O
-    O
range    O
fiberoptic    O
communication    O
such    O
as    O
Gigabit    O
Ethernet    O
and    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
.    O

This    O
may    O
include    O
"    O
persistent    O
reservation    O
fencing    O
"    O
via    O
the    O
SCSI3    O
,    O
fibre    O
channel    O
fencing    O
to    O
disable    O
the    O
fibre    B-Supercomputer104358117
channel    I-Supercomputer104358117
port    O
,    O
or    O
global    O
network    O
block    O
device    O
(    O
GNBD    O
)    O
fencing    O
to    O
disable    O
access    O
to    O
the    O
GNBD    O
server    O
.    O

It    O
is    O
optimized    O
for    O
use    O
in    O
virtualized    O
server    O
environments    O
including    O
NAS    O
,    O
iSCSI    O
,    O
and    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
applications    O
,    O
and    O
is    O
built    O
around    O
the    O
ZFS    O
file    O
system    O
.    O

TrueNAS    O
is    O
a    O
network    O
-    O
attached    O
storage    O
(    O
NAS    O
)    O
system    O
and    O
storage    O
area    O
network    O
(    O
SAN    O
)    O
device    O
that    O
supports    O
the    O
SMB    O
,    O
AFP    O
,    O
NFS    O
,    O
iSCSI    O
,    O
SSH    O
,    O
rsync    O
and    O
FTP    O
/    O
TFTP    O
sharing    O
protocols    O
over    O
Ethernet    O
and    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
network    O
fabrics    O
.    O

Software    O
runs    O
on    O
a    O
version    O
of    O
Linux    O
and    O
supports    O
the    O
iSCSI    O
protocol    O
as    O
well    O
supporting    O
Emulex    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
Host    O
Bus    O
Adapters    O
.    O

Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
host    O
ports    O
per    O
Director    O
-    O
8    O
or    O
16    O

SAN    O
protocols    O
include    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
iSCSI    O
,    O
ATA    O
over    O
Ethernet    O
(    O
AoE    O
)    O
and    O
HyperSCSI    O
.    O

In    O
2001    O
,    O
Dell    O
and    O
EMC    O
entered    O
into    O
a    O
partnership    O
whereby    O
both    O
companies    O
jointly    O
design    O
products    O
and    O
Dell    O
provided    O
support    O
for    O
certain    O
EMC    O
products    O
including    O
midrange    O
storage    O
systems    O
,    O
such    O
as    O
fibre    B-Supercomputer104358117
channel    I-Supercomputer104358117
and    O
iSCSI    O
storage    O
area    O
networks    O
.    O

By    O
2012    O
,    O
analysts    O
noticed    O
that    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
vendors    O
such    O
as    O
Brocade    O
Communications    O
Systems    O
had    O
already    O
lost    O
market    O
share    O
to    O
Cisco    O
.    O

Fabric    O
Shortest    O
Path    O
First    O
(    O
FSPF    O
)    O
is    O
a    O
routing    O
protocol    O
used    O
in    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
networks    O
.    O

StorNext    O
enables    O
multiple    O
Windows    O
,    O
Linux    O
and    O
Apple    O
workstations    O
to    O
access    O
shared    O
block    O
storage    O
over    O
a    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
network    O
.    O

NX    O
-    O
OS    O
is    O
a    O
network    O
operating    O
system    O
for    O
the    O
Nexus    O
-    O
series    O
Ethernet    O
switches    O
and    O
MDS    O
-    O
series    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
storage    O
area    O
network    O
switches    O
made    O
by    O
Cisco    O
Systems    O
.    O

Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
and    O
FICON    O

Xsan    O
enables    O
multiple    O
Mac    O
desktop    O
and    O
Xserve    O
systems    O
to    O
access    O
shared    O
block    O
storage    O
over    O
a    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
network    O
.    O

Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
SAS    O
,    O
RAID    O
and    O
SCSI    O
HBAs    O
.    O

the    O
standard    O
includes    O
link    O
integrity    O
monitoring    O
,    O
connection    O
management    O
and    O
mapping    O
mechanisms    O
for    O
both    O
SONET    O
/    O
SDH    O
and    O
non    O
-    O
SONET    O
/    O
SDH    O
clients    O
such    O
as    O
Ethernet    O
and    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
.    O

MPIO    O
for    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
disks    O

Since    O
HyperSCSI    O
was    O
in    O
direct    O
competition    O
with    O
the    O
older    O
and    O
well    O
established    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
,    O
and    O
the    O
standardized    O
iSCSI    O
,    O
it    O
was    O
not    O
adopted    O
by    O
commercial    O
vendors    O
.    O

In    O
computer    O
storage    O
,    O
a    O
logical    O
unit    O
number    O
,    O
or    O
LUN    O
,    O
is    O
a    O
number    O
used    O
to    O
identify    O
a    O
logical    O
unit    O
,    O
which    O
is    O
a    O
device    O
addressed    O
by    O
the    O
SCSI    O
protocol    O
or    O
Storage    O
Area    O
Network    O
protocols    O
which    O
encapsulate    O
SCSI    O
,    O
such    O
as    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
or    O
iSCSI    O
.    O

In    O
the    O
computer    O
storage    O
field    O
,    O
a    O
Fibre    O
Channel    O
switch    O
is    O
a    O
network    O
switch    O
compatible    O
with    O
the    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
(    O
FC    O
)    O
protocol    O
.    O

Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117

SAN    O
-    O
switch    O
Qlogic    O
with    O
optical    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
connectors    O
installed    O
.    O

FICON    O
(    O
Fibre    O
Connection    O
)    O
is    O
the    O
IBM    O
proprietary    O
name    O
for    O
the    O
ANSI    O
"    O
FC    O
-    O
SB-3    O
Single    O
-    O
Byte    O
Command    O
Code    O
Sets-3    O
Mapping    O
Protocol    O
"    O
for    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
(    O
FC    O
)    O
protocol    O
.    O

In    O
computing    O
,    O
the    O
proposed    O
Internet    O
Storage    O
Name    O
Service    O
(    O
iSNS    O
)    O
protocol    O
allows    O
automated    O
discovery    O
,    O
management    O
and    O
configuration    O
of    O
iSCSI    O
and    O
Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117
devices    O
(    O
using    O
iFCP    O
gateways    O
)    O
on    O
a    O
TCP    O
/    O
IP    O
network    O
.    O

Fibre    B-Supercomputer104358117
Channel    I-Supercomputer104358117

It    O
is    O
based    O
on    O
the    O
same    O
architecture    O
as    O
the    O
Origin    B-Supercomputer104358117
2000    I-Supercomputer104358117
but    O
has    O
an    O
unrelated    O
hardware    O
implementation    O
.    O

The    O
artists    O
at    O
South    O
Park    O
Studios    O
(    O
at    O
the    O
time    O
,    O
called    O
South    O
Park    O
Productions    O
)    O
used    O
a    O
multiprocessor    O
SGI    B-Supercomputer104358117
Origin    I-Supercomputer104358117
2000    I-Supercomputer104358117
and    O
31    O
multiprocessor    O
Origin    O
200    O
servers    O
(    O
with    O
1.14    O
terabytes    O
of    O
storage    O
)    O
for    O
both    O
rendering    O
and    O
asset    O
management    O
.    O

The    O
SN1    O
was    O
intended    O
to    O
replace    O
the    O
T3E    O
and    O
SGI    B-Supercomputer104358117
Origin    I-Supercomputer104358117
2000    I-Supercomputer104358117
systems    O
and    O
later    O
became    O
the    O
"    O
SN    O
-    O
MIPS    O
"    O
or    O
SGI    O
Origin    O
3000    O
architecture    O
.    O

The    O
ES-1    B-Supercomputer104358117
was    O
Evans    O
&    O
Sutherland    O
's    O
abortive    O
attempt    O
to    O
enter    O
the    O
supercomputer    O
market    O
.    O

The    O
features    O
were    O
provided    O
on    O
a    O
line    O
-    O
by    O
-    O
line    O
basis    O
by    O
the    O
selection    O
of    O
particular    O
Key    O
Telephone    O
Units    O
(    O
KTUs    O
)    O
plugged    O
into    O
a    O
pre    O
-    O
wired    O
backplane    B-Supercomputer104358117
in    O
the    O
central    O
control    O
unit    O
.    O

PICOe    O
(    O
PICO    O
Express    O
)    O
is    O
a    O
computer    O
form    O
factor    O
in    O
which    O
a    O
half    O
sized    O
card    O
slot    O
Single    O
Board    O
Computer    O
(    O
SBC    O
)    O
is    O
inserted    O
into    O
a    O
gold    O
fingers    O
card    O
slot    O
of    O
a    O
passive    O
or    O
active    O
backplane    B-Supercomputer104358117
.    O

In    O
a    O
backplane    B-Supercomputer104358117
design    O
such    O
as    O
the    O
Amiga    O
2000    O
,    O
connecting    O
the    O
/CFGOUT    O
of    O
one    O
slot    O
directly    O
to    O
the    O
/CFGIN    O
of    O
the    O
next    O
would    O
create    O
the    O
problem    O
that    O
an    O
unoccupied    O
slot    O
would    O
break    O
the    O
configuration    O
chain    O
.    O

Boards    O
are    O
standardized    O
to    O
3U    O
or    O
6U    O
sizes    O
,    O
and    O
are    O
typically    O
interconnected    O
via    O
a    O
passive    O
backplane    B-Supercomputer104358117
.    O

The    O
chassis    O
should    O
only    O
backplane    B-Supercomputer104358117
power    O
and    O
management    O
capabilities    O
to    O
extend    O
the    O
life    O
of    O
the    O
chassis    O
and    O
reduce    O
chassis    O
change    O
costs    O

Backplanes    B-Supercomputer104358117
,    O
test    O
adapters    O
,    O
and    O
power    O
strips    O
for    O
electronics    O
industry    O

TCS    O
,    O
based    O
in    O
Nashua    O
,    O
New    O
Hampshire    O
,    O
manufactures    O
high    O
-    O
density    O
electronic    O
connectors    O
,    O
complete    O
backplane    B-Supercomputer104358117
,    O
and    O
systems    O
packaging    O
,    O
a    O
product    O
line    O
that    O
complements    O
Amphenol    O
's    O
existing    O
lines    O
of    O
business    O
.    O

Engineers    O
used    O
the    O
common    O
techniques    O
of    O
standardized    O
bundles    O
of    O
wires    O
and    O
extended    O
the    O
concept    O
as    O
backplane    B-Supercomputer104358117
were    O
used    O
to    O
hold    O
printed    O
circuit    O
boards    O
in    O
these    O
early    O
machines    O
.    O

Instead    O
,    O
the    O
keyboard    O
switches    O
were    O
digitally    O
scanned    O
,    O
and    O
control    O
signals    O
sent    O
over    O
a    O
computer    O
backplane    B-Supercomputer104358117
where    O
they    O
were    O
inputs    O
to    O
the    O
computer    O
processor    O
,    O
which    O
would    O
then    O
route    O
the    O
signals    O
to    O
the    O
synthesis    O
modules    O
,    O
which    O
were    O
output    O
devices    O
on    O
the    O
backplane    O
.    O

Backplane    B-Supercomputer104358117

DVD    O
duplication    O
systems    O
are    O
generally    O
built    O
out    O
of    O
stacks    O
of    O
these    O
drives    O
,    O
connected    O
through    O
a    O
computer    O
-    O
based    O
backplane    B-Supercomputer104358117
.    O

Ozark    O
was    O
home    O
to    O
Simclar    O
Interconnect    O
Technologies    O
,    O
a    O
Simclar    O
Group    O
factory    O
,    O
providing    O
backplane    B-Supercomputer104358117
fabrication    O
for    O
telecommunications    O
and    O
data    O
communications    O
customers    O
.    O

It    O
consisted    O
of    O
several    O
Eurocard    O
PCB    O
's    O
with    O
DIN    O
41612    O
connectors    O
,    O
and    O
a    O
backplane    B-Supercomputer104358117
all    O
based    O
on    O
a    O
19-inch    O
rack    O
configuration    O
.    O

P    O
-    O
GENESIS    O
,    O
a    O
parallel    O
version    O
of    O
GENESIS    O
,    O
was    O
first    O
run    O
in    O
1990    O
on    O
the    O
Intel    O
Delta    O
,    O
which    O
was    O
the    O
prototype    O
for    O
the    O
Intel    B-Supercomputer104358117
Paragon    I-Supercomputer104358117
family    O
of    O
massively    O
parallel    O
supercomputers    O
.    O

In    O
UNICOS    O
/    O
lc    O
1.x    O
,    O
the    O
"    O
Compute    O
"    O
PEs    O
run    O
a    O
Sandia    O
developed    O
microkernel    O
called    O
Catamount    O
,    O
which    O
is    O
descended    O
from    O
the    O
SUNMOS    O
OS    O
of    O
the    O
Intel    B-Supercomputer104358117
Paragon    I-Supercomputer104358117
;    O
in    O
UNICOS    O
/    O
lc    O
2.0    O
,    O
Catamount    O
was    O
replaced    O
by    O
a    O
specially    O
tuned    O
version    O
of    O
Linux    O
called    O
Compute    O
Node    O
Linux    O
(    O
CNL    O
)    O
.    O

The    O
iPSC    O
line    O
was    O
superseded    O
by    O
a    O
research    O
project    O
called    O
the    O
Touchstone    O
Delta    O
at    O
the    O
California    O
Institute    O
of    O
Technology    O
which    O
evolved    O
into    O
the    O
Intel    B-Supercomputer104358117
Paragon    I-Supercomputer104358117
.    O

Cray    O
purchased    O
Supertek    O
in    O
1990    O
and    O
sold    O
the    O
S-1    O
as    O
the    O
Cray    B-Supercomputer104358117
XMS    I-Supercomputer104358117
,    O
but    O
the    O
machine    O
proved    O
problematic    O
;    O
meanwhile    O
,    O
their    O
not    O
-    O
yet    O
-    O
completed    O
S-2    O
,    O
a    O
Y    O
-    O
MP    O
clone    O
,    O
was    O
later    O
offered    O
as    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
EL    I-Supercomputer104358117
(    O
later    O
becoming    O
the    O
EL90    B-Supercomputer104358117
series    I-Supercomputer104358117
)    O
which    O
started    O
to    O
sell    O
in    O
reasonable    O
numbers    O
in    O
1991    O
-    O
92—to    O
mostly    O
smaller    O
companies    O
,    O
notably    O
in    O
the    O
oil    O
exploration    O
business    O
.    O

Watson    B-Supercomputer104358117
(disambiguation)    I-Supercomputer104358117

In    O
the    O
Soviet    O
Union    O
,    O
the    O
Elbrus    B-Supercomputer104358117
series    O
of    O
supercomputers    O
pioneered    O
the    O
use    O
of    O
tagged    O
architectures    O
in    O
1973    O
.    O

Elbrus-1    B-Supercomputer104358117

Elbrus-2    B-Supercomputer104358117

Elbrus-3    B-Supercomputer104358117

Earth    B-Supercomputer104358117
Simulator    I-Supercomputer104358117

All    O
the    O
fastest    O
supercomputers    O
in    O
the    O
decade    O
since    O
the    O
"    O
Earth    B-Supercomputer104358117
Simulator    I-Supercomputer104358117
"    O
have    O
used    O
Linux    O
.    O

It    O
was    O
launched    O
September    O
12    O
,    O
2003    O
and    O
on    O
September    O
13    O
,    O
2003    O
the    O
project    O
exceeded    O
the    O
capacity    O
of    O
the    O
Earth    B-Supercomputer104358117
Simulator    I-Supercomputer104358117
to    O
become    O
the    O
world    O
's    O
largest    O
climate    O
modelling    O
facility    O
.    O

-    O
Earth    B-Supercomputer104358117
Simulator    I-Supercomputer104358117

SUPER-UX    B-Supercomputer104358117

;    O
MathKeisan    O
:    O
NEC    O
's    O
math    O
library    O
,    O
supporting    O
NEC    B-Supercomputer104358117
SX    I-Supercomputer104358117
architecture    I-Supercomputer104358117
under    O
SUPER-UX    B-Supercomputer104358117
,    O
and    O
Itanium    O
under    O
Linux    O

Babel    O
works    O
on    O
all    O
known    O
POSIX    O
and    O
Unix    O
variants    O
,    O
including    O
Linux    O
,    O
Mac    O
OS    O
X    O
,    O
AIX    O
,    O
IRIX    O
,    O
Solaris    O
,    O
Tru64    O
,    O
Cray    O
's    O
XT4    B-Supercomputer104358117
,    O
IBM    O
's    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
,    O
and    O
many    O
commodity    O
clusters    B-Supercomputer104358117
.    O

Compute    O
Node    O
Linux    O
(    O
CNL    O
)    O
is    O
a    O
runtime    O
environment    O
based    O
on    O
the    O
Linux    O
kernel    O
for    O
the    O
Cray    B-Supercomputer104358117
XT3    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT5    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XT6    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
XE6    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
XK6    I-Supercomputer104358117
supercomputer    O
systems    O
based    O
on    O
SUSE    O
Linux    O
Enterprise    O
Server    O
.    O

The    O
Cray    B-Supercomputer104358117
XT4    I-Supercomputer104358117
,    O
introduced    O
in    O
2006    O
added    O
support    O
for    O
DDR2    O
memory    O
,    O
newer    O
dual    O
-    O
core    O
and    O
future    O
quad    O
-    O
core    O
Opteron    O
processors    O
and    O
utilized    O
a    O
second    O
generation    O
SeaStar2    O
communication    O
coprocessor    O
.    O

Its    O
design    O
was    O
influenced    O
by    O
the    O
VPP500/5000    O
models    O
of    O
the    O
Fujitsu    B-Supercomputer104358117
VP    I-Supercomputer104358117
2000    B-Supercomputer104358117
vector    O
processor    O
supercomputer    B-Supercomputer104358117
line    O
.    O

Blue    B-Supercomputer104358117
Waters    I-Supercomputer104358117
in    O
Illinois    O

He    O
previously    O
played    O
for    O
Blue    B-Supercomputer104358117
Waters    I-Supercomputer104358117
in    O
Namibia    O
.    O

The    O
university    O
contracted    O
with    O
Cray    O
to    O
build    O
the    O
National    O
Science    O
Foundation    O
-    O
funded    O
supercomputer    O
Blue    B-Supercomputer104358117
Waters    I-Supercomputer104358117
The    O
system    O
also    O
boasts    O
the    O
largest    O
public    O
online    O
storage    O
system    O
in    O
the    O
world    O
with    O
more    O
than    O
25    O
petabytes    O
of    O
usable    O
space    O
.    O

In    O
2011    O
Cray    O
also    O
announced    O
it    O
had    O
been    O
awarded    O
the    O
$    O
188    O
M    O
US    O
Blue    B-Supercomputer104358117
Waters    I-Supercomputer104358117
contract    O
with    O
the    O
University    O
of    O
Illinois    O
,    O
after    O
IBM    O
had    O
pulled    O
out    O
of    O
the    O
delivery    O
.    O

Fastra    B-Supercomputer104358117
II    I-Supercomputer104358117

Goodyear    O
Aerospace    O
,    O
a    O
holding    O
that    O
developed    O
from    O
the    O
Goodyear    O
Aircraft    O
Company    O
after    O
World    O
War    O
II    O
designed    O
a    O
supercomputer    O
for    O
NASA    O
's    O
Goddard    O
Spaceflight    O
Center    O
in    O
1979    O
,    O
the    O
MPP    B-Supercomputer104358117
.    O

Goodyear    B-Supercomputer104358117
MPP    I-Supercomputer104358117
was    O
an    O
early    O
implementation    O
of    O
a    O
massively    O
parallel    O
computer    O
architecture    O
.    O

Although    O
massive    O
parallelism    O
is    O
not    O
a    O
new    O
idea    O
(    O
earlier    O
examples    O
include    O
ILLIAC    B-Supercomputer104358117
IV    I-Supercomputer104358117
and    O
Goodyear    B-Supercomputer104358117
MPP    I-Supercomputer104358117
)    O
the    O
Xetal    O
1    O
was    O
one    O
of    O
the    O
first    O
to    O
apply    O
this    O
approach    O
to    O
image    O
processing    O
.    O

In    O
2002    O
,    O
Cray    O
Inc.    O
announced    O
their    O
first    O
new    O
model    O
,    O
the    O
Cray    B-Supercomputer104358117
X1    I-Supercomputer104358117
combined    O
architecture    O
vector    O
/    O
MPP    O
supercomputer    O
.    O

As    O
of    O
November    O
2004    O
,    O
the    O
Cray    B-Supercomputer104358117
X1    I-Supercomputer104358117
had    O
a    O
maximum    O
measured    O
performance    O
of    O
5.9    O
teraflops    O
,    O
being    O
the    O
29th    O
fastest    O
supercomputer    O
in    O
the    O
world    O
.    O

IBM    B-Supercomputer104358117
Kittyhawk    I-Supercomputer104358117
,    O
2008    O
supercomputer    O
designed    O
by    O
IBM    O
to    O
run    O
entire    O
Internet    O

IBM    B-Supercomputer104358117
7950    I-Supercomputer104358117
:    O
Cryptanalytic    O
computer    O
using    O
7030    O
as    O
CPU    O
;    O
1962    O
(    O
"    O
Harvest    O
"    O
)    O

He    O
worked    O
on    O
the    O
architecture    O
of    O
the    O
IBM    B-Supercomputer104358117
7030    I-Supercomputer104358117
Stretch    I-Supercomputer104358117
,    O
a    O
$    O
10    O
million    O
scientific    O
supercomputer    O
of    O
which    O
nine    O
were    O
sold    O
,    O
and    O
the    O
IBM    B-Supercomputer104358117
7950    I-Supercomputer104358117
Harvest    I-Supercomputer104358117
computer    O
for    O
the    O
National    O
Security    O
Agency    O
.    O

Alternatively    O
,    O
challenge    O
funds    O
are    O
often    O
used    O
as    O
ways    O
to    O
address    O
what    O
development    O
partners    O
describe    O
as    O
a    O
Grand    B-Supercomputer104358117
Challenge    I-Supercomputer104358117
,    O
which    O
is    O
a    O
challenge    O
fund    O
focused    O
on    O
soliciting    O
proposals    O
around    O
a    O
very    O
specific    O
critical    O
barrier    O
that    O
,    O
if    O
removed    O
,    O
would    O
help    O
solve    O
an    O
important    O
health    O
problem    O
in    O
the    O
developing    O
world    O
,    O
with    O
a    O
high    O
likelihood    O
of    O
global    O
impact    O
through    O
widespread    O
implementation    O
.    O

His    O
current    O
research    O
is    O
concerned    O
primarily    O
with    O
petascale    B-Supercomputer104358117
computing    O
and    O
its    O
application    O
to    O
Grand    B-Supercomputer104358117
Challenge    I-Supercomputer104358117
problems    O
.    O

Via    O
Spanish    B-Supercomputer104358117
Supercomputing    I-Supercomputer104358117
Network    I-Supercomputer104358117
.    O

It    O
is    O
part    O
of    O
the    O
Spanish    B-Supercomputer104358117
Supercomputing    I-Supercomputer104358117
Network    I-Supercomputer104358117
,    O
and    O
has    O
been    O
designed    O
to    O
increase    O
the    O
computational    O
power    O
provided    O
by    O
Iamus    O
.    O

It    O
was    O
slowly    O
phased    O
out    O
as    O
its    O
successors    O
at    O
NAS    O
,    O
the    O
petascale    B-Supercomputer104358117
Pleiades    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
and    O
the    O
Endeavour    B-Supercomputer104358117
shared    O
-    O
memory    O
system    O
,    O
expanded    O
to    O
meet    O
with    O
NASA    O
’s    O
growing    O
high    O
-    O
end    O
computing    O
needs    O
.    O

Also    O
named    O
after    O
him    O
is    O
the    O
Pawsey    O
Centre    O
,    O
the    O
home    O
of    O
petascale    B-Supercomputer104358117
supercomputing    O
facilities    O
and    O
expertise    O
to    O
support    O
international    O
Square    O
Kilometre    O
Array    O
research    O
and    O
other    O
high    O
-    O
end    O
science    O
(    O
based    O
at    O
Technology    O
Park    O
in    O
the    O
Perth    O
suburb    O
of    O
Bentley    O
)    O
.    O

The    O
Aggressive    O
COMA    O
research    O
project    O
was    O
selected    O
as    O
one    O
of    O
the    O
"    O
Eight    O
Point    O
-    O
Design    O
Studies    O
"    O
that    O
DARPA    O
,    O
NSF    O
,    O
NSA    O
and    O
NASA    O
supported    O
in    O
the    O
mid    O
-    O
nineties    O
in    O
a    O
nationwide    O
effort    O
to    O
accelerate    O
the    O
arrival    O
of    O
a    O
petascale    B-Supercomputer104358117
machine    O
.    O

Linux    O
also    O
ran    O
on    O
the    O
first    O
teraFLOPS    O
supercomputer    O
,    O
"    O
ASCI    B-Supercomputer104358117
Red    I-Supercomputer104358117
"    O
in    O
1997    O
,    O
and    O
on    O
"    O
IBM    B-Supercomputer104358117
Roadrunner    I-Supercomputer104358117
"    O
in    O
2008    O
,    O
which    O
was    O
the    O
first    O
petascale    B-Supercomputer104358117
computer    O
.    O

The    O
term    O
first    O
entered    O
usage    O
in    O
2010    O
with    O
the    O
advent    O
of    O
petascale    B-Supercomputer104358117
computing    I-Supercomputer104358117
,    O
and    O
has    O
since    O
been    O
measured    O
for    O
many    O
of    O
the    O
world    O
's    O
largest    O
supercomputers    O
.    O

The    O
later    O
CDC    B-Supercomputer104358117
Cyber    I-Supercomputer104358117
70    O
and    O
170    O
computers    O
were    O
very    O
similar    O
to    O
the    O
CDC    O
6600    O
in    O
overall    O
design    O
and    O
were    O
nearly    O
completely    O
backwards    O
compatible    O
.    O

It    O
was    O
essentially    O
a    O
modernized    O
version    O
of    O
the    O
CDC    B-Supercomputer104358117
Cyber    I-Supercomputer104358117
computer    O
,    O
and    O
deliberately    O
kept    O
compatibility    O
with    O
it    O
.    O

By    O
1975    O
the    O
PLATO    O
System    O
served    O
almost    O
150    O
locations    O
from    O
a    O
donated    O
CDC    B-Supercomputer104358117
Cyber    I-Supercomputer104358117
73    O
,    O
including    O
not    O
only    O
the    O
users    O
of    O
the    O
PLATO    O
III    O
system    O
,    O
but    O
a    O
number    O
of    O
grammar    O
schools    O
,    O
high    O
schools    O
,    O
colleges    O
and    O
universities    O
,    O
and    O
military    O
installations    O
.    O

In    O
1974    O
PLATO    O
was    O
running    O
on    O
in    O
-    O
house    O
machines    O
at    O
CDC    O
headquarters    O
in    O
Minneapolis    O
,    O
and    O
in    O
1976    O
they    O
purchased    O
the    O
commercial    O
rights    O
in    O
exchange    O
for    O
a    O
new    O
CDC    B-Supercomputer104358117
Cyber    I-Supercomputer104358117
machine    O
.    O

That    O
trend    O
was    O
partly    O
responsible    O
for    O
a    O
move    O
away    O
from    O
the    O
in    O
-    O
house    O
Cray    O
Operating    O
System    O
to    O
UNICOS    B-Supercomputer104358117
system    O
based    O
on    O
Unix    O
.    O

These    O
variants    O
of    O
Unix    O
included    O
IBM    O
AIX    O
,    O
the    O
open    O
source    O
Linux    O
system    O
,    O
and    O
other    O
adaptations    O
such    O
as    O
UNICOS    B-Supercomputer104358117
from    O
Cray    O
.    O

Unlike    O
many    O
other    O
MPP    O
systems    O
,    O
including    O
the    O
T3D    O
,    O
the    O
T3E    O
was    O
fully    O
self    O
-    O
hosted    O
and    O
ran    O
the    O
UNICOS    B-Supercomputer104358117
/    O
mk    O
distributed    O
operating    O
system    O
with    O
a    O
"    O
GigaRing    O
"    O
I    O
/    O
O    O
subsystem    O
integrated    O
into    O
the    O
torus    O
for    O
network    O
,    O
disk    O
and    O
tape    O
I    O
/    O
O.    O

CNL    O
forms    O
part    O
of    O
the    O
Cray    B-Supercomputer104358117
Linux    I-Supercomputer104358117
Environment    I-Supercomputer104358117
.    O

Cray    B-Supercomputer104358117
Linux    I-Supercomputer104358117
Environment    I-Supercomputer104358117

At    O
that    O
time    O
it    O
was    O
the    O
second    O
most    O
powerful    O
supercomputer    O
in    O
Europe    O
,    O
after    O
the    O
MareNostrum    B-Supercomputer104358117
in    O
Barcelona    O
.    O

MareNostrum    B-Supercomputer104358117
,    O
one    O
of    O
the    O
most    O
powerful    O
supercomputers    O
in    O
Europe    O

In    O
November    O
2002    O
,    O
Yasumasa    O
Kanada    O
and    O
a    O
team    O
of    O
9    O
others    O
used    O
the    O
Hitachi    B-Supercomputer104358117
SR8000    I-Supercomputer104358117
,    O
a    O
64-node    O
supercomputer    O
with    O
1    O
terabyte    O
of    O
main    O
memory    O
,    O
to    O
calculate    O
to    O
roughly    O
1.24    O
trillion    O
digits    O
in    O
around    O
600    O
hours    O
.    O

Cray    O
eventually    O
realized    O
that    O
the    O
approach    O
was    O
likely    O
the    O
only    O
way    O
forward    O
and    O
started    O
a    O
five    O
-    O
year    O
project    O
to    O
capture    O
the    O
lead    O
in    O
this    O
area    O
:    O
the    O
plan    O
's    O
result    O
was    O
the    O
DEC    O
Alpha    O
-    O
based    O
Cray    B-Supercomputer104358117
T3D    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
T3E    I-Supercomputer104358117
series    O
,    O
which    O
left    O
Cray    O
as    O
the    O
only    O
remaining    O
supercomputer    O
vendor    O
in    O
the    O
market    O
besides    O
NEC    B-Supercomputer104358117
by    O
2000    O
.    O

the    O
LOFAR    B-Supercomputer104358117
is    O
looking    O
for    O
radio    O
transients    O
.    O

LOFAR    B-Supercomputer104358117
,    O
Low    B-Supercomputer104358117
Frequency    I-Supercomputer104358117
Array    I-Supercomputer104358117

The    O
observatory    O
also    O
hosts    O
the    O
UK    O
's    O
LOFAR    B-Supercomputer104358117
station    O
.    O

2010    O
-    O
LOFAR    B-Supercomputer104358117
station    O
UK608    O
constructed    O

LORUN    O
stands    O
for    O
LOFAR    B-Supercomputer104358117
at    O
Radboud    O
University    O
Nijmegen    O
.    O

It    O
is    O
a    O
radio    O
telescope    O
based    O
on    O
antennas    O
designed    O
for    O
the    O
THETA    O
test    O
station    O
for    O
the    O
LOFAR    B-Supercomputer104358117
radio    O
telescope    O
.    O

COBALT    O
,    O
The    O
radio    O
correlator    O
for    O
LOFAR    B-Supercomputer104358117
,    O
see    O
LOFAR#Timeline    O

As    O
of    O
June    O
2017    O
,    O
K    O
is    O
the    O
world    O
's    O
eighth    O
-    O
fastest    O
computer    O
,    O
with    O
the    O
Chinese    O
Sunway    O
TaihuLight    O
and    O
Tianhe-2    B-Supercomputer104358117
being    O
the    O
fastest    O
supercomputers    O
.    O

As    O
of    O
June    O
2013    O
,    O
the    O
world    O
's    O
fastest    O
supercomputer    O
is    O
China    O
's    O
Tianhe-2    B-Supercomputer104358117
,    O
capable    O
of    O
a    O
LINPACK    O
performance    O
of    O
over    O
33    O
petaflops;.    O

Tianhe-I    B-Supercomputer104358117
and    O
Tianhe-2    B-Supercomputer104358117
,    O
supercomputers    O
built    O
by    O
China    O
.    O

As    O
an    O
ongoing    O
program    O
,    O
it    O
has    O
produced    O
several    O
notable    O
developments    O
including    O
the    O
Loongson    B-Supercomputer104358117
computer    O
processor    O
family    O
(    O
originally    O
named    O
"    O
Godson    O
"    O
)    O
,    O
the    O
Tianhe    B-Supercomputer104358117
supercomputers    O
,    O
and    O
aspects    O
of    O
the    O
Shenzhou    O
spacecraft    O
.    O

TOP500    B-Supercomputer104358117
reports    O
that    O
China    O
's    O
Tianhe-2    B-Supercomputer104358117
supercomputer    O
is    O
the    O
world    O
's    O
most    O
powerful    O
computer    O
,    O
capable    O
of    O
performing    O
over    O
33    O
quadrillion    O
floating    O
point    O
operations    O
per    O
second    O
.    O

CCC    O
was    O
building    O
the    O
Cray-3/SSS    B-Supercomputer104358117
when    O
it    O
went    O
into    O
Chapter    O
11    O
in    O
March    O
1995    O
.    O

ILLIAC    O
II    O
and    O
The    O
IBM    B-Supercomputer104358117
7030    I-Supercomputer104358117
Stretch    I-Supercomputer104358117
were    O
two    O
competing    O
projects    O
to    O
build    O
1st    O
-    O
generation    O
transistorized    O
supercomputers    O

Generally    O
considered    O
to    O
be    O
the    O
first    O
successful    O
supercomputer    O
,    O
it    O
outperformed    O
the    O
industry    O
's    O
prior    O
recordholder    O
,    O
the    O
IBM    B-Supercomputer104358117
7030    I-Supercomputer104358117
Stretch    I-Supercomputer104358117
,    O
by    O
a    O
factor    O
of    O
three    O
.    O

The    O
6600    O
was    O
three    O
times    O
faster    O
than    O
the    O
previous    O
record    O
-    O
holder    O
,    O
the    O
IBM    B-Supercomputer104358117
7030    I-Supercomputer104358117
Stretch    I-Supercomputer104358117
;    O
this    O
alarmed    O
IBM    O
.    O

The    O
ACS    O
project    O
began    O
in    O
1961    O
as    O
"    O
Project    O
Y    O
"    O
with    O
a    O
goal    O
of    O
“    O
building    O
a    O
machine    O
that    O
was    O
one    O
hundred    O
times    O
faster    O
than    O
Stretch    B-Supercomputer104358117
”    O
.    O

STRETCH    O
Assembly    O
Program    O
(    O
STRAP    O
)    O
was    O
the    O
assembler    O
for    O
the    O
IBM    B-Supercomputer104358117
7030    I-Supercomputer104358117
Stretch    I-Supercomputer104358117
computer    O
.    O

Rather    O
than    O
naming    O
the    O
pipeline    O
stages    O
,    O
"    O
Fetch    O
,    O
Decode    O
,    O
and    O
Execute    O
"    O
(    O
as    O
on    O
Stretch    B-Supercomputer104358117
)    O
,    O
the    O
pipelined    O
stages    O
were    O
named    O
,    O
"    O
Advanced    O
Control    O
,    O
Delayed    O
Control    O
,    O
and    O
Interplay    O
"    O
.    O

Like    O
the    O
IBM    O
Stretch    B-Supercomputer104358117
computer    O
,    O
ILLIAC    O
II    O
was    O
designed    O
using    O
"    O
future    O
transistors    O
"    O
that    O
had    O
not    O
yet    O
been    O
invented    O
.    O

Amdahl    O
discusses    O
his    O
graduate    O
work    O
at    O
the    O
University    O
of    O
Wisconsin    O
and    O
his    O
design    O
of    O
WISC    O
.    O
Discusses    O
his    O
role    O
in    O
the    O
design    O
of    O
several    O
computers    O
for    O
IBM    O
including    O
the    O
STRETCH    B-Supercomputer104358117
,    O
IBM    O
701    O
,    O
and    O
IBM    O
704    O
.    O

In    O
November    O
2004    O
,    O
Columbia    B-Supercomputer104358117
entered    O
the    O
list    O
at    O
#    O
2    O
with    O
51.8    O
Teraflops    O
,    O
and    O
there    O
was    O
at    O
least    O
one    O
Itanium    O
-    O
based    O
computer    O
in    O
the    O
top    O
10    O
from    O
then    O
until    O
June    O
2007    O
.    O

November    O
:    O
"    O
Columbia    B-Supercomputer104358117
"    O
,    O
an    O
SGI    O
Altix    B-Supercomputer104358117
3700    O
with    O
10160    O
Itanium    O
2    O
processors    O
at    O
NASA    O
Ames    O
Research    O
Center    O
,    O
is    O
listed    O
on    O
the    O
TOP500    B-Supercomputer104358117
list    O
at    O
position    O
#    O
2    O
.    O

SAGE    O
used    O
massive    O
AN/FSQ-7    B-Supercomputer104358117
computers    O
to    O
combine    O
reports    O
sent    O
in    O
via    O
teletype    O
linked    O
radar    O
stations    O
to    O
produce    O
a    O
picture    O
of    O
all    O
of    O
the    O
air    O
traffic    O
in    O
a    O
particular    O
"    O
sector"s    O
area    O
.    O

The    O
wing    O
was    O
responsible    O
to    O
ensure    O
that    O
SAGE    B-Supercomputer104358117
computer    I-Supercomputer104358117
systems    I-Supercomputer104358117
were    O
based    O
on    O
approved    O
operational    O
concepts    O
for    O
air    O
defense    O
and    O
provided    O
guidance    O
to    O
operational    O
units    O
concerning    O
the    O
implementation    O
,    O
installation    O
,    O
testing    O
,    O
and    O
utilization    O
of    O
SAGE    O
computer    O
programs    O
.    O

The    O
IBM    O
728    O
magnetic    O
tape    O
drive    O
was    O
used    O
on    O
the    O
SAGE    O
AN/FSQ-7    B-Supercomputer104358117
computer    O
.    O

By    O
1954    O
the    O
MARC    O
(    O
Matador    O
Airborne    O
Radio    O
Control    O
)    O
used    O
the    O
AN    O
/    O
MSQ-1A    O
for    O
missile    O
guidance    O
to    O
the    O
terminal    O
dive    O
point    O
,    O
and    O
SAGE    B-Supercomputer104358117
GCI    O
provided    O
computer    O
-    O
controlled    O
guidance    O
of    O
aircraft    O
to    O
continuously    O
-    O
computed    O
interception    O
points    O
(    O
1958    O
AN/FSQ-7    B-Supercomputer104358117
Bomarc    O
missile    O
guidance    O
and    O
the    O
later    O
Ground    O
to    O
Air    O
Data    O
Link    O
Subsystem    O
for    O
fighters    O
)    O
.    O

AN/FSQ-7    B-Supercomputer104358117
:    O
IBM    O
air    O
defense    O
command    O
and    O
control    O
computer    O
;    O
Combat    O
Direction    O
Central    O

AN/FSQ-8    B-Supercomputer104358117
:    O
IBM    O
air    O
defense    O
command    O
and    O
control    O
computer    O
;    O
Combat    O
Control    O
Central    O

DC-20    O
with    O
its    O
AN/FSQ-7    B-Supercomputer104358117
computer    O

,    O
ADC    O
requested    O
the    O
Air    O
Defense    O
Systems    O
Integration    O
Division    O
to    O
study    O
accelerating    O
the    O
scheduled    O
1962    O
deployment    O
of    O
those    O
4    O
sites    O
.    O
)    O
Super    O
Combat    O
Centers    O
and    O
solid    O
-    O
state    O
AN    O
/    O
FSQ-32s    O
were    O
cancelled    O
in    O
1960    O
and    O
on    O
March    O
22    O
,    O
1960    O
,    O
the    O
United    O
States    O
Secretary    O
of    O
Defense    O
authorized    O
an    O
IBM    B-Supercomputer104358117
AN/FSQ-7    I-Supercomputer104358117
Combat    I-Supercomputer104358117
Direction    I-Supercomputer104358117
Central    I-Supercomputer104358117
(    O
BOMARC    O
ground    O
equipment    O
)    O
be    O
provided    O
for    O
CADIN    O
instead    O
of    O
an    O
AN    O
/    O
FSQ-32    O
.    O

Kan    B-Supercomputer104358117
Balam    I-Supercomputer104358117
,    O
super    O
computer    O
in    O
Latin    O
America    O

The    O
SX-9    O
Series    O
implements    O
an    O
SMP    O
system    O
in    O
a    O
compact    O
node    O
module    O
and    O
uses    O
an    O
enhanced    O
version    O
of    O
the    O
single    O
chip    O
vector    O
processor    O
that    O
was    O
introduced    O
with    O
the    O
SX-6    B-Supercomputer104358117
.    O

Roadrunner    B-Supercomputer104358117
,    O
built    O
by    O
IBM    O
,    O
was    O
the    O
first    O
computer    O
to    O
go    O
petascale    O
,    O
and    O
did    O
so    O
on    O
May    O
25    O
,    O
2008    O
,    O
with    O
sustained    O
performance    O
of    O
1.026    O
petaflops    O
.    O

It    O
has    O
produced    O
the    O
System    O
i    O
series    O
,    O
been    O
home    O
to    O
the    O
first    O
Blue    B-Supercomputer104358117
Gene    I-Supercomputer104358117
prototype    O
,    O
and    O
contributed    O
the    O
servers    O
for    O
Roadrunner    B-Supercomputer104358117
.    O

On    O
May    O
25    O
,    O
2008    O
,    O
an    O
American    O
supercomputer    O
built    O
by    O
IBM    O
,    O
named    O
'    O
Roadrunner    B-Supercomputer104358117
'    O
,    O
reached    O
the    O
computing    O
milestone    O
of    O
one    O
petaFLOPS    O
.    O

It    O
is    O
one    O
of    O
the    O
building    O
blocks    O
of    O
the    O
IBM    B-Supercomputer104358117
Roadrunner    I-Supercomputer104358117
cluster    O
,    O
which    O
was    O
the    O
first    O
supercomputer    O
architecture    O
to    O
break    O
the    O
PFLOPS    O
barrier    O
.    O

Linux    O
also    O
ran    O
on    O
the    O
first    O
teraFLOPS    O
supercomputer    O
,    O
"    O
ASCI    B-Supercomputer104358117
Red    I-Supercomputer104358117
"    O
in    O
1997    O
,    O
and    O
on    O
"    O
IBM    B-Supercomputer104358117
Roadrunner    I-Supercomputer104358117
"    O
in    O
2008    O
,    O
which    O
was    O
the    O
first    O
petascale    B-Supercomputer104358117
computer    O
.    O

Cray    O
set    O
up    O
Cray    O
Research    O
Superservers    O
,    O
Inc.    O
(    O
later    O
the    O
Business    O
Systems    O
Division    O
)    O
to    O
sell    O
this    O
system    O
as    O
the    O
Cray    B-Supercomputer104358117
S-MP    I-Supercomputer104358117
,    O
later    O
replacing    O
it    O
with    O
the    O
Cray    B-Supercomputer104358117
CS6400    I-Supercomputer104358117
.    O

Cray    B-Supercomputer104358117
CS6400    I-Supercomputer104358117

SiCortex    B-Supercomputer104358117
"    O
SiCortex    O
node    O
"    O
has    O
six    O
MIPS64    O
cores    O
on    O
a    O
single    O
chip    O
.    O

:*    O
DEGIMA    B-Supercomputer104358117
(computer    I-Supercomputer104358117
cluster)    I-Supercomputer104358117

Its    O
design    O
was    O
influenced    O
by    O
the    O
VPP500/5000    O
models    O
of    O
the    O
Fujitsu    B-Supercomputer104358117
VP    I-Supercomputer104358117
2000    B-Supercomputer104358117
vector    O
processor    O
supercomputer    B-Supercomputer104358117
line    O
.    O

Arctur    O
also    O
provides    O
compute    O
resources    O
on    O
its    O
Arctur-1    B-Supercomputer104358117
supercomputer    O
to    O
the    O
Slovenian    O
NGI    O
.    O

Its    O
design    O
later    O
influenced    O
the    O
BBN    B-Supercomputer104358117
Butterfly    I-Supercomputer104358117
computer    O
.    O

Computer    O
Systems    O
:    O
RS/6000    O
,    O
Scalable    B-Supercomputer104358117
POWERparallel    I-Supercomputer104358117

IBM    B-Supercomputer104358117
Scalable    I-Supercomputer104358117
POWERparallel    I-Supercomputer104358117
,    O
a    O
supercomputer    O
platform    O
with    O
a    O
version    O
named    O
SP2    O

WARP    B-Supercomputer104358117
(systolic    I-Supercomputer104358117
array)    I-Supercomputer104358117

The    O
Bigben    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
was    O
a    O
Cray    O
XT3    O
MPP    O
system    O
with    O
2068    O
nodes    O
located    O
at    O
Pittsburgh    O
Supercomputing    O
Center    O
.    O

ASCI    B-Supercomputer104358117
White    I-Supercomputer104358117

IBM    B-Supercomputer104358117
NORC    I-Supercomputer104358117
–    O
1954    O
,    O
the    O
first    O
supercomputer    O

IBM    B-Supercomputer104358117
NORC    I-Supercomputer104358117
:    O
Naval    O
Ordnance    O
Research    O
Calculator    O
;    O
1954    O

The    O
Cray    B-Supercomputer104358117
APP    I-Supercomputer104358117
(    O
"    O
Attached    O
Parallel    O
Processor    O
"    O
)    O
was    O
a    O
parallel    B-Supercomputer104358117
computer    I-Supercomputer104358117
sold    O
by    O
Cray    O
Research    O
from    O
1992    O
onwards    O
.    O

Throughout    O
,    O
Cray    O
continued    O
to    O
be    O
the    O
performance    O
leader    O
,    O
continually    O
beating    O
the    O
competition    O
with    O
a    O
series    O
of    O
machines    O
that    O
led    O
to    O
the    O
Cray-2    B-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
X-MP    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
.    O

After    O
Chen    O
's    O
departure    O
,    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
C90    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
T90    I-Supercomputer104358117
were    O
developed    O
on    O
the    O
original    O
Cray-1    O
architecture    O
but    O
achieved    O
much    O
greater    O
performance    O
via    O
multiple    O
additional    O
processors    O
,    O
faster    O
clocks    O
,    O
and    O
wider    O
vector    O
pipes    O
.    O

Cray    O
purchased    O
Supertek    O
in    O
1990    O
and    O
sold    O
the    O
S-1    O
as    O
the    O
Cray    B-Supercomputer104358117
XMS    I-Supercomputer104358117
,    O
but    O
the    O
machine    O
proved    O
problematic    O
;    O
meanwhile    O
,    O
their    O
not    O
-    O
yet    O
-    O
completed    O
S-2    O
,    O
a    O
Y    O
-    O
MP    O
clone    O
,    O
was    O
later    O
offered    O
as    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
EL    I-Supercomputer104358117
(    O
later    O
becoming    O
the    O
EL90    B-Supercomputer104358117
series    I-Supercomputer104358117
)    O
which    O
started    O
to    O
sell    O
in    O
reasonable    O
numbers    O
in    O
1991    O
-    O
92—to    O
mostly    O
smaller    O
companies    O
,    O
notably    O
in    O
the    O
oil    O
exploration    O
business    O
.    O

A    O
1990    O
model    O
,    O
was    O
quoted    O
at    O
$    O
123,400    O
,    O
for    O
example    O
,    O
although    O
much    O
less    O
than    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
.    O

In    O
a    O
correspondence    O
match    O
lasting    O
many    O
months    O
,    O
he    O
won    O
two    O
games    O
and    O
drew    O
a    O
third    O
against    O
Hydra    B-Supercomputer104358117
,    O
the    O
most    O
powerful    O
chess    O
supercomputer    O
in    O
the    O
world    O
at    O
that    O
time    O
(    O
2005    O
)    O
.    O

Hydra    B-Supercomputer104358117
,    O
predecessor    O
was    O
called    O
Brutus    O

MVAPICH2-EA    O
,    O
which    O
is    O
energy    O
-    O
aware    O
and    O
supports    O
InfiniBand    B-Supercomputer104358117
,    O
iWARP    B-Supercomputer104358117
,    O
and    O
RoCE    O

iWARP    B-Supercomputer104358117

Cray    O
purchased    O
Supertek    O
in    O
1990    O
and    O
sold    O
the    O
S-1    O
as    O
the    O
Cray    B-Supercomputer104358117
XMS    I-Supercomputer104358117
,    O
but    O
the    O
machine    O
proved    O
problematic    O
;    O
meanwhile    O
,    O
their    O
not    O
-    O
yet    O
-    O
completed    O
S-2    O
,    O
a    O
Y    O
-    O
MP    O
clone    O
,    O
was    O
later    O
offered    O
as    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
EL    I-Supercomputer104358117
(    O
later    O
becoming    O
the    O
EL90    B-Supercomputer104358117
series    I-Supercomputer104358117
)    O
which    O
started    O
to    O
sell    O
in    O
reasonable    O
numbers    O
in    O
1991    O
-    O
92—to    O
mostly    O
smaller    O
companies    O
,    O
notably    O
in    O
the    O
oil    O
exploration    O
business    O
.    O

It    O
was    O
slowly    O
phased    O
out    O
as    O
its    O
successors    O
at    O
NAS    O
,    O
the    O
petascale    B-Supercomputer104358117
Pleiades    B-Supercomputer104358117
supercomputer    I-Supercomputer104358117
and    O
the    O
Endeavour    B-Supercomputer104358117
shared    O
-    O
memory    O
system    O
,    O
expanded    O
to    O
meet    O
with    O
NASA    O
’s    O
growing    O
high    O
-    O
end    O
computing    O
needs    O
.    O

IBM    B-Supercomputer104358117
Mira    I-Supercomputer104358117

Between    O
January    O
and    O
August    O
1999    O
,    O
RSA-155    O
,    O
a    O
challenge    O
number    O
prepared    O
by    O
the    O
RSA    O
company    O
,    O
was    O
factorised    O
using    O
GNFS    O
with    O
relations    O
again    O
contributed    O
by    O
a    O
large    O
group    O
,    O
and    O
the    O
final    O
stages    O
of    O
the    O
calculation    O
performed    O
in    O
just    O
over    O
nine    O
days    O
on    O
the    O
Cray    O
C916    B-Supercomputer104358117
supercomputer    O
at    O
the    O
SARA    O
Amsterdam    O
Academic    O
Computer    O
Center    O
.    O

After    O
Chen    O
's    O
departure    O
,    O
the    O
Cray    B-Supercomputer104358117
Y-MP    I-Supercomputer104358117
,    O
Cray    B-Supercomputer104358117
C90    I-Supercomputer104358117
and    O
Cray    B-Supercomputer104358117
T90    I-Supercomputer104358117
were    O
developed    O
on    O
the    O
original    O
Cray-1    O
architecture    O
but    O
achieved    O
much    O
greater    O
performance    O
via    O
multiple    O
additional    O
processors    O
,    O
faster    O
clocks    O
,    O
and    O
wider    O
vector    O
pipes    O
.    O

The    O
M45    B-Supercomputer104358117
Project    I-Supercomputer104358117
(    O
or    O
M45    B-Supercomputer104358117
)    O
is    O
the    O
name    O
of    O
a    O
cluster    O
announced    O
in    O
November    O
2007    O
by    O
Yahoo!.    O

This    O
system    O
,    O
with    O
over    O
224,000    O
processing    O
cores    O
,    O
was    O
dubbed    O
"    O
Jaguar    B-Supercomputer104358117
"    O
and    O
was    O
the    O
fastest    O
computer    O
in    O
the    O
world    O
as    O
measured    O
by    O
the    O
LINPACK    O
benchmark    O
at    O
the    O
speed    O
of    O
1.75    O
petaflops    O
until    O
being    O
surpassed    O
by    O
the    O
Tianhe-1A    B-Supercomputer104358117
in    O
October    O
2010    O
.    O

Tianhe-I    B-Supercomputer104358117
(    O
TH-1    O
)    O
,    O
a    O
Chinese    O
super    O
computer    O

The    O
previous    O
record    O
holder    O
was    O
the    O
Chinese    O
National    O
University    O
of    O
Defense    O
Technology    O
's    O
Tianhe-1A    B-Supercomputer104358117
,    O
which    O
performed    O
at    O
2.507    O
petaflops    O
.    O

Tianhe-I    B-Supercomputer104358117
and    O
Tianhe-2    B-Supercomputer104358117
,    O
supercomputers    O
built    O
by    O
China    O
.    O

In    O
Lustre    O
2.3    O
and    O
earlier    O
,    O
Myrinet    B-Supercomputer104358117
,    O
Quadrics    O
,    O
Cray    O
SeaStar    O
and    O
RapidArray    O
networks    O
were    O
also    O
supported    O
,    O
but    O
these    O
network    O
drivers    O
were    O
deprecated    O
when    O
these    O
networks    O
were    O
no    O
longer    O
commercially    O
available    O
,    O
and    O
support    O
was    O
removed    O
completely    O
in    O
Lustre    O
2.8    O
.    O

Myrinet    B-Supercomputer104358117

All    O
the    O
nodes    O
are    O
interconnected    O
with    O
a    O
low    O
latency    O
(    O
2.6    O
–    O
3.2    O
μs    O
)    O
and    O
high    O
bandwidth    O
network    O
called    O
Myrinet    B-Supercomputer104358117
.    O

Many    O
high    O
-    O
performance    O
interconnects    O
including    O
Myrinet    B-Supercomputer104358117
,    O
Quadrics    O
,    O
IEEE    O
1355    O
,    O
and    O
SpaceWire    O
support    O
source    O
routing    O
.    O

When    O
using    O
source    O
routing    O
with    O
Myrinet    B-Supercomputer104358117
,    O
the    O
sender    O
of    O
the    O
packet    O
prepends    O
the    O
complete    O
route    O
,    O
one    O
byte    O
for    O
every    O
crossbar    O
,    O
to    O
each    O
packet    O
header    O
.    O

In    O
2009    O
,    O
Purdue    O
named    O
the    O
Coates    B-Supercomputer104358117
supercomputing    B-Supercomputer104358117
cluster    I-Supercomputer104358117
,    O
after    O
him    O
,    O
continuing    O
a    O
practice    O
of    O
naming    O
the    O
machines    O
for    O
prominent    O
figures    O
in    O
the    O
history    O
of    O
computing    O
at    O
the    O
university    O
,    O
which    O
began    O
with    O
Purdue    O
's    O
Steele    B-Supercomputer104358117
cluster    I-Supercomputer104358117
.    O

The    O
Cray    B-Supercomputer104358117
CX1000    I-Supercomputer104358117
is    O
a    O
family    O
of    O
high    O
-    O
performance    O
computers    O
which    O
is    O
manufactured    O
by    O
Cray    O
Inc.    O
,    O
and    O
consists    O
of    O
two    O
individual    O
groups    O
of    O
computer    O
systems    O
.    O

In    O
June    O
1997    O
,    O
Intel    O
's    O
ASCI    B-Supercomputer104358117
Red    I-Supercomputer104358117
was    O
the    O
world    O
's    O
first    O
computer    O
to    O
achieve    O
one    O
teraFLOPS    O
and    O
beyond    O
.    O

ASCI    B-Supercomputer104358117
Red    I-Supercomputer104358117

Linux    O
also    O
ran    O
on    O
the    O
first    O
teraFLOPS    O
supercomputer    O
,    O
"    O
ASCI    B-Supercomputer104358117
Red    I-Supercomputer104358117
"    O
in    O
1997    O
,    O
and    O
on    O
"    O
IBM    B-Supercomputer104358117
Roadrunner    I-Supercomputer104358117
"    O
in    O
2008    O
,    O
which    O
was    O
the    O
first    O
petascale    B-Supercomputer104358117
computer    O
.    O

BESM-2    B-Supercomputer104358117

BESM-4    B-Supercomputer104358117

BESM-6    B-Supercomputer104358117

The    O
report    O
was    O
translated    O
into    O
Russian    O
,    O
German    O
,    O
French    O
and    O
Bulgarian    O
,    O
and    O
allowed    O
programming    O
in    O
languages    O
with    O
larger    O
character    O
sets    O
,    O
e.g.    O
Cyrillic    O
alphabet    O
of    O
the    O
Soviet    O
BESM-4    B-Supercomputer104358117
.    O

Steele    B-Supercomputer104358117
(supercomputer)    I-Supercomputer104358117
,    O
at    O
Purdue    O
University    O

November    O
:    O
"    O
Columbia    B-Supercomputer104358117
"    O
,    O
an    O
SGI    O
Altix    B-Supercomputer104358117
3700    O
with    O
10160    O
Itanium    O
2    O
processors    O
at    O
NASA    O
Ames    O
Research    O
Center    O
,    O
is    O
listed    O
on    O
the    O
TOP500    B-Supercomputer104358117
list    O
at    O
position    O
#    O
2    O
.    O

This    O
goal    O
was    O
never    O
achieved    O
before    O
SGI    O
divested    O
itself    O
of    O
the    O
Cray    O
business    O
,    O
and    O
the    O
SN2    O
name    O
was    O
later    O
associated    O
with    O
the    O
"    O
SN    O
-    O
IA    O
"    O
or    O
SGI    B-Supercomputer104358117
Altix    I-Supercomputer104358117
3000    I-Supercomputer104358117
architecture    O
.    O

This    O
contrasts    O
from    O
their    O
previous    O
systems    O
architecture    O
,    O
which    O
consisted    O
of    O
eight    O
Silicon    O
Graphics    O
Altix    B-Supercomputer104358117
computers    O
,    O
each    O
housing    O
1024    O
processor    O
cores    O
.    O

Blinkenlights    O
on    O
the    O
NSA    O
's    O
FROSTBURG    B-Supercomputer104358117
supercomputer    O
from    O
the    O
1990s    O
.    O


